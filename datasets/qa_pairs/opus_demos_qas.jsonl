{"question":"\nI have an English-to-Russian translation model and the ruDALL-E text-to-image generation model. With my current code, I can input English text, translate it to Russian, feed the Russian text to ruDALL-E to generate an image, and output the generated image.\n\nI would like to create a Gradio web interface to demonstrate this pipeline end-to-end:\n- Have a text input box where the user can enter an English text prompt \n- When the user submits, first translate the English text to Russian\n- Display the translated Russian text\n- Feed the Russian text to ruDALL-E to generate an image \n- Display the generated image in the interface\n- Allow the user to resubmit the form to generate new images from the same prompt\n- Provide some pre-filled example prompts the user can try\n- Put the translation and image generation components side-by-side in the interface\n- Launch the interface with a Hugging Face theme\n- Include links to the model's GitHub repo and a related article\n","answer":"here is the gradio solution for your use case: ```python\nimport random\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom functools import partialmethod\nimport gradio as gr\nfrom gradio.mix import Series\nfrom transformers import pipeline, FSMTForConditionalGeneration, FSMTTokenizer\nfrom rudalle.pipelines import generate_images\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae\n\n# disable tqdm logging from the rudalle pipeline\ntqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntranslation_model = FSMTForConditionalGeneration.from_pretrained(\"facebook\/wmt19-en-ru\", torch_dtype=torch.float16).half().to(device)\ntranslation_tokenizer = FSMTTokenizer.from_pretrained(\"facebook\/wmt19-en-ru\")\ndalle = get_rudalle_model(\"Malevich\", pretrained=True, fp16=True, device=device)\ntokenizer = get_tokenizer()\nvae = get_vae().to(device)\n\ndef translation_wrapper(text: str):\n    input_ids = translation_tokenizer.encode(text, return_tensors=\"pt\")\n    outputs = translation_model.generate(input_ids.to(device))\n    decoded = translation_tokenizer.decode(outputs[0].float(), skip_special_tokens=True)\n    return decoded\n\ndef dalle_wrapper(prompt: str):\n    top_k, top_p = random.choice([\n        (1024, 0.98),\n        (512, 0.97),\n        (384, 0.96),\n    ])\n    \n    images , _ = generate_images(\n        prompt, \n        tokenizer, \n        dalle, \n        vae, \n        top_k=top_k, \n        images_num=1, \n        top_p=top_p\n    )\n    title = f\"<b>{prompt}<\/b>\"\n    return title, images[0]\n\n\ntranslator = gr.Interface(fn=translation_wrapper, \n                          inputs=[gr.inputs.Textbox(label='What would you like to see?')],\n                          outputs=\"text\")\noutputs = [\n    gr.outputs.HTML(label=\"\"),   \n    gr.outputs.Image(label=\"\"),\n]\ngenerator = gr.Interface(fn=dalle_wrapper, inputs=\"text\", outputs=outputs)\n\n\ndescription = (\n    \"ruDALL-E is a 1.3B params text-to-image model by SberAI (links at the bottom). \"\n    \"This demo uses an English-Russian translation model to adapt the prompts. \"\n    \"Try pressing [Submit] multiple times to generate new images!\"\n)\narticle = (\n    \"<p style='text-align: center'>\"\n    \"<a href='https:\/\/github.com\/sberbank-ai\/ru-dalle'>GitHub<\/a> | \"\n    \"<a href='https:\/\/habr.com\/ru\/company\/sberbank\/blog\/586926\/'>Article (in Russian)<\/a>\"\n    \"<\/p>\"\n)\nexamples = [[\"A still life of grapes and a bottle of wine\"], \n            [\"\u0413\u043e\u0440\u043e\u0434 \u0432 \u0441\u0442\u0438\u043b\u0435 \u043a\u0438\u0431\u0435\u0440\u043f\u0430\u043d\u043a\"], \n            [\"A colorful photo of a coral reef\"], \n            [\"A white cat sitting in a cardboard box\"]]\n            \nseries = Series(translator, generator, \n                title='Kinda-English ruDALL-E',\n                description=description,\n                article=article,\n                layout='horizontal',\n                theme='huggingface',\n                examples=examples,\n                allow_flagging=False,\n                live=False, \n                enable_queue=True,\n               )\nseries.launch()\n\n\n```"}
{"question":"\nI have a Tacotron2 model trained on Mandarin Chinese speech data using the Coqui TTS toolkit. With my current code, I can input Mandarin text and the model will synthesize speech audio.\n\nI would like to create a Gradio web interface for this Mandarin text-to-speech model with the following:\n- A text input box for the user to enter the Mandarin text they want converted to speech\n- Automatically run the text-to-speech when the user submits their input text \n- Display the generated speech audio output for the user to listen to\n- Provide some example Mandarin text inputs that the user can easily test out\n- A description of what the model does\n- Links to more information such as a blog post and the code repository\n","answer":"here is the gradio solution for your use case: ```python\nimport tempfile\nimport gradio as gr\n# import numpy as np\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\n\n\ntitle = \"Mandarin Text-to-Speech (TTS)\"\ndescription = \"Generate mandarin speech from text using a Tachotron2 model with Coqui TTS \ud83d\udc38, \" \\\n              \"a deep learning toolkit for Text-to-Speech.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/news.machinelearning.sg\/posts\/mandarin_text_to_speech_with_coqui_tts\/' target='blank' class='footer'>Blog<\/a> | <a href='https:\/\/github.com\/eugenesiow\/practical-ml' target='_blank' class='footer'>Github Repo<\/a><\/p>\"\nexamples = [\n    [\"\u8bed\u97f3\u5408\u6210\u662f\u901a\u8fc7\u673a\u68b0\u7684\u3001\u7535\u5b50\u7684\u65b9\u6cd5\u4ea7\u751f\u4eba\u9020\u8bed\u97f3\u7684\u6280\u672f\u3002\"],\n    [\"\u674e\u663e\u9f99\u603b\u7406\u8868\u793a\uff0c\u6211\u56fd\u8981\u8fbe\u5230\u50cf\u610f\u5927\u5229\u7684\u5f00\u653e\u7a0b\u5ea6\uff0c\u51e0\u4e4e\u56de\u5230\u51a0\u75c5\u75ab\u60c5\u524d\u7684\u751f\u6d3b\uff0c\u8fd8\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\u3002\"]\n]\n\n\nmanager = ModelManager()\nmodel_path, config_path, model_item = manager.download_model(\"tts_models\/zh-CN\/baker\/tacotron2-DDC-GST\")\nsynthesizer = Synthesizer(\n    model_path, config_path, None, None, None,\n)\n\n\ndef inference(text: str):\n    wavs = synthesizer.tts(text)\n    # output = (synthesizer.output_sample_rate, np.array(wavs).astype(np.float32))\n    # return output\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        synthesizer.save_wav(wavs, fp)\n        return fp.name\n\n\ngr.Interface(\n    fn=inference,\n    inputs=[\n        gr.inputs.Textbox(\n            label=\"Input\",\n            default=\"\u4f60\u597d\u5417\uff1f\u6211\u5f88\u597d\u3002\",\n        ),\n    ],\n    outputs=gr.outputs.Audio(label=\"Output\"),\n    title=title,\n    description=description,\n    article=article,\n    examples=examples,\n    enable_queue=True,\n    allow_flagging=False,\n    ).launch(debug=False)\n\n```"}
{"question":"\nI have a set of summarization models that were each trained on a dataset called MLSUM in different languages - French, German, Turkish, and Spanish. The models are encoder-decoder transformer models. With my current code, I can load the appropriate model and tokenizer for a selected language, pass in some input text, and generate a summary of that text in the same language.\n\nI would like to create an interactive demo using Gradio to showcase these multilingual summarization models. The Gradio interface should include:\n\n- A radio button input for the user to select the summarization language from the supported options \n- A text input box for the user to paste the article\/text they want to summarize\n- Display the generated summary output after running the model\n- A title, description and article to explain the demo and link to more info\n- Enable the queued interface so multiple people can try it out without blocking\n- Use the \"darkgrass\" theme to style the UI\n\nThe backend code will load all the models and tokenizers upfront, then based on the language selected by the user, will use the appropriate model to summarize the input text and return the output summary to display in the interface.\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport torch\nfrom transformers import RobertaTokenizerFast, BertTokenizerFast, EncoderDecoderModel\n\n\nLANGUAGES = [\"fr\", \"de\", \"tu\", \"es\"]\n\nmodels = dict()\ntokenizers = dict()\nmodels_paths = dict()\n\nmodels_paths[\"fr\"] = \"mrm8488\/camembert2camembert_shared-finetuned-french-summarization\"\nmodels_paths[\"de\"] = \"mrm8488\/bert2bert_shared-german-finetuned-summarization\"\nmodels_paths[\"tu\"] = \"mrm8488\/bert2bert_shared-turkish-summarization\"\nmodels_paths[\"es\"] = \"Narrativa\/bsc_roberta2roberta_shared-spanish-finetuned-mlsum-summarization\"\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nfor lang in LANGUAGES:\n    tokenizers[lang] = RobertaTokenizerFast.from_pretrained(models_paths[lang]) if lang in [\"fr\", \"es\"] else BertTokenizerFast.from_pretrained(models_paths[lang])\n    models[lang] = EncoderDecoderModel.from_pretrained(models_paths[lang]).to(device)\n                   \n\ndef summarize(lang, text):\n    tokenizer = tokenizers[lang]\n    model = models[lang]\n    inputs = tokenizer([text], padding=\"max_length\",\n                       truncation=True, max_length=512, return_tensors=\"pt\")\n    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n    output = model.generate(input_ids, attention_mask=attention_mask)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n\n\ntheme = \"darkgrass\"\n\ntitle = \"Multilingual Summarization model (MLSUM)\"\n\ndescription = \"Gradio Demo for Summarization models trained on MLSUM dataset by Manuel Romero\"\n\narticle = \"<p style='text-align: center'><a href='https:\/\/hf.co\/mrm8488' target='_blank'>More models<\/a><\/p>\"\n\n\ngr.Interface(fn=summarize, inputs=[gr.inputs.Radio(LANGUAGES), gr.inputs.Textbox(\n    lines=7, label=\"Input Text\")], outputs=\"text\", theme=theme, title=title, description=description, article=article, enable_queue=True).launch(inline=False)\n\n```"}
{"question":"\nI have a Python script that can extract the main text content from a news article given the URL. It uses the newspaper library to download and parse the article. I also have access to a pretrained summarization model from Hugging Face called bart-large-cnn that can summarize long articles.\n\nI would like to create a Gradio web app that combines these two functionalities into a news article summarizer with the following features:\n- A text input box where the user can paste in a URL to a news article\n- An \"Extract\" button that will scrape the article text from the URL and display it \n- A \"Summarize\" button that will take the extracted article text, run it through the bart-large-cnn model, and display the summarized version\n- Put the article extractor and summarizer components in series, so the output of the extractor becomes the input to the summarizer\n- Customize the appearance with a title, description, theme, and layout\n- Provide some sample URLs the user can try out to see how it works\n","answer":"here is the gradio solution for your use case: ```python\nfrom newspaper import Article\nfrom newspaper import Config\nimport gradio as gr\nfrom gradio.mix import Parallel, Series\n\n\n\ndef extrac_text(url):\n  USER_AGENT = 'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko\/20100101 Firefox\/78.0'\n  config = Config()\n  config.browser_user_agent = USER_AGENT\n  config.request_timeout = 10\n\n  article = Article(url, config=config)\n  article.download()\n  article.parse()\n  text = article.text\n  return text\n\nextractor = gr.Interface(extrac_text, 'text', 'text')\nsummarizer = gr.Interface.load(\"huggingface\/facebook\/bart-large-cnn\")\n\nsample_url = [['https:\/\/www.cp24.com\/news\/ontario-reports-481-new-covid-19-cases-1-death-1.5667950'],\n]\n\ndesc =  '''\n         The news summarizer app uses bart-large-cnn model by Facebook to summarize the text of a news article.\n        '''\n\niface = Series(extractor, summarizer, \n  inputs = gr.inputs.Textbox(\n      lines = 2,\n      label = 'Enter URL below'\n  ),\n  outputs = 'text',\n  title = 'News Summarizer',\n  theme = 'grass',\n  layout = 'horizontal',\n  description = desc,\n  examples=sample_url)\n\niface.launch()\n```"}
{"question":"\nI have a pre-trained Korean language model called KoGPT from Kakao Brain. It is a large generative transformer model that can take in Korean text prompts and generate realistic continuations of the text.\n\nWith my current code, I can load the KoGPT model and tokenizer, input a prompt, and have it generate a sequence of text that continues from the prompt in a natural way.\n\nI would like to create an interactive Gradio web demo to showcase the KoGPT model and make it easy for people to experiment with. The interface should have the following components:\n\n- A text input box for the user to enter a prompt in Korean \n- A submit button to generate the continuation text from the prompt\n- Display the generated text output \n- A set of example prompts that the user can select to easily load and test\n- A title, description, and links to learn more about the KoGPT model\n\nSome key features I'd like:\n- Use the Hugging Face API to easily load the pre-trained KoGPT model and tokenizer \n- Adjust generation parameters like max length and sampling temperature \n- Queue requests on the backend to handle multiple users\n- Clean and simple UI with Gradio components\n- Mobile friendly responsive layout\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \n\n\ntokenizer = AutoTokenizer.from_pretrained(\n  'kakaobrain\/kogpt', revision='KoGPT6B-ryan1.5b',\n  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n  'kakaobrain\/kogpt', revision='KoGPT6B-ryan1.5b',\n  pad_token_id=tokenizer.eos_token_id,\n  torch_dtype=torch.float16, low_cpu_mem_usage=True\n).to(device='cpu', non_blocking=True)\n_ = model.eval()\n\ntitle = \"KoGPT\"\ndescription = \"Gradio demo for KoGPT(Korean Generative Pre-trained Transformer). To use it, simply add your text, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/kakaobrain\/kogpt' target='_blank'>KoGPT: KakaoBrain Korean(hangul) Generative Pre-trained Transformer<\/a> | <a href='https:\/\/huggingface.co\/kakaobrain\/kogpt' target='_blank'>Huggingface Model<\/a><\/p>\"\nexamples=[['\uc778\uac04\ucc98\ub7fc \uc0dd\uac01\ud558\uace0, \ud589\ub3d9\ud558\ub294 \\'\uc9c0\ub2a5\\'\uc744 \ud1b5\ud574 \uc778\ub958\uac00 \uc774\uc81c\uae4c\uc9c0 \ud480\uc9c0 \ubabb\ud588\ub358']]\ndef greet(text):\n    prompt = text\n    with torch.no_grad():\n      tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cpu', non_blocking=True)\n      gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n      generated = tokenizer.batch_decode(gen_tokens)[0]\n      \n    return generated\n    \niface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\", title=title, description=description, article=article, examples=examples,enable_queue=True)\niface.launch()\n```"}
{"question":"\nI have a sentiment analysis model that can take in Turkish text and output a sentiment score between -1 and 1, where negative values indicate negative sentiment and positive values indicate positive sentiment. With my current code, I can input Turkish sentences and get back the sentiment scores.\n\nI would like to create an interactive Gradio web interface for my sentiment analysis model with the following features:\n- A textbox input where the user can enter multiple Turkish sentences, one per line\n- Example inputs that the user can easily load and test \n- Process the input sentences with the sentiment model and adjust the scores to a 0 to 1 scale\n- Display the sentiment scores as a histogram plot output\n- Launch the interface to share and test it out\n","answer":"here is the gradio solution for your use case: ```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel = AutoModelForSequenceClassification.from_pretrained(\"savasy\/bert-base-turkish-sentiment-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy\/bert-base-turkish-sentiment-cased\")\nsa= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n\ndef adjust(x):\n  if x<0:\n    return 2*x+1\n  return 2*x-1\n\ndef sa2(s):\n  res= sa(s)\n  return [adjust(-1*r['score']) if r['label']=='negative' else adjust(r['score']) for r in res ]\n \n  \ndef get_examples():\n  #return [e for e in  open(\"examplesTR.csv\").readlines()]\n  return [\"Bu filmi be\u011fenmedim\\n bu filmi be\u011fendim\\n ceketin \u00e7ok g\u00fczel\\n bug\u00fcn ne yesek\"]\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\ndef grfunc(comments):\n  df=pd.DataFrame()\n  c2=[s.strip() for s in comments.split(\"\\n\") if len(s.split())>2]\n  df[\"scores\"]= sa2(c2)\n  df.plot(kind='hist')\n  return plt.gcf()\n \nimport gradio as gr\n\niface = gr.Interface(\n  fn=grfunc, \n  inputs=gr.inputs.Textbox(placeholder=\"put your sentences line by line\", lines=5), \n  outputs=\"plot\",\n  examples=get_examples())\niface.launch()\n\n```"}
{"question":"\nI have access to the GPT-J-6B language model from EleutherAI on Hugging Face. This is a large 6 billion parameter autoregressive Transformer model that can generate human-like text completions given a text prompt as input.\n\nI would like to create an interactive Gradio demo to showcase the GPT-J-6B model and allow users to experiment with it. The demo should have the following features:\n\n- A large text input box for the user to enter the text prompt \n- A submit button to generate the text completion from the model\n- Display the generated text output \n- A theme and color scheme to make the interface look nice\n- A title, description, and link to more info about the model \n- A few preset example prompts that the user can easily load and try out\n- Enable the queuing system to handle multiple users\n\nI would also like to host this on Hugging Face Spaces so it is easily accessible for anyone to try out the model.\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\ntheme = \"darkgrass\"\ntitle = \"GPT-J-6B Demo\"\ndescription = \"Gradio Demo for GPT-J 6B, a Transformer model trained using Ben Wang's Mesh Transformer JAX. 'GPT-J' refers to the class of model, while '6B' represents the number of trainable parameters.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/kingoflolz\/mesh-transformer-jax' target='_blank'>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model<\/a><\/p>\"\nexamples = [\n    ['Narrativa is a NLP\/NLG company that '],\n    [\"from transformers import AutoTokenizer\"],\n    [\"- How are you?\"]\n]\ngr.Interface.load(\"huggingface\/EleutherAI\/gpt-j-6B\", inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n                  theme=theme, title=title, description=description, article=article, examples=examples, enable_queue=True).launch()\n\n```"}
{"question":"\nI have a named entity recognition model that can extract mentions of adverse drug reactions (ADRs) and drug names from unstructured medical text. With my current code, I can input a piece of text and the model will output the text with the extracted ADR and drug entities highlighted.\n\nI would like to create an interactive Gradio demo for this ADR extraction model with the following features:\n- A large text input box for the user to paste in a piece of medical text to extract ADRs and drug names from\n- Display the output text with the extracted ADR and drug entities highlighted in different colors \n- Include a description of what the demo does and the definition of an adverse drug reaction\n- Provide several example inputs that demonstrate the model's ADR and drug name extraction capabilities on different medical text\n- Use the \"huggingface\" Gradio theme and a horizontal layout for the input and output components\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nfrom spacy import displacy\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"abhibisht89\/spanbert-large-cased-finetuned-ade_corpus_v2\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"abhibisht89\/spanbert-large-cased-finetuned-ade_corpus_v2\").to('cpu')\nadr_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer,grouped_entities=True)      \n\ndef get_adr_from_text(sentence):\n    tokens = adr_ner_model(sentence)\n    entities = []\n    \n    for token in tokens:\n        label = token[\"entity_group\"]\n        if label != \"O\":\n            token[\"label\"] = label\n            entities.append(token)\n    \n    params = [{\"text\": sentence,\n               \"ents\": entities,\n               \"title\": None}]\n    \n    html = displacy.render(params, style=\"ent\", manual=True, options={\n        \"colors\": {\n                   \"DRUG\": \"#f08080\",\n                   \"ADR\": \"#9bddff\",\n               },\n    })\n    return html\n\nexp=[\"Abortion, miscarriage or uterine hemorrhage associated with misoprostol (Cytotec), a labor-inducing drug.\",\n    \"Addiction to many sedatives and analgesics, such as diazepam, morphine, etc.\",\n    \"Birth defects associated with thalidomide\",\n    \"Bleeding of the intestine associated with aspirin therapy\",\n    \"Cardiovascular disease associated with COX-2 inhibitors (i.e. Vioxx)\",\n    \"Deafness and kidney failure associated with gentamicin (an antibiotic)\",\n    \"Having fever after taking paracetamol\"]\n\ndesc=\"An adverse drug reaction (ADR) can be defined as an appreciably harmful or unpleasant reaction resulting from an intervention related to the use of a medicinal product.\\\n The goal of this project is to extracts the adverse drug reaction from unstructured text with the Drug.\"\n\ninp=gr.inputs.Textbox(lines=5, placeholder=None, default=\"\", label=\"text to extract adverse drug reaction and drug mention\")\nout=gr.outputs.HTML(label=None)\n\niface = gr.Interface(fn=get_adr_from_text, inputs=inp, outputs=out,examples=exp,article=desc,title=\"Adverse Drug Reaction Xtractor\",theme=\"huggingface\",layout='horizontal')\niface.launch()\n```"}
{"question":"\nI have a semantic search system powered by SentenceTransformers that can take a natural language query and retrieve the most relevant passages from a corpus of Wikipedia articles. The system uses a bi-encoder for efficient retrieval and a cross-encoder for more accurate re-ranking of the retrieved results.\n\nWith my current code, I can input a query and get back the top 5 most relevant passages from the corpus. \n\nI would like to create an interactive Gradio web interface for this neural search engine with the following features:\n\n- A text input box for the user to enter their search query\n- Display the top 5 most relevant passages that are retrieved for the query, each in a separate output textbox\n- Include some preset example queries that the user can easily click to populate the search box\n- Add a title, description, and some styling to the interface to make it more engaging and informative for the user\n\nThe interface should allow the user to iteratively search the Wikipedia corpus and view the most relevant results to their queries.\n","answer":"here is the gradio solution for your use case: ```python\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util\nimport torch\nimport pickle\nimport pandas as pd\nimport gradio as gr\n\nbi_encoder = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\ncross_encoder = CrossEncoder(\"cross-encoder\/ms-marco-MiniLM-L-6-v2\")\ncorpus_embeddings=pd.read_pickle(\"corpus_embeddings_cpu.pkl\")\ncorpus=pd.read_pickle(\"corpus.pkl\")\n\ndef search(query,top_k=100):\n    print(\"Top 5 Answer by the NSE:\")\n    print()\n    ans=[]\n    ##### Sematic Search #####\n    # Encode the query using the bi-encoder and find potentially relevant passages\n    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n    hits = hits[0]  # Get the hits for the first query\n\n    ##### Re-Ranking #####\n    # Now, score all retrieved passages with the cross_encoder\n    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n\n    # Sort results by the cross-encoder scores\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n\n    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    \n    for idx, hit in enumerate(hits[0:5]):\n        ans.append(corpus[hit['corpus_id']])\n    return ans[0],ans[1],ans[2],ans[3],ans[4]\n\nexp=[\"Who is steve jobs?\",\"What is coldplay?\",\"What is a turing test?\",\"What is the most interesting thing about our universe?\",\"What are the most beautiful places on earth?\"]\n\ndesc=\"This is a semantic search engine powered by SentenceTransformers (Nils_Reimers) with a retrieval and reranking system on Wikipedia corpus. This will return the top 5 results. So Quest on with Transformers.\"\n\ninp=gr.inputs.Textbox(lines=1, placeholder=None, default=\"\", label=\"search you query here\")\nout=gr.outputs.Textbox(type=\"auto\",label=\"search results\")\n\niface = gr.Interface(fn=search, inputs=inp, outputs=[out,out,out,out,out],examples=exp,article=desc,title=\"Neural Search Engine\",theme=\"huggingface\",layout='vertical')\niface.launch()\n```"}
{"question":"\nI have a fine-tuned XLM-RoBERTa model that can take in product text (like a product name, description, category etc.) and predict the corresponding ECOICOP (European Classification of Individual Consumption by Purpose) product category, which is used to calculate Consumer Price Indexes. The model was trained on product data from online supermarkets in Germany, France, Austria and Italy. With my current code, I can input some text, run the model, and get the predicted ECOICOP label, index, and the model's confidence score as outputs.\n\nI would like to create an interactive demo of my model using Gradio with the following features:\n- A textbox input for the user to paste in some product text to classify \n- Display the following outputs after running the model:\n    - The predicted ECOICOP label \n    - The predicted ECOICOP index\n    - The model's confidence score for the prediction\n- A title and description explaining what the model does\n- Several example inputs that the user can easily load and test\n- A launch button to run the Gradio interface with debug mode enabled\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nfrom transformers import pipeline\n\npipe = pipeline(\"text-classification\", model=\"peter2000\/xlm-roberta-base-finetuned-ecoicop\")\n\ndef predict(text):\n    preds = pipe(text)[0]\n    return preds[\"label\"].split('_')[1],preds[\"label\"].split('_')[0], round(preds[\"score\"], 5)\n\ngradio_ui = gr.Interface(\n    fn=predict,\n    title=\"Predicting E-Coicop Product Categories from Product texts\",\n    description=\"The model is trained on product texts (shop category | text from URL | product name) from different online supermarkets in Germany, France, Austria and Italy. It predicts the corresponding ECOICOP product category (used to calculate Consumer Price Index) for food and baverages, out of 75 sub-labels on the 5-digits level of ECOICOP hierarchy.\",\n    inputs=[\n        gr.inputs.Textbox(lines=5, label=\"Paste some text here\"),\n    ],\n    outputs=[\n        gr.outputs.Textbox(label=\"ECOICOP Label\"),\n        gr.outputs.Textbox(label=\"ECOICOP Index\"),\n        gr.outputs.Textbox(label=\"Certainty\")\n    ],\n    examples=[\n        [\"Tiefk\u00fchl Eiscreme & Eisw\u00fcrfel Bechereis | rewe beste wahl peanut butter eiscreme | REWE Beste Wahl Peanut Butter Eiscreme 500ml\"], \n        [\"epicerie-sucree | cereales chocolat fraise nat | C\u00e9r\u00e9ales chocolat & fraise NAT\"],\n        [\"Pelati e passate | unknown | Mutti Polpa di Pomodoro 3 x 400 g\"]\n    ],\n)\n\ngradio_ui.launch(debug=True)\n```"}
{"question":"\nI have a pre-trained transformer model that can classify the emotions in tweets into 6 categories: sadness, joy, love, anger, fear and surprise. With my current code, I can input a tweet and get back the predicted probability scores for each emotion category.\n\nI would like to create an interactive Gradio web interface for this tweet emotion classifier with the following features:\n- A text input box for the user to enter a tweet (up to 5 lines of text)\n- Display the predicted probability scores for all 6 emotion categories as the output\n- Include a title and description explaining what the interface does\n- Provide a few example tweets that can be loaded into the interface to test it out\n- Use the \"grass\" theme to style the interface\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\n\nmodel_path = \"trnt\/twitter_emotions\"\nis_gpu = False\ndevice = torch.device('cuda') if is_gpu else torch.device('cpu')\nprint(device)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.to(device)\nmodel.eval()\nprint(\"Model was loaded\")\n\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=is_gpu-1)\nemotions = {'LABEL_0': 'sadness', 'LABEL_1': 'joy', 'LABEL_2': 'love', 'LABEL_3': 'anger', 'LABEL_4': 'fear',\n            'LABEL_5': 'surprise'}\nexamples = [\"I love you!\", \"I hate you!\"]\n\n\ndef predict(twitter):\n    pred = classifier(twitter, return_all_scores=True)[0]\n    res = {\"Sadness\": pred[0][\"score\"],\n           \"Joy\": pred[1][\"score\"],\n           \"Love\": pred[2][\"score\"],\n           \"Anger\": pred[3][\"score\"],\n           \"Fear\": pred[4][\"score\"],\n           \"Surprise\": pred[5][\"score\"]}\n    # \"This tweet is %s with probability=%.2f\" % (emotions[pred['label']], 100 * pred['score']) + \"%\"\n    return res\n\n\nif __name__ == '__main__':\n    interFace = gr.Interface(fn=predict,\n                             inputs=gr.inputs.Textbox(placeholder=\"Enter a tweet here\", label=\"Tweet content\", lines=5),\n                             outputs=gr.outputs.Label(num_top_classes=6, label=\"Emotions of this tweet is \"),\n                             verbose=True,\n                             examples=examples,\n                             title=\"Emotions of English tweet\",\n                             description=\"\",\n                             theme=\"grass\")\n    interFace.launch()\n\n```"}
{"question":"\nI have a fine-tuned language detection model based on xlm-roberta-base that can classify text into one of 45 different languages. With my current code, I can input a text string and the model will output the predicted language.\n\nI would like to create an interactive Gradio demo for this language detection model with the following features:\n- A large text input box for the user to paste or type the text they want to classify. It should have a placeholder with instructions and support multiple lines of text.\n- An output label that displays the detected language after the user submits their text.\n- Several examples of text in different languages that the user can click to populate the text input box and test out the model. \n- A title, description, and article explaining what the model does, what languages it supports, and a brief overview of how it works.\n- Use the \"huggingface\" Gradio theme to style the demo.\n","answer":"here is the gradio solution for your use case: ```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom languages import LANGUANGE_MAP\nimport gradio as gr\nimport torch\n\n\nmodel_ckpt = \"ivanlau\/language-detection-fine-tuned-on-xlm-roberta-base\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n\ndef detect_language(sentence):\n    tokenized_sentence = tokenizer(sentence, return_tensors='pt')\n    output = model(**tokenized_sentence)\n    predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n    _, preds = torch.max(predictions, dim=-1)\n    return LANGUANGE_MAP[preds.item()]\n\nexamples = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"\u606d\u559c\u53d1\u8d22!\",\n    \"Jumpa lagi, saya pergi kerja.\",\n    \"\u4f60\u98df\u5497\u98ef\u672a\u5440?\",\n    \"\u3082\u3046\u98df\u3079\u307e\u3057\u305f\u304b?\",\n    \"as-tu mang\u00e9\",\n    \"\u0623\u0631\u064a\u062f \u0623\u0646 \u0623\u0644\u0639\u0628 \u0643\u0631\u0629 \u0627\u0644\u0631\u064a\u0634\u0629\"\n]\n\ninputs=gr.inputs.Textbox(placeholder=\"Enter your text here\", label=\"Text content\", lines=5)\noutputs=gr.outputs.Label(label=\"Language detected:\")\narticle = \"\"\"\nFine-tuned on xlm-roberta-base model.\\n\nSupported languages:\\n \n    'Arabic', 'Basque', 'Breton', 'Catalan', 'Chinese_China', 'Chinese_Hongkong', 'Chinese_Taiwan', 'Chuvash', 'Czech', \n    'Dhivehi', 'Dutch', 'English', 'Esperanto', 'Estonian', 'French', 'Frisian', 'Georgian', 'German', 'Greek', 'Hakha_Chin', \n    'Indonesian', 'Interlingua', 'Italian', 'Japanese', 'Kabyle', 'Kinyarwanda', 'Kyrgyz', 'Latvian', 'Maltese', \n    'Mangolian', 'Persian', 'Polish', 'Portuguese', 'Romanian', 'Romansh_Sursilvan', 'Russian', 'Sakha', 'Slovenian', \n    'Spanish', 'Swedish', 'Tamil', 'Tatar', 'Turkish', 'Ukranian', 'Welsh'\n\"\"\"\n\ngr.Interface(\n    fn=detect_language,\n    inputs=inputs,\n    outputs=outputs,\n    verbose=True,\n    examples = examples,\n    title=\"Language Detector \ud83d\udd20\",\n    description=\"A simple interface to detect 45 languages.\",\n    article=article,\n    theme=\"huggingface\"\n).launch()\n\n```"}
{"question":"\nI have a login system that checks the provided username and password against a database. With my current code, I can validate the credentials and return an error message if the username or password is invalid.\n\nI would like to create a simple Gradio interface for the login system with the following:\n- A text input for the user to enter their username \n- A password input for the user to enter their password\n- A submit button to validate the login credentials\n- Display an \"Invalid username or password\" error message if the login fails\n","answer":"here is the gradio solution for your use case: ```python\nInvalid username or password.\n```"}
{"question":"\nI have a TAPAS (Table Parser) machine learning model that can take in natural language questions about tabular data in a CSV file and produce the answer. With my current code, I can load a CSV file, run the TAPAS model on a given query, and output the resulting answer.\n\nI would like to create an interactive Gradio web interface for my TAPAS model with the following features:\n- A text input box for the user to enter their natural language query about the CSV data\n- A file upload component for the user to provide the CSV file to query  \n- Display the model's output answer after running the query\n- Also display the full CSV data in a table after running the query\n- Provide some pre-populated example queries and CSV files for users to easily test\n- Add a title, description, and footer article with links to relevant pages\n- Configure the interface to run with Docker and enable queuing\n","answer":"here is the gradio solution for your use case: ```python\nfrom app.tapas import execute_query\nimport gradio as gr\n\n\ndef main():\n    description = \"Table query demo app, it runs TAPAS model. You can ask a question about tabular data, TAPAS model \" \\\n                  \"will produce the result. Think about it as SQL query running against DB table. The advantage of \" \\\n                  \"TAPAS model - there is no need to upload data to DB or process it in a spreadsheet, data can be \" \\\n                  \"processed in memory by ML model. Pre-trained TAPAS model runs on max 64 rows and 32 columns data. \" \\\n                  \"Make sure CSV file data doesn't exceed these dimensions.\"\n\n    article = \"<p style='text-align: center'><a href='https:\/\/katanaml.io' target='_blank'>Katana ML<\/a> | <a href='https:\/\/github.com\/katanaml\/table-query-model' target='_blank'>Github repo<\/a> | <a href='https:\/\/huggingface.co\/google\/tapas-base-finetuned-wtq' target='_blank'>TAPAS Model<\/a><\/p><center><img src='https:\/\/visitor-badge.glitch.me\/badge?page_id=abaranovskij_tablequery' alt='visitor badge'><\/center>\"\n\n    iface = gr.Interface(fn=execute_query,\n                         inputs=[gr.Textbox(label=\"Search query\"),\n                                 gr.File(label=\"CSV file\")],\n                         outputs=[gr.JSON(label=\"Result\"),\n                                  gr.Dataframe(label=\"All data\")],\n                         examples=[\n                             [\"What are the items with total higher than 8?\", \"taxables.csv\"],\n                             [\"What is the cost for Maxwell item?\", \"taxables.csv\"],\n                             [\"Show items with cost lower than 2 and tax higher than 0.05\", \"taxables.csv\"]\n                         ],\n                         title=\"Table Question Answering (TAPAS)\",\n                         description=description,\n                         article=article,\n                         allow_flagging='never')\n    # Use this config when running on Docker\n    # iface.launch(server_name=\"0.0.0.0\", server_port=7000)\n    iface.launch(enable_queue=True)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```"}
{"question":"\nI have a machine learning model from Spotify that can take song lyrics as input and predict the top genres that the song belongs to. With my current code, I can input lyrics and the model will output a dictionary mapping the predicted genre labels to their probability scores.\n\nI would like to create an interactive Gradio web interface for this song genre classification model with the following features:\n- A large text input box for the user to paste in song lyrics \n- Display the top 5 predicted genre labels as clickable buttons after submitting the lyrics\n- Include a description of the model and interface \n- Give the interface a relevant title \n- Provide 4-5 example lyrics that the user can easily load and test\n- Launch the interface so it can be used and shared\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport numpy as np\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom examples import yellow, stairway, numb, puppets, firework \n\ndef lyrics_categories(input_text):\n  spotify_model = \"juliensimon\/autonlp-song-lyrics-18753417\"\n  model = AutoModelForSequenceClassification.from_pretrained(spotify_model)\n  tokenizer = AutoTokenizer.from_pretrained(spotify_model)\n  labels = model.config.id2label\n  inputs = tokenizer(input_text, return_tensors=\"pt\")\n  outputs = model(**inputs)\n  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n  predictions = predictions.detach().numpy()[0]\n  index_sorted = np.argsort(predictions)[::-1]\n  clean_outputs = {labels[idx]:str(predictions[idx]) for idx in index_sorted}\n  print(clean_outputs)\n  return clean_outputs\n  \ndescription = \"With lyrics, find the top 5 genres this song belongs to! (Powered by Spotify)\"\n \niface = gr.Interface(fn=lyrics_categories, \n                     inputs=gr.inputs.Textbox(lines=20, placeholder=\"Enter song lyrics here...\", label=\"Song Lyrics\"),\n                     outputs=gr.outputs.Label(num_top_classes=5, label=\"Genres\/Categories\"),\n                     examples=[stairway, numb, puppets, firework, yellow],\n                     article=description,\n                     title=\"Song Genre Predictor\",\n                    )\niface.launch()\n\n```"}
{"question":"\nI have a collection of diffusion-based image generation models from the Hugging Face diffusers library. With my current code, I can generate images from a given text prompt using the model of choice.\n\nI would like to create an interactive Gradio demo to showcase these diffusion models. The demo should have the following components:\n- A dropdown to select the diffusion model to use from the available options \n- A text input box to enter the text prompt to generate an image from\n- The interface should display the generated image output\n- Provide some pre-populated example prompts that can be selected to quickly test different models\n- Include an article and description explaining the demo \n\nThe demo should handle the logic to:\n- Dynamically populate the dropdown options with the available diffusion models\n- Validate that the selected model supports the prompt input, otherwise show an error\n- Load the selected model and generate an image based on the provided prompt\n- Display the generated image in the interface\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.diffusion import (\n    DiffusersGenerationAlgorithm,\n    DDPMGenerator,\n    DDIMGenerator,\n    ScoreSdeGenerator,\n    LDMTextToImageGenerator,\n    LDMGenerator,\n    StableDiffusionGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(model_type: str, prompt: str):\n\n    if prompt == \"\":\n        config = eval(f\"{model_type}()\")\n    else:\n        config = eval(f'{model_type}(prompt=\"{prompt}\")')\n    if config.modality != \"token2image\" and prompt != \"\":\n        raise ValueError(\n            f\"{model_type} is an unconditional generative model, please remove prompt (not={prompt})\"\n        )\n    model = DiffusersGenerationAlgorithm(config)\n    image = list(model.sample(1))[0]\n\n    return image\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"Diff\" in x[\"algorithm_name\"], all_algos))\n    ]\n    algos = [a for a in algos if not \"GeoDiff\" in a]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"Diffusion-based image generators\",\n        inputs=[\n            gr.Dropdown(\n                algos, label=\"Diffusion model\", value=\"StableDiffusionGenerator\"\n            ),\n            gr.Textbox(label=\"Text prompt\", placeholder=\"A blue tree\", lines=1),\n        ],\n        outputs=gr.outputs.Image(type=\"pil\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a set of machine learning models called Patent Generative Transformers (PGT) that can perform various text generation and coherence checking tasks related to patent data. The models include:\n\n- PGTGenerator: Can generate patent abstracts from titles, titles from abstracts, claims from abstracts, and abstracts from claims\n- PGTEditor: Can edit and refine patent abstracts or claims \n- PGTCoherenceChecker: Can check the coherence between patent titles and abstracts, titles and claims, or abstracts and claims\n\nWith my current code, I can load one of these PGT models, pass it input text, and generate output text based on the selected task. The models use sampling parameters like max length, top-k and top-p to control the generation.\n\nI would like to create an interactive Gradio demo to showcase these PGT models. The demo should have:\n\n- A dropdown to select the PGT model type (Generator, Editor or Coherence Checker)\n- Dropdowns to select the specific task for each model type \n- A text input box for the primary input text\n- A secondary text input box for the Coherence Checker's second input \n- Sliders to adjust the max length, top-k and top-p sampling parameters\n- A text output box to display the generated text\n- Example preset inputs that can be loaded into the demo\n- Descriptions explaining the PGT models and demo usage\n\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.pgt import (\n    PGT,\n    PGTCoherenceChecker,\n    PGTEditor,\n    PGTGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nMODEL_FN = {\n    \"PGTGenerator\": PGTGenerator,\n    \"PGTEditor\": PGTEditor,\n    \"PGTCoherenceChecker\": PGTCoherenceChecker,\n}\n\n\ndef run_inference(\n    model_type: str,\n    generator_task: str,\n    editor_task: str,\n    checker_task: str,\n    prompt: str,\n    second_prompt: str,\n    length: int,\n    k: int,\n    p: float,\n):\n\n    kwargs = {\"max_length\": length, \"top_k\": k, \"top_p\": p}\n\n    if model_type == \"PGTGenerator\":\n        config = PGTGenerator(task=generator_task, input_text=prompt, **kwargs)\n    elif model_type == \"PGTEditor\":\n        config = PGTEditor(input_type=editor_task, input_text=prompt, **kwargs)\n    elif model_type == \"PGTCoherenceChecker\":\n        config = PGTCoherenceChecker(\n            coherence_type=checker_task, input_a=prompt, input_b=second_prompt, **kwargs\n        )\n\n    model = PGT(config)\n    text = list(model.sample(1))[0]\n\n    return text\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"PGT\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(\n        metadata_root.joinpath(\"examples.csv\"), sep=\"|\", header=None\n    ).fillna(\"\")\n    print(\"Examples: \", examples.values.tolist())\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    gen_tasks = [\n        \"title-to-abstract\",\n        \"abstract-to-title\",\n        \"abstract-to-claim\",\n        \"claim-to-abstract\",\n    ]\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"Patent Generative Transformer\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Model type\", value=\"PGTGenerator\"),\n            gr.Dropdown(gen_tasks, label=\"Generator task\", value=\"title-to-abstract\"),\n            gr.Dropdown([\"abstract\", \"claim\"], label=\"Editor task\", value=\"abstract\"),\n            gr.Dropdown(\n                [\"title-abstract\", \"title-claim\", \"abstract-claim\"],\n                label=\"Checker task\",\n                value=\"title-abstract\",\n            ),\n            gr.Textbox(\n                label=\"Primary Text prompt\",\n                placeholder=\"Artificial intelligence and machine learning infrastructure\",\n                lines=5,\n            ),\n            gr.Textbox(\n                label=\"Secondary text prompt (only coherence checker)\",\n                placeholder=\"\",\n                lines=1,\n            ),\n            gr.Slider(\n                minimum=5, maximum=1024, value=512, label=\"Maximal length\", step=1\n            ),\n            gr.Slider(minimum=2, maximum=500, value=50, label=\"Top-k\", step=1),\n            gr.Slider(minimum=0.5, maximum=1.0, value=0.95, label=\"Top-p\"),\n        ],\n        outputs=gr.Textbox(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a KeywordBERT model that can extract keywords from a given text passage. With my current code, I can input some text and the model will return the top N most relevant keywords.\n\nI would like to create an interactive Gradio demo for this KeywordBERT model with the following features:\n- A dropdown to select the specific KeywordBERT model version to use\n- A text input box for the user to paste in the passage of text to extract keywords from\n- Sliders to adjust the minimum and maximum length of keyword phrases to extract (in number of words)\n- A text input to specify custom stop words (words to ignore\/filter out)\n- Toggle switches to enable\/disable MaxSum and Max Marginal Relevance (MMR) functionality \n- If MaxSum is enabled, add a slider to control the number of MaxSum candidates\n- If MMR is enabled, add a slider to adjust the diversity parameter\n- A slider to set the total number of keywords to return\n- Display the extracted keywords as text output\n- Include an article explaining more about the KeywordBERT model \n- Show some preset example texts that can be loaded into the demo\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.conditional_generation.key_bert import (\n    KeywordBERTGenerationAlgorithm,\n    KeyBERTGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(\n    algorithm_version: str,\n    text: str,\n    minimum_keyphrase_ngram: int,\n    maximum_keyphrase_ngram: int,\n    stop_words: str,\n    use_maxsum: bool,\n    number_of_candidates: int,\n    use_mmr: bool,\n    diversity: float,\n    number_of_keywords: int,\n):\n\n    config = KeyBERTGenerator(\n        algorithm_version=algorithm_version,\n        minimum_keyphrase_ngram=minimum_keyphrase_ngram,\n        maximum_keyphrase_ngram=maximum_keyphrase_ngram,\n        stop_words=stop_words,\n        top_n=number_of_keywords,\n        use_maxsum=use_maxsum,\n        use_mmr=use_mmr,\n        diversity=diversity,\n        number_of_candidates=number_of_candidates,\n    )\n    model = KeywordBERTGenerationAlgorithm(configuration=config, target=text)\n    text = list(model.sample(number_of_keywords))\n\n    return text\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"KeywordBERT\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(\n        metadata_root.joinpath(\"examples.csv\"), sep=\",\", header=None\n    ).fillna(\"\")\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"KeywordBERT\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"circa_bert_v2\"),\n            gr.Textbox(\n                label=\"Text prompt\",\n                placeholder=\"This is a text I want to understand better\",\n                lines=5,\n            ),\n            gr.Slider(\n                minimum=1, maximum=5, value=1, label=\"Minimum keyphrase ngram\", step=1\n            ),\n            gr.Slider(\n                minimum=2, maximum=10, value=1, label=\"Maximum keyphrase ngram\", step=1\n            ),\n            gr.Textbox(label=\"Stop words\", placeholder=\"english\", lines=1),\n            gr.Radio(choices=[True, False], label=\"MaxSum\", value=False),\n            gr.Slider(\n                minimum=5, maximum=100, value=20, label=\"MaxSum candidates\", step=1\n            ),\n            gr.Radio(\n                choices=[True, False],\n                label=\"Max. marginal relevance control\",\n                value=False,\n            ),\n            gr.Slider(minimum=0.1, maximum=1, value=0.5, label=\"Diversity\"),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of keywords\", step=1\n            ),\n        ],\n        outputs=gr.Textbox(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have several pre-trained language models from Hugging Face that can take in a text prompt and generate text completions. The models include GPT-2, Transformer-XL, OpenAI GPT, XLM, XLNet, and CTRL. With my current code, I can load a specified model, pass it a prompt, and generate a text completion of a specified maximum length.\n\nI would like to create an interactive Gradio demo that allows users to experiment with the different language models and generation parameters. The interface should include:\n\n- A dropdown to select the language model to use (include model name and version)\n- A text input box for specifying the text prompt \n- Sliders to adjust the generation parameters:\n    - Max length of generated text (5-100)\n    - Decoding temperature (0.6-1.5) \n    - Top-k sampling (2-500)\n    - Top-p sampling (0.5-1.0)\n    - Repetition penalty (0.5-5.0)\n- A text input box for an optional prefix to prepend to the prompt\n- A submit button to generate the text completion\n- Display the generated text output \n- Provide some preset example prompts for users to try\n- Include a description of the demo and an informational article about the models\n\nThe demo should allow users to interactively experiment with different prompts and generation parameters to observe how they impact the language model outputs.\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.hugging_face import (\n    HuggingFaceCTRLGenerator,\n    HuggingFaceGenerationAlgorithm,\n    HuggingFaceGPT2Generator,\n    HuggingFaceTransfoXLGenerator,\n    HuggingFaceOpenAIGPTGenerator,\n    HuggingFaceXLMGenerator,\n    HuggingFaceXLNetGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nMODEL_FN = {\n    \"HuggingFaceCTRLGenerator\": HuggingFaceCTRLGenerator,\n    \"HuggingFaceGPT2Generator\": HuggingFaceGPT2Generator,\n    \"HuggingFaceTransfoXLGenerator\": HuggingFaceTransfoXLGenerator,\n    \"HuggingFaceOpenAIGPTGenerator\": HuggingFaceOpenAIGPTGenerator,\n    \"HuggingFaceXLMGenerator\": HuggingFaceXLMGenerator,\n    \"HuggingFaceXLNetGenerator\": HuggingFaceXLNetGenerator,\n}\n\n\ndef run_inference(\n    model_type: str,\n    prompt: str,\n    length: float,\n    temperature: float,\n    prefix: str,\n    k: float,\n    p: float,\n    repetition_penalty: float,\n):\n    model = model_type.split(\"_\")[0]\n    version = model_type.split(\"_\")[1]\n\n    if model not in MODEL_FN.keys():\n        raise ValueError(f\"Model type {model} not supported\")\n    config = MODEL_FN[model](\n        algorithm_version=version,\n        prompt=prompt,\n        length=length,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        k=k,\n        p=p,\n        prefix=prefix,\n    )\n\n    model = HuggingFaceGenerationAlgorithm(config)\n    text = list(model.sample(1))[0]\n\n    return text\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_application\"] + \"_\" + x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"HuggingFace\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n    print(\"Examples: \", examples.values.tolist())\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"HuggingFace language models\",\n        inputs=[\n            gr.Dropdown(\n                algos,\n                label=\"Language model\",\n                value=\"HuggingFaceGPT2Generator_gpt2\",\n            ),\n            gr.Textbox(\n                label=\"Text prompt\",\n                placeholder=\"I'm a stochastic parrot.\",\n                lines=1,\n            ),\n            gr.Slider(minimum=5, maximum=100, value=20, label=\"Maximal length\", step=1),\n            gr.Slider(\n                minimum=0.6, maximum=1.5, value=1.1, label=\"Decoding temperature\"\n            ),\n            gr.Textbox(\n                label=\"Prefix\", placeholder=\"Some prefix (before the prompt)\", lines=1\n            ),\n            gr.Slider(minimum=2, maximum=500, value=50, label=\"Top-k\", step=1),\n            gr.Slider(minimum=0.5, maximum=1, value=1.0, label=\"Decoding-p\", step=1),\n            gr.Slider(minimum=0.5, maximum=5, value=1.0, label=\"Repetition penalty\"),\n        ],\n        outputs=gr.Textbox(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have an English-to-Russian translation model and the ruDALL-E text-to-image generation model. With my current code, I can take an English text prompt, translate it to Russian, feed it into ruDALL-E to generate an image, and output the translated Russian prompt and generated image.\n\nI would like to create a Gradio web interface to demonstrate this pipeline, with the following features:\n- A text input box for the user to enter an English prompt \n- An interface that first shows the output of translating the English prompt to Russian\n- Passing the translated Russian prompt to a second interface that generates and displays an image from the prompt\n- Allow generating multiple images from the same prompt by resubmitting \n- Show the interfaces in a horizontal layout \n- Add a description of the demo, links to more info, and some example prompts to try\n- Enable the Hugging Face theme\n- Turn on queuing of requests\n","answer":"here is the gradio solution for your use case: ```python\nimport random\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom functools import partialmethod\nimport gradio as gr\nfrom gradio.mix import Series\nfrom transformers import pipeline, FSMTForConditionalGeneration, FSMTTokenizer\nfrom rudalle.pipelines import generate_images\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae\n\n# disable tqdm logging from the rudalle pipeline\ntqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntranslation_model = FSMTForConditionalGeneration.from_pretrained(\"facebook\/wmt19-en-ru\", torch_dtype=torch.float16).half().to(device)\ntranslation_tokenizer = FSMTTokenizer.from_pretrained(\"facebook\/wmt19-en-ru\")\ndalle = get_rudalle_model(\"Malevich\", pretrained=True, fp16=True, device=device)\ntokenizer = get_tokenizer()\nvae = get_vae().to(device)\n\ndef translation_wrapper(text: str):\n    input_ids = translation_tokenizer.encode(text, return_tensors=\"pt\")\n    outputs = translation_model.generate(input_ids.to(device))\n    decoded = translation_tokenizer.decode(outputs[0].float(), skip_special_tokens=True)\n    return decoded\n\ndef dalle_wrapper(prompt: str):\n    top_k, top_p = random.choice([\n        (1024, 0.98),\n        (512, 0.97),\n        (384, 0.96),\n    ])\n    \n    images , _ = generate_images(\n        prompt, \n        tokenizer, \n        dalle, \n        vae, \n        top_k=top_k, \n        images_num=1, \n        top_p=top_p\n    )\n    title = f\"<b>{prompt}<\/b>\"\n    return title, images[0]\n\n\ntranslator = gr.Interface(fn=translation_wrapper, \n                          inputs=[gr.inputs.Textbox(label='What would you like to see?')],\n                          outputs=\"text\")\noutputs = [\n    gr.outputs.HTML(label=\"\"),   \n    gr.outputs.Image(label=\"\"),\n]\ngenerator = gr.Interface(fn=dalle_wrapper, inputs=\"text\", outputs=outputs)\n\n\ndescription = (\n    \"ruDALL-E is a 1.3B params text-to-image model by SberAI (links at the bottom). \"\n    \"This demo uses an English-Russian translation model to adapt the prompts. \"\n    \"Try pressing [Submit] multiple times to generate new images!\"\n)\narticle = (\n    \"<p style='text-align: center'>\"\n    \"<a href='https:\/\/github.com\/sberbank-ai\/ru-dalle'>GitHub<\/a> | \"\n    \"<a href='https:\/\/habr.com\/ru\/company\/sberbank\/blog\/586926\/'>Article (in Russian)<\/a>\"\n    \"<\/p>\"\n)\nexamples = [[\"A still life of grapes and a bottle of wine\"], \n            [\"\u0413\u043e\u0440\u043e\u0434 \u0432 \u0441\u0442\u0438\u043b\u0435 \u043a\u0438\u0431\u0435\u0440\u043f\u0430\u043d\u043a\"], \n            [\"A colorful photo of a coral reef\"], \n            [\"A white cat sitting in a cardboard box\"]]\n            \nseries = Series(translator, generator, \n                title='Kinda-English ruDALL-E',\n                description=description,\n                article=article,\n                layout='horizontal',\n                theme='huggingface',\n                examples=examples,\n                allow_flagging=False,\n                live=False, \n                enable_queue=True,\n               )\nseries.launch()\n\n\n```"}
{"question":"\nI have two pre-trained AnimeGANv2 models that can take an input portrait image and transform it into an anime style portrait. The two models trade off between better stylization vs more robustness\/consistency.\n\nWith my current code, I can load the pre-trained models, run inference on an input image with either model, and output the generated anime portrait.\n\nI would like to create an interactive Gradio web demo showcasing this AnimeGANv2 portrait stylization. The Gradio app should have the following features:\n\n- Allow the user to upload their own portrait image \n- Have a radio button input to select between the two model versions\n- Display the generated anime portrait after submitting the image\n- Include example images that can be selected to test the models\n- Show a description of the app, links to the model repo, and a visitor counter\n- Disable flagging and screenshot features\n","answer":"here is the gradio solution for your use case: ```python\nfrom PIL import Image\nimport torch\nimport gradio as gr\n\n\n\nmodel2 = torch.hub.load(\n    \"AK391\/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    device=\"cuda\",\n    progress=False\n)\n\n\nmodel1 = torch.hub.load(\"AK391\/animegan2-pytorch:main\", \"generator\", pretrained=\"face_paint_512_v1\",  device=\"cuda\")\nface2paint = torch.hub.load(\n    'AK391\/animegan2-pytorch:main', 'face2paint', \n    size=512, device=\"cuda\",side_by_side=False\n)\ndef inference(img, ver):\n    if ver == 'version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)':\n        out = face2paint(model2, img)\n    else:\n        out = face2paint(model1, img)\n    return out\n  \ntitle = \"AnimeGANv2\"\ndescription = \"Gradio Demo for AnimeGanv2 Face Portrait. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below. Please use a cropped portrait picture for best results similar to the examples below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/bryandlee\/animegan2-pytorch' target='_blank'>Github Repo Pytorch<\/a><\/p> <center><img src='https:\/\/visitor-badge.glitch.me\/badge?page_id=akhaliq_animegan' alt='visitor badge'><\/center><\/p>\"\nexamples=[['groot.jpeg','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['bill.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['tony.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['elon.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['IU.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['billie.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['will.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['beyonce.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['gongyoo.jpeg','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)']]\ngr.Interface(inference, [gr.inputs.Image(type=\"pil\"),gr.inputs.Radio(['version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'], type=\"value\", default='version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)', label='version')\n], gr.outputs.Image(type=\"pil\"),title=title,description=description,article=article,examples=examples,allow_flagging=False,allow_screenshot=False).launch()\n```"}
{"question":"\nI have a computer vision model for multi-object tracking called ByteTrack. Given an input image, my current code can run object detection, track the detected objects across frames, and output an image with the tracked objects labeled.\n\nI would like to create an interactive Gradio demo to showcase the ByteTrack model, with the following features:\n- Allow the user to upload an image file \n- Run the ByteTrack model inference on the uploaded image\n- Display the output image with the tracked objects labeled\n- Provide some example images that can be loaded into the demo\n- Show a title, description, and links to the ByteTrack paper and code repository\n- Add the ability for multiple users to queue their submissions\n","answer":"here is the gradio solution for your use case: ```python\nimport os\nimport gradio as gr\nos.system(\"pip3 install cython_bbox gdown 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\")\nos.system(\"pip3 install -v -e .\")\nos.system(\"gdown https:\/\/drive.google.com\/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5\")\n\ndef inference(image):\n  os.system(\"python3 tools\/demo_track.py image -f exps\/example\/mot\/yolox_x_mix_det.py -c .\/bytetrack_x_mot17.pth.tar --save_result --device 'cpu' --path \"+image.name)\n  return \"out.jpg\"\n  \n\ntitle = \"Bytetrack\"\ndescription = \"Gradio demo for ByteTrack: Multi-Object Tracking by Associating Every Detection Box. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/arxiv.org\/abs\/2110.06864' target='_blank'> ByteTrack: Multi-Object Tracking by Associating Every Detection Box <\/a> | <a href='https:\/\/github.com\/ifzhang\/ByteTrack' target='_blank'>Github Repo<\/a><\/p>\"\n\nexamples=[['paint.jpeg']]\ngr.Interface(\n    inference, \n    gr.inputs.Image(type=\"file\"), \n    gr.outputs.Image(type=\"file\"),\n    title=title,\n    description=description,\n    article=article,\n    enable_queue=True,\n    examples=examples\n    ).launch(debug=True)\n```"}
{"question":"\nI have an anime face detection model that can detect faces and facial landmarks in anime-style images. With my current code, I can load an image, run the face detection, and draw bounding boxes around the detected faces and plot the facial landmark points.\n\nI would like to create an interactive Gradio demo for my anime face detector model with the following features:\n- Allow the user to upload an image or select from a set of sample images\n- Provide a dropdown to select between two different detector model architectures (Faster R-CNN and YOLOv3) \n- Include sliders to adjust the confidence score thresholds for filtering the face detections and landmark points\n- Display the output image with the face bounding boxes and landmark points drawn\n- Show multiple examples of different anime images and optimal threshold settings\n- Add a title, description, and article to the interface to explain the demo\n\nSome key Gradio components I want to use:\n- Image input component for uploading\/selecting images\n- Radio component for selecting the detector model \n- Slider components for the confidence thresholds\n- Image output component to display the processed image\n- Examples to show multiple sample inputs\/outputs\n- Interpretation to provide descriptions and article about the demo\n","answer":"here is the gradio solution for your use case: ```python\n#!\/usr\/bin\/env python\n\nfrom __future__ import annotations\n\nimport argparse\nimport functools\nimport os\nimport pathlib\nimport subprocess\nimport tarfile\n\nif os.environ.get('SYSTEM') == 'spaces':\n    import mim\n\n    mim.uninstall('mmcv-full', confirm_yes=True)\n    mim.install('mmcv-full==1.3.16', is_yes=True)\n\n    subprocess.call('pip uninstall -y opencv-python'.split())\n    subprocess.call('pip uninstall -y opencv-python-headless'.split())\n    subprocess.call('pip install opencv-python-headless'.split())\n\nimport anime_face_detector\nimport cv2\nimport gradio as gr\nimport huggingface_hub\nimport numpy as np\nimport torch\n\nTITLE = 'hysts\/anime-face-detector'\nDESCRIPTION = 'This is a demo for https:\/\/github.com\/hysts\/anime-face-detector.'\nARTICLE = '<center><img src=\"https:\/\/visitor-badge.glitch.me\/badge?page_id=hysts.anime-face-detector\" alt=\"visitor badge\"\/><\/center>'\n\nTOKEN = os.environ['TOKEN']\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', type=str, default='cpu')\n    parser.add_argument('--theme', type=str)\n    parser.add_argument('--live', action='store_true')\n    parser.add_argument('--share', action='store_true')\n    parser.add_argument('--port', type=int)\n    parser.add_argument('--disable-queue',\n                        dest='enable_queue',\n                        action='store_false')\n    parser.add_argument('--allow-flagging', type=str, default='never')\n    parser.add_argument('--allow-screenshot', action='store_true')\n    return parser.parse_args()\n\n\ndef load_sample_image_paths() -> list[pathlib.Path]:\n    image_dir = pathlib.Path('images')\n    if not image_dir.exists():\n        dataset_repo = 'hysts\/sample-images-TADNE'\n        path = huggingface_hub.hf_hub_download(dataset_repo,\n                                               'images.tar.gz',\n                                               repo_type='dataset',\n                                               use_auth_token=TOKEN)\n        with tarfile.open(path) as f:\n            f.extractall()\n    return sorted(image_dir.glob('*'))\n\n\ndef detect(\n        image: np.ndarray, detector_name: str, face_score_threshold: float,\n        landmark_score_threshold: float,\n        detectors: dict[str,\n                        anime_face_detector.LandmarkDetector]) -> np.ndarray:\n    detector = detectors[detector_name]\n    # RGB -> BGR\n    image = image[:, :, ::-1]\n    preds = detector(image)\n\n    res = image.copy()\n    for pred in preds:\n        box = pred['bbox']\n        box, score = box[:4], box[4]\n        if score < face_score_threshold:\n            continue\n        box = np.round(box).astype(int)\n\n        line_width = max(2, int(3 * (box[2:] - box[:2]).max() \/ 256))\n\n        cv2.rectangle(res, tuple(box[:2]), tuple(box[2:]), (0, 255, 0),\n                      line_width)\n\n        pred_pts = pred['keypoints']\n        for *pt, score in pred_pts:\n            if score < landmark_score_threshold:\n                color = (0, 255, 255)\n            else:\n                color = (0, 0, 255)\n            pt = np.round(pt).astype(int)\n            cv2.circle(res, tuple(pt), line_width, color, cv2.FILLED)\n    return res[:, :, ::-1]\n\n\ndef main():\n    args = parse_args()\n    device = torch.device(args.device)\n\n    detector_names = ['faster-rcnn', 'yolov3']\n    detectors = {\n        detector_name: anime_face_detector.create_detector(detector_name,\n                                                           device=device)\n        for detector_name in detector_names\n    }\n\n    func = functools.partial(detect, detectors=detectors)\n    func = functools.update_wrapper(func, detect)\n\n    image_paths = load_sample_image_paths()\n    examples = [[path.as_posix(), 'yolov3', 0.5, 0.3] for path in image_paths]\n\n    gr.Interface(\n        func,\n        [\n            gr.inputs.Image(type='numpy', label='Input'),\n            gr.inputs.Radio(detector_names,\n                            type='value',\n                            default='yolov3',\n                            label='Detector'),\n            gr.inputs.Slider(\n                0, 1, step=0.05, default=0.5, label='Face Score Threshold'),\n            gr.inputs.Slider(\n                0, 1, step=0.05, default=0.3,\n                label='Landmark Score Threshold'),\n        ],\n        gr.outputs.Image(type='numpy', label='Output'),\n        examples=examples,\n        title=TITLE,\n        description=DESCRIPTION,\n        article=ARTICLE,\n        theme=args.theme,\n        allow_screenshot=args.allow_screenshot,\n        allow_flagging=args.allow_flagging,\n        live=args.live,\n    ).launch(\n        enable_queue=args.enable_queue,\n        server_port=args.port,\n        share=args.share,\n    )\n\n\nif __name__ == '__main__':\n    main()\n\n```"}
{"question":"\nI have a collection of state-of-the-art image super-resolution models that can upscale images by 2x, 3x or 4x. With my current code, I can load a pretrained model, take an input image, run super-resolution on it, and output the upscaled image.\n\nI would like to create an interactive Gradio web interface to demonstrate the super-resolution models with the following features:\n- Allow the user to upload an image \n- Let the user select the upscaling factor (2x, 3x, 4x) with radio buttons\n- Let the user choose the super-resolution model from a dropdown of available models\n- Display the output upscaled image after running super-resolution\n- Include some example images the user can quickly test\n- Provide info and links to the model documentation and code\n","answer":"here is the gradio solution for your use case: ```python\nimport cv2\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nfrom super_image import ImageLoader, EdsrModel, MsrnModel, MdsrModel, AwsrnModel, A2nModel, CarnModel, PanModel, \\\n    HanModel, DrlnModel, RcanModel\n\ntitle = \"super-image\"\ndescription = \"State of the Art Image Super-Resolution Models.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/eugenesiow\/super-image'>Github Repo<\/a>\" \\\n          \"| <a href='https:\/\/eugenesiow.github.io\/super-image\/'>Documentation<\/a> \" \\\n          \"| <a href='https:\/\/github.com\/eugenesiow\/super-image#scale-x2'>Models<\/a><\/p>\"\n\n\ndef get_model(model_name, scale):\n    if model_name == 'EDSR':\n        model = EdsrModel.from_pretrained('eugenesiow\/edsr', scale=scale)\n    elif model_name == 'MSRN':\n        model = MsrnModel.from_pretrained('eugenesiow\/msrn', scale=scale)\n    elif model_name == 'MDSR':\n        model = MdsrModel.from_pretrained('eugenesiow\/mdsr', scale=scale)\n    elif model_name == 'AWSRN-BAM':\n        model = AwsrnModel.from_pretrained('eugenesiow\/awsrn-bam', scale=scale)\n    elif model_name == 'A2N':\n        model = A2nModel.from_pretrained('eugenesiow\/a2n', scale=scale)\n    elif model_name == 'CARN':\n        model = CarnModel.from_pretrained('eugenesiow\/carn', scale=scale)\n    elif model_name == 'PAN':\n        model = PanModel.from_pretrained('eugenesiow\/pan', scale=scale)\n    elif model_name == 'HAN':\n        model = HanModel.from_pretrained('eugenesiow\/han', scale=scale)\n    elif model_name == 'DRLN':\n        model = DrlnModel.from_pretrained('eugenesiow\/drln', scale=scale)\n    elif model_name == 'RCAN':\n        model = RcanModel.from_pretrained('eugenesiow\/rcan', scale=scale)\n    else:\n        model = EdsrModel.from_pretrained('eugenesiow\/edsr-base', scale=scale)\n    return model\n\n\ndef inference(img, scale_str, model_name):\n    max_res = 1024\n    scale = int(scale_str.replace('x', ''))\n    width, height = img.size\n    print(width, height)\n    if width > max_res or height > max_res:\n        img = img.thumbnail((max_res, max_res), Image.ANTIALIAS)\n    model = get_model(model_name, scale)\n    try:\n        inputs = ImageLoader.load_image(img)\n        preds = model(inputs)\n        preds = preds.data.cpu().numpy()\n        pred = preds[0].transpose((1, 2, 0)) * 255.0\n        return Image.fromarray(pred.astype('uint8'), 'RGB')\n    except Exception as e:\n        print(e)\n        return None\n\n\ntorch.hub.download_url_to_file('http:\/\/people.rennes.inria.fr\/Aline.Roumy\/results\/images_SR_BMVC12\/input_groundtruth\/baby_mini_d3_gaussian.bmp',\n                               'baby.bmp')\ntorch.hub.download_url_to_file('http:\/\/people.rennes.inria.fr\/Aline.Roumy\/results\/images_SR_BMVC12\/input_groundtruth\/woman_mini_d3_gaussian.bmp',\n                               'woman.bmp')\ntorch.hub.download_url_to_file('http:\/\/people.rennes.inria.fr\/Aline.Roumy\/results\/images_SR_BMVC12\/input_groundtruth\/bird_mini_d4_gaussian.bmp',\n                               'bird.bmp')\n\n# models = ['EDSR-base', 'DRLN', 'EDSR', 'MDSR', 'A2N', 'PAN', 'AWSRN-BAM', 'MSRN']\nmodels = ['EDSR-base', 'A2N', 'PAN', 'AWSRN-BAM', 'MSRN']\nscales = [2, 3, 4]\nfor model_name in models:\n    for scale in scales:\n        get_model(model_name, scale)\n\ngr.Interface(\n    inference,\n    [\n        gr.inputs.Image(type=\"pil\", label=\"Input\"),\n        gr.inputs.Radio([\"x2\", \"x3\", \"x4\"], label='scale'),\n        gr.inputs.Dropdown(choices=models,\n                           label='Model')\n    ],\n    gr.outputs.Image(type=\"pil\", label=\"Output\"),\n    title=title,\n    description=description,\n    article=article,\n    examples=[\n        ['baby.bmp', 'x2', 'EDSR-base'],\n        ['woman.bmp', 'x3', 'MSRN'],\n        ['bird.bmp', 'x4', 'PAN']\n    ],\n    allow_flagging='never',\n    ).launch(debug=False)\n\n```"}
{"question":"\nI have a machine learning model trained on the Oxford Pets dataset using fastai that can classify different pet breeds from images. With my current code, I can input an image and the model will output the predicted breed probabilities.\n\nI would like to create an interactive Gradio demo to showcase this pet breed classifier model, with the following features:\n- Allow the user to upload an image, which will be the input to the classifier \n- Display the top 3 predicted breeds and their probabilities as labels\n- Use a default image size of 512x512 for the input\n- Add a title \"Pet Breed Classifier\" and description explaining the model \n- Include an article link to a blog post with more information about the demo\n- Provide some example images that can be loaded into the interface\n- Enable the default interpretation to understand what parts of the image influence the predictions\n- Turn on the queuing option to handle multiple users\n- Launch the interface to make it publicly accessible\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nfrom fastai.vision.all import *\nimport skimage\n\nlearn = load_learner('export.pkl')\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n \ntitle = \"Pet Breed Classifier\"\ndescription = \"A pet breed classifier trained on the Oxford Pets dataset with fastai. Created as a demo for Gradio and HuggingFace Spaces.\"\narticle=\"<p style='text-align: center'><a href='https:\/\/tmabraham.github.io\/blog\/gradio_hf_spaces_tutorial' target='_blank'>Blog post<\/a><\/p>\"\nexamples = ['siamese.jpg']\ninterpretation='default'\nenable_queue=True\n\ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),title=title,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()\n\n```"}
{"question":"\nI have a LayoutLMv2 model from Microsoft that has been fine-tuned on the FUNSD dataset for document understanding. The model can take in an image of a form\/document and identify the text elements as question, answer, header or other. With my current code, I can run the model on an input image and it will draw colored bounding boxes around the text elements and label them.\n\nI would like to create an interactive Gradio demo to showcase this model. The demo should have the following:\n\n- Allow the user to upload their own document image \n- Also provide an example document image that the user can easily test out\n- Display the input image on the left side of the interface\n- When the user clicks Submit, run the model on the input image and display the annotated results image on the right side\n- Since processing might take a few seconds, enable queuing to handle multiple requests\n- Provide a title, description, and link to the research paper and Github repo for the model\n- Make sure the input and output images are large enough to see the details and annotations clearly\n\n","answer":"here is the gradio solution for your use case: ```python\nimport os\nos.system('pip install pyyaml==5.1')\n# workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https:\/\/github.com\/facebookresearch\/detectron2\/issues\/3158)\nos.system('pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html')\n\n# install detectron2 that matches pytorch 1.8\n# See https:\/\/detectron2.readthedocs.io\/tutorials\/install.html for instructions\nos.system('pip install -q detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu101\/torch1.8\/index.html')\n\n## install PyTesseract\nos.system('pip install -q pytesseract')\n\nimport gradio as gr\nimport numpy as np\nfrom transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification\nfrom datasets import load_dataset\nfrom PIL import Image, ImageDraw, ImageFont\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft\/layoutlmv2-base-uncased\")\nmodel = LayoutLMv2ForTokenClassification.from_pretrained(\"nielsr\/layoutlmv2-finetuned-funsd\")\n\n# load image example\ndataset = load_dataset(\"nielsr\/funsd\", split=\"test\")\nimage = Image.open(dataset[0][\"image_path\"]).convert(\"RGB\")\nimage = Image.open(\".\/invoice.png\")\nimage.save(\"document.png\")\n# define id2label, label2color\nlabels = dataset.features['ner_tags'].feature.names\nid2label = {v: k for v, k in enumerate(labels)}\nlabel2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n\ndef unnormalize_box(bbox, width, height):\n     return [\n         width * (bbox[0] \/ 1000),\n         height * (bbox[1] \/ 1000),\n         width * (bbox[2] \/ 1000),\n         height * (bbox[3] \/ 1000),\n     ]\n\ndef iob_to_label(label):\n    label = label[2:]\n    if not label:\n      return 'other'\n    return label\n\ndef process_image(image):\n    width, height = image.size\n\n    # encode\n    encoding = processor(image, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\")\n    offset_mapping = encoding.pop('offset_mapping')\n\n    # forward pass\n    outputs = model(**encoding)\n\n    # get predictions\n    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n    token_boxes = encoding.bbox.squeeze().tolist()\n\n    # only keep non-subword predictions\n    is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0\n    true_predictions = [id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]]\n    true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes) if not is_subword[idx]]\n\n    # draw predictions over the image\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    for prediction, box in zip(true_predictions, true_boxes):\n        predicted_label = iob_to_label(prediction).lower()\n        draw.rectangle(box, outline=label2color[predicted_label])\n        draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font)\n    \n    return image\n\n\ntitle = \"Interactive demo: LayoutLMv2\"\ndescription = \"Demo for Microsoft's LayoutLMv2, a Transformer for state-of-the-art document image understanding tasks. This particular model is fine-tuned on FUNSD, a dataset of manually annotated forms. It annotates the words appearing in the image as QUESTION\/ANSWER\/HEADER\/OTHER. To use it, simply upload an image or use the example image below and click 'Submit'. Results will show up in a few seconds. If you want to make the output bigger, right-click on it and select 'Open image in new tab'.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/arxiv.org\/abs\/2012.14740' target='_blank'>LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding<\/a> | <a href='https:\/\/github.com\/microsoft\/unilm' target='_blank'>Github Repo<\/a><\/p>\"\nexamples =[['document.png']]\n\ncss = \".output-image, .input-image {height: 40rem !important; width: 100% !important;}\"\n#css = \"@media screen and (max-width: 600px) { .output_image, .input_image {height:20rem !important; width: 100% !important;} }\"\n# css = \".output_image, .input_image {height: 600px !important}\"\n\ncss = \".image-preview {height: auto !important;}\"\n\niface = gr.Interface(fn=process_image, \n                     inputs=gr.inputs.Image(type=\"pil\"), \n                     outputs=gr.outputs.Image(type=\"pil\", label=\"annotated image\"),\n                     title=title,\n                     description=description,\n                     article=article,\n                     examples=examples,\n                     css=css,\n                     enable_queue=True)\niface.launch(debug=True)\n```"}
{"question":"\nI have a YOLOv5 object detection model that can detect objects in images and videos. With my current code, I can load the YOLOv5 model, run inference on an input image or video, and get the detection results which include the bounding box coordinates, class labels, and confidence scores for each detected object.\n\nI would like to create an interactive Gradio web interface for my YOLOv5 model with the following features:\n- Allow the user to select between image mode and video mode using a tabbed interface \n- For image mode:\n  - Have an image upload component to input an image \n  - Allow the user to select model parameters like device, model size, confidence threshold, IoU threshold, max detections, and classes to detect\n  - Display the output image with bounding boxes and labels drawn around detected objects\n  - Show cropped images of each detected object \n  - Display detection details in a paginated dataframe\n  - Allow exporting results to PDF, JSON, CSV, Excel formats\n  - Show summary statistics on object sizes and class proportions\n- For video mode:  \n  - Have a video upload component that also supports webcam input\n  - Allow selecting similar model parameters as image mode\n  - Display the output video with detections\n- Include example images\/videos to test easily\n- Add project description, author information, links etc.\n- Optionally allow authenticating with a username\/password\n","answer":"here is the gradio solution for your use case: ```python\n# Gradio YOLOv5 Det v0.4\n# author: Zeng Yifu\uff08\u66fe\u9038\u592b\uff09\n# creation time: 2022-05-28\n# email: zyfiy1314@163.com\n# project homepage: https:\/\/gitee.com\/CV_Lab\/gradio_yolov5_det\n\nimport argparse\nimport sys\nimport csv\ncsv.field_size_limit(sys.maxsize)\n\nimport gc\nimport json\nimport os\nfrom collections import Counter\nfrom pathlib import Path\n\nimport cv2\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nimport torch\nimport yaml\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom util.fonts_opt import is_fonts\nfrom util.pdf_opt import pdf_generate\n\nROOT_PATH = sys.path[0]  # root directory\n\n# model path\nmodel_path = \"ultralytics\/yolov5\"\n\n# Gradio YOLOv5 Det version\nGYD_VERSION = \"Gradio YOLOv5 Det v0.4\"\n\n# model name temporary variable\nmodel_name_tmp = \"\"\n\n# Device temporary variables\ndevice_tmp = \"\"\n\n# File extension\nsuffix_list = [\".csv\", \".yaml\"]\n\n# font size\nFONTSIZE = 25\n\n# object style\nobj_style = [\"Small Object\", \"Medium Object\", \"Large Object\"]\n\n\ndef parse_args(known=False):\n    parser = argparse.ArgumentParser(description=\"Gradio YOLOv5 Det v0.4\")\n    parser.add_argument(\"--source\", \"-src\", default=\"upload\", type=str, help=\"input source\")\n    parser.add_argument(\"--source_video\", \"-src_v\", default=\"webcam\", type=str, help=\"video input source\")\n    parser.add_argument(\"--img_tool\", \"-it\", default=\"editor\", type=str, help=\"input image tool\")\n    parser.add_argument(\"--model_name\", \"-mn\", default=\"yolov5s\", type=str, help=\"model name\")\n    parser.add_argument(\n        \"--model_cfg\",\n        \"-mc\",\n        default=\".\/model_config\/model_name_p5_p6_all.yaml\",\n        type=str,\n        help=\"model config\",\n    )\n    parser.add_argument(\n        \"--cls_name\",\n        \"-cls\",\n        default=\".\/cls_name\/cls_name_en.yaml\",\n        type=str,\n        help=\"cls name\",\n    )\n    parser.add_argument(\n        \"--nms_conf\",\n        \"-conf\",\n        default=0.5,\n        type=float,\n        help=\"model NMS confidence threshold\",\n    )\n    parser.add_argument(\"--nms_iou\", \"-iou\", default=0.45, type=float, help=\"model NMS IoU threshold\")\n    parser.add_argument(\n        \"--device\",\n        \"-dev\",\n        default=\"cpu\",\n        type=str,\n        help=\"cuda or cpu\",\n    )\n    parser.add_argument(\"--inference_size\", \"-isz\", default=640, type=int, help=\"model inference size\")\n    parser.add_argument(\"--max_detnum\", \"-mdn\", default=50, type=float, help=\"model max det num\")\n    parser.add_argument(\"--slider_step\", \"-ss\", default=0.05, type=float, help=\"slider step\")\n    parser.add_argument(\n        \"--is_login\",\n        \"-isl\",\n        action=\"store_true\",\n        default=False,\n        help=\"is login\",\n    )\n    parser.add_argument('--usr_pwd',\n                        \"-up\",\n                        nargs='+',\n                        type=str,\n                        default=[\"admin\", \"admin\"],\n                        help=\"user & password for login\")\n    parser.add_argument(\n        \"--is_share\",\n        \"-is\",\n        action=\"store_true\",\n        default=False,\n        help=\"is login\",\n    )\n\n    args = parser.parse_known_args()[0] if known else parser.parse_args()\n    return args\n\n\n# yaml file parsing\ndef yaml_parse(file_path):\n    return yaml.safe_load(open(file_path, encoding=\"utf-8\").read())\n\n\n# yaml csv file parsing\ndef yaml_csv(file_path, file_tag):\n    file_suffix = Path(file_path).suffix\n    if file_suffix == suffix_list[0]:\n        # model name\n        file_names = [i[0] for i in list(csv.reader(open(file_path)))]  # csv version\n    elif file_suffix == suffix_list[1]:\n        # model name\n        file_names = yaml_parse(file_path).get(file_tag)  # yaml version\n    else:\n        print(f\"{file_path} is not in the correct format! Program exits!\")\n        sys.exit()\n\n    return file_names\n\n\n# model loading\ndef model_loading(model_name, device, opt=[]):\n\n    # \u52a0\u8f7d\u672c\u5730\u6a21\u578b\n    try:\n        # load model\n        model = torch.hub.load(model_path,\n                               model_name,\n                               force_reload=[True if \"refresh_yolov5\" in opt else False][0],\n                               device=device,\n                               _verbose=False)\n    except Exception as e:\n        print(e)\n    else:\n        print(f\"\ud83d\ude80 welcome to {GYD_VERSION}\uff0c{model_name} loaded successfully!\")\n\n    return model\n\n\n# check information\ndef export_json(results, img_size):\n\n    return [[{\n        \"ID\": i,\n        \"CLASS\": int(result[i][5]),\n        \"CLASS_NAME\": model_cls_name_cp[int(result[i][5])],\n        \"BOUNDING_BOX\": {\n            \"XMIN\": round(result[i][:4].tolist()[0], 6),\n            \"YMIN\": round(result[i][:4].tolist()[1], 6),\n            \"XMAX\": round(result[i][:4].tolist()[2], 6),\n            \"YMAX\": round(result[i][:4].tolist()[3], 6),},\n        \"CONF\": round(float(result[i][4]), 2),\n        \"FPS\": round(1000 \/ float(results.t[1]), 2),\n        \"IMG_WIDTH\": img_size[0],\n        \"IMG_HEIGHT\": img_size[1],} for i in range(len(result))] for result in results.xyxyn]\n\n\n# frame conversion\ndef pil_draw(img, countdown_msg, textFont, xyxy, font_size, opt, obj_cls_index, color_list):\n\n    img_pil = ImageDraw.Draw(img)\n\n    img_pil.rectangle(xyxy, fill=None, outline=color_list[obj_cls_index])  # bounding box\n\n    if \"label\" in opt:\n        text_w, text_h = textFont.getsize(countdown_msg)  # Label size\n\n        img_pil.rectangle(\n            (xyxy[0], xyxy[1], xyxy[0] + text_w, xyxy[1] + text_h),\n            fill=color_list[obj_cls_index],\n            outline=color_list[obj_cls_index],\n        )  # label background\n\n        img_pil.multiline_text(\n            (xyxy[0], xyxy[1]),\n            countdown_msg,\n            fill=(255, 255, 255),\n            font=textFont,\n            align=\"center\",\n        )\n\n    return img\n\n\n# Label and bounding box color settings\ndef color_set(cls_num):\n    color_list = []\n    for i in range(cls_num):\n        color = tuple(np.random.choice(range(256), size=3))\n        # color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])]\n        color_list.append(color)\n\n    return color_list\n\n\n# YOLOv5 image detection function\ndef yolo_det_img(img, device, model_name, infer_size, conf, iou, max_num, model_cls, opt):\n\n    global model, model_name_tmp, device_tmp\n\n    # object size num\n    s_obj, m_obj, l_obj = 0, 0, 0\n    # object area list\n    area_obj_all = []\n    # cls num stat\n    cls_det_stat = []\n\n    if model_name_tmp != model_name:\n        # Model judgment to avoid repeated loading\n        model_name_tmp = model_name\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n    elif device_tmp != device:\n        # Device judgment to avoid repeated loading\n        device_tmp = device\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n    else:\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n\n    # -------------Model tuning -------------\n    model.conf = conf  # NMS confidence threshold\n    model.iou = iou  # NMS IoU threshold\n    model.max_det = int(max_num)  # Maximum number of detection frames\n    model.classes = model_cls  # model classes\n\n    color_list = color_set(len(model_cls_name_cp))  # \u8bbe\u7f6e\u989c\u8272\n\n    img_size = img.size  # frame size\n\n    results = model(img, size=infer_size)  # detection\n\n    # ----------------\u76ee\u6807\u88c1\u526a----------------\n    crops = results.crop(save=False)\n    img_crops = []\n    for i in range(len(crops)):\n        img_crops.append(crops[i][\"im\"][..., ::-1])\n\n    # Data Frame\n    dataframe = results.pandas().xyxy[0].round(2)\n\n    det_csv = \".\/Det_Report.csv\"\n    det_excel = \".\/Det_Report.xlsx\"\n\n    if \"csv\" in opt:\n        dataframe.to_csv(det_csv, index=False)\n    else:\n        det_csv = None\n\n    if \"excel\" in opt:\n        dataframe.to_excel(det_excel, sheet_name='sheet1', index=False)\n    else:\n        det_excel = None\n\n    # ----------------Load fonts----------------\n    yaml_index = cls_name.index(\".yaml\")\n    cls_name_lang = cls_name[yaml_index - 2:yaml_index]\n\n    if cls_name_lang == \"zh\":\n        # Chinese\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/SimSun.ttf\"), size=FONTSIZE)\n    elif cls_name_lang in [\"en\", \"ru\", \"es\", \"ar\"]:\n        # English, Russian, Spanish, Arabic\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/TimesNewRoman.ttf\"), size=FONTSIZE)\n    elif cls_name_lang == \"ko\":\n        # Korean\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/malgun.ttf\"), size=FONTSIZE)\n\n    for result in results.xyxyn:\n        for i in range(len(result)):\n            id = int(i)  # instance ID\n            obj_cls_index = int(result[i][5])  # category index\n            obj_cls = model_cls_name_cp[obj_cls_index]  # category\n            cls_det_stat.append(obj_cls)\n\n            # ------------ border coordinates ------------\n            x0 = float(result[i][:4].tolist()[0])\n            y0 = float(result[i][:4].tolist()[1])\n            x1 = float(result[i][:4].tolist()[2])\n            y1 = float(result[i][:4].tolist()[3])\n\n            # ------------ Actual coordinates of the border ------------\n            x0 = int(img_size[0] * x0)\n            y0 = int(img_size[1] * y0)\n            x1 = int(img_size[0] * x1)\n            y1 = int(img_size[1] * y1)\n\n            conf = float(result[i][4])  # confidence\n            # fps = f\"{(1000 \/ float(results.t[1])):.2f}\" # FPS\n\n            det_img = pil_draw(\n                img,\n                f\"{id}-{obj_cls}:{conf:.2f}\",\n                textFont,\n                [x0, y0, x1, y1],\n                FONTSIZE,\n                opt,\n                obj_cls_index,\n                color_list,\n            )\n\n            # ----------add object size----------\n            w_obj = x1 - x0\n            h_obj = y1 - y0\n            area_obj = w_obj * h_obj\n            area_obj_all.append(area_obj)\n\n    # ------------JSON generate------------\n    det_json = export_json(results, img.size)[0]  # Detection information\n    det_json_format = json.dumps(det_json, sort_keys=False, indent=4, separators=(\",\", \":\"),\n                                 ensure_ascii=False)  # JSON formatting\n\n    if \"json\" not in opt:\n        det_json = None\n\n    # -------PDF generate-------\n    report = \".\/Det_Report.pdf\"\n    if \"pdf\" in opt:\n        pdf_generate(f\"{det_json_format}\", report, GYD_VERSION)\n    else:\n        report = None\n\n    # --------------object size compute--------------\n    for i in range(len(area_obj_all)):\n        if (0 < area_obj_all[i] <= 32 ** 2):\n            s_obj = s_obj + 1\n        elif (32 ** 2 < area_obj_all[i] <= 96 ** 2):\n            m_obj = m_obj + 1\n        elif (area_obj_all[i] > 96 ** 2):\n            l_obj = l_obj + 1\n\n    sml_obj_total = s_obj + m_obj + l_obj\n\n    objSize_dict = {obj_style[i]: [s_obj, m_obj, l_obj][i] \/ sml_obj_total for i in range(3)}\n\n    # ------------cls stat------------\n    clsRatio_dict = {}\n    clsDet_dict = Counter(cls_det_stat)\n    clsDet_dict_sum = sum(clsDet_dict.values())\n\n    for k, v in clsDet_dict.items():\n        clsRatio_dict[k] = v \/ clsDet_dict_sum\n\n    return det_img, img_crops, objSize_dict, clsRatio_dict, dataframe, det_json, report, det_csv, det_excel\n\n\n# YOLOv5 video detection function\ndef yolo_det_video(video, device, model_name, infer_size, conf, iou, max_num, model_cls, opt):\n\n    global model, model_name_tmp, device_tmp\n\n    os.system(\"\"\"\n        if [ -e '.\/output.mp4' ]; then\n        rm .\/output.mp4\n        fi\n        \"\"\")\n\n    if model_name_tmp != model_name:\n        # Model judgment to avoid repeated loading\n        model_name_tmp = model_name\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n    elif device_tmp != device:\n        # Device judgment to avoid repeated loading\n        device_tmp = device\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n    else:\n        print(f\"Loading model {model_name_tmp}......\")\n        model = model_loading(model_name_tmp, device, opt)\n\n    # -------------Model tuning -------------\n    model.conf = conf  # NMS confidence threshold\n    model.iou = iou  # NMS IOU threshold\n    model.max_det = int(max_num)  # Maximum number of detection frames\n    model.classes = model_cls  # model classes\n\n    color_list = color_set(len(model_cls_name_cp))  # \u8bbe\u7f6e\u989c\u8272\n\n    # ----------------Load fonts----------------\n    yaml_index = cls_name.index(\".yaml\")\n    cls_name_lang = cls_name[yaml_index - 2:yaml_index]\n\n    if cls_name_lang == \"zh\":\n        # Chinese\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/SimSun.ttf\"), size=FONTSIZE)\n    elif cls_name_lang in [\"en\", \"ru\", \"es\", \"ar\"]:\n        # English, Russian, Spanish, Arabic\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/TimesNewRoman.ttf\"), size=FONTSIZE)\n    elif cls_name_lang == \"ko\":\n        # Korean\n        textFont = ImageFont.truetype(str(f\"{ROOT_PATH}\/fonts\/malgun.ttf\"), size=FONTSIZE)\n\n    # video->frame\n    gc.collect()\n    output_video_path = \".\/output.avi\"\n    cap = cv2.VideoCapture(video)\n    fourcc = cv2.VideoWriter_fourcc(*\"I420\")  # encoder\n\n    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n    while cap.isOpened():\n        ret, frame = cap.read()\n        # Determine empty frame\n        if not ret:\n            break\n\n        results = model(frame, size=infer_size)  # detection\n        h, w, _ = frame.shape  # frame size\n        img_size = (w, h)  # frame size\n\n        for result in results.xyxyn:\n            for i in range(len(result)):\n                id = int(i)  # instance ID\n                obj_cls_index = int(result[i][5])  # category index\n                obj_cls = model_cls_name_cp[obj_cls_index]  # category\n\n                # ------------ border coordinates ------------\n                x0 = float(result[i][:4].tolist()[0])\n                y0 = float(result[i][:4].tolist()[1])\n                x1 = float(result[i][:4].tolist()[2])\n                y1 = float(result[i][:4].tolist()[3])\n\n                # ------------ Actual coordinates of the border ------------\n                x0 = int(img_size[0] * x0)\n                y0 = int(img_size[1] * y0)\n                x1 = int(img_size[0] * x1)\n                y1 = int(img_size[1] * y1)\n\n                conf = float(result[i][4])  # confidence\n                # fps = f\"{(1000 \/ float(results.t[1])):.2f}\" # FPS\n\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                frame = pil_draw(\n                    frame,\n                    f\"{id}-{obj_cls}:{conf:.2f}\",\n                    textFont,\n                    [x0, y0, x1, y1],\n                    FONTSIZE,\n                    opt,\n                    obj_cls_index,\n                    color_list,\n                )\n\n                frame = cv2.cvtColor(np.asarray(frame), cv2.COLOR_RGB2BGR)\n\n            # frame->video\n            out.write(frame)\n    out.release()\n    cap.release()\n    # cv2.destroyAllWindows()\n\n    return output_video_path\n\n\ndef main(args):\n    gr.close_all()\n\n    global model, model_cls_name_cp, cls_name\n\n    source = args.source\n    source_video = args.source_video\n    img_tool = args.img_tool\n    nms_conf = args.nms_conf\n    nms_iou = args.nms_iou\n    model_name = args.model_name\n    model_cfg = args.model_cfg\n    cls_name = args.cls_name\n    device = args.device\n    inference_size = args.inference_size\n    max_detnum = args.max_detnum\n    slider_step = args.slider_step\n    is_login = args.is_login\n    usr_pwd = args.usr_pwd\n    is_share = args.is_share\n\n    is_fonts(f\"{ROOT_PATH}\/fonts\")  # Check font files\n\n    # model loading\n    model = model_loading(model_name, device)\n\n    model_names = yaml_csv(model_cfg, \"model_names\")  # model names\n    model_cls_name = yaml_csv(cls_name, \"model_cls_name\")  # class name\n\n    model_cls_name_cp = model_cls_name.copy()  # class name\n\n    # ------------------- Input Components -------------------\n    inputs_img = gr.Image(image_mode=\"RGB\", source=source, tool=img_tool, type=\"pil\", label=\"original image\")\n    inputs_device01 = gr.Radio(choices=[\"cuda:0\", \"cpu\"], value=device, label=\"device\")\n    inputs_model01 = gr.Dropdown(choices=model_names, value=model_name, type=\"value\", label=\"model\")\n    inputs_size01 = gr.Radio(choices=[320, 640, 1280], value=inference_size, label=\"inference size\")\n    input_conf01 = gr.Slider(0, 1, step=slider_step, value=nms_conf, label=\"confidence threshold\")\n    inputs_iou01 = gr.Slider(0, 1, step=slider_step, value=nms_iou, label=\"IoU threshold\")\n    inputs_maxnum01 = gr.Number(value=max_detnum, label=\"Maximum number of detections\")\n    inputs_clsName01 = gr.CheckboxGroup(choices=model_cls_name, value=model_cls_name, type=\"index\", label=\"category\")\n    inputs_opt01 = gr.CheckboxGroup(choices=[\"refresh_yolov5\", \"label\", \"pdf\", \"json\", \"csv\", \"excel\"],\n                                    value=[\"label\", \"pdf\"],\n                                    type=\"value\",\n                                    label=\"operate\")\n\n    # ------------------- Input Components -------------------\n    inputs_video = gr.Video(format=\"mp4\", source=source_video, label=\"original video\")  # webcam\n    inputs_device02 = gr.Radio(choices=[\"cuda:0\", \"cpu\"], value=device, label=\"device\")\n    inputs_model02 = gr.Dropdown(choices=model_names, value=model_name, type=\"value\", label=\"model\")\n    inputs_size02 = gr.Radio(choices=[320, 640, 1280], value=inference_size, label=\"inference size\")\n    input_conf02 = gr.Slider(0, 1, step=slider_step, value=nms_conf, label=\"confidence threshold\")\n    inputs_iou02 = gr.Slider(0, 1, step=slider_step, value=nms_iou, label=\"IoU threshold\")\n    inputs_maxnum02 = gr.Number(value=max_detnum, label=\"Maximum number of detections\")\n    inputs_clsName02 = gr.CheckboxGroup(choices=model_cls_name, value=model_cls_name, type=\"index\", label=\"category\")\n    inputs_opt02 = gr.CheckboxGroup(choices=[\"refresh_yolov5\", \"label\"], value=[\"label\"], type=\"value\", label=\"operate\")\n\n    # Input parameters\n    inputs_img_list = [\n        inputs_img,  # input image\n        inputs_device01,  # device\n        inputs_model01,  # model\n        inputs_size01,  # inference size\n        input_conf01,  # confidence threshold\n        inputs_iou01,  # IoU threshold\n        inputs_maxnum01,  # maximum number of detections\n        inputs_clsName01,  # category\n        inputs_opt01,  # detect operations\n    ]\n\n    inputs_video_list = [\n        inputs_video,  # input image\n        inputs_device02,  # device\n        inputs_model02,  # model\n        inputs_size02,  # inference size\n        input_conf02,  # confidence threshold\n        inputs_iou02,  # IoU threshold\n        inputs_maxnum02,  # maximum number of detections\n        inputs_clsName02,  # category\n        inputs_opt02,  # detect operation\n    ]\n\n    # -------------------output component-------------------\n    outputs_img = gr.Image(type=\"pil\", label=\"Detection image\")\n    outputs_crops = gr.Gallery(label=\"Object crop\")\n    outputs_df = gr.Dataframe(max_rows=5,\n                              overflow_row_behaviour=\"paginate\",\n                              type=\"pandas\",\n                              label=\"List of detection information\")\n    outputs_objSize = gr.Label(label=\"Object size ratio statistics\")\n    outputs_clsSize = gr.Label(label=\"Category detection proportion statistics\")\n    outputs_json = gr.JSON(label=\"Detection information\")\n    outputs_pdf = gr.File(label=\"pdf detection report\")\n    outputs_csv = gr.File(label=\"csv detection report\")\n    outputs_excel = gr.File(label=\"xlsx detection report\")\n\n    # -------------------output component-------------------\n    outputs_video = gr.Video(format='mp4', label=\"Detection video\")\n\n    # output parameters\n    outputs_img_list = [\n        outputs_img, outputs_crops, outputs_objSize, outputs_clsSize, outputs_df, outputs_json, outputs_pdf,\n        outputs_csv, outputs_excel]\n    outputs_video_list = [outputs_video]\n\n    # title\n    title = \"Gradio YOLOv5 Det v0.4\"\n\n    # describe\n    description = \"Author: \u66fe\u9038\u592b\uff08Zeng Yifu\uff09, Project Address: https:\/\/gitee.com\/CV_Lab\/gradio_yolov5_det, Github: https:\/\/github.com\/Zengyf-CVer, thanks to [Gradio](https:\/\/github.com\/gradio-app\/gradio) & [YOLOv5](https:\/\/github.com\/ultralytics\/yolov5)\"\n    # article=\"https:\/\/gitee.com\/CV_Lab\/gradio_yolov5_det\"\n\n    # example image\n    examples = [\n        [\n            \".\/img_example\/bus.jpg\",\n            \"cpu\",\n            \"yolov5s\",\n            640,\n            0.6,\n            0.5,\n            10,\n            [\"person\", \"bus\"],\n            [\"label\", \"pdf\"],],\n        [\n            \".\/img_example\/giraffe.jpg\",\n            \"cpu\",\n            \"yolov5l\",\n            320,\n            0.5,\n            0.45,\n            12,\n            [\"giraffe\"],\n            [\"label\", \"pdf\"],],\n        [\n            \".\/img_example\/zidane.jpg\",\n            \"cpu\",\n            \"yolov5m\",\n            640,\n            0.6,\n            0.5,\n            15,\n            [\"person\", \"tie\"],\n            [\"pdf\", \"json\"],],\n        [\n            \".\/img_example\/Millenial-at-work.jpg\",\n            \"cpu\",\n            \"yolov5s6\",\n            1280,\n            0.5,\n            0.5,\n            20,\n            [\"person\", \"chair\", \"cup\", \"laptop\"],\n            [\"label\", \"pdf\"],],]\n\n    # interface\n    gyd_img = gr.Interface(\n        fn=yolo_det_img,\n        inputs=inputs_img_list,\n        outputs=outputs_img_list,\n        title=title,\n        description=description,\n        # article=article,\n        examples=examples,\n        # cache_examples=False,\n        # theme=\"seafoam\",\n        # live=True, # Change output in real time\n        flagging_dir=\"run\",  # output directory\n        # allow_flagging=\"manual\",\n        # flagging_options=[\"good\", \"generally\", \"bad\"],\n    )\n\n    gyd_video = gr.Interface(\n        # fn=yolo_det_video_test,\n        fn=yolo_det_video,\n        inputs=inputs_video_list,\n        outputs=outputs_video_list,\n        title=title,\n        description=description,\n        # article=article,\n        # examples=examples,\n        # theme=\"seafoam\",\n        # live=True, # Change output in real time\n        flagging_dir=\"run\",  # output directory\n        allow_flagging=\"never\",\n        # flagging_options=[\"good\", \"generally\", \"bad\"],\n    )\n\n    gyd = gr.TabbedInterface(interface_list=[gyd_img, gyd_video], tab_names=[\"Image Mode\", \"Video Mode\"])\n\n    if not is_login:\n        gyd.launch(\n            inbrowser=True,  # Automatically open default browser\n            show_tips=True,  # Automatically display the latest features of gradio\n            share=is_share,  # Project sharing, other devices can access\n            favicon_path=\".\/icon\/logo.ico\",  # web icon\n            show_error=True,  # Display error message in browser console\n            quiet=True,  # Suppress most print statements\n        )\n    else:\n        gyd.launch(\n            inbrowser=True,  # Automatically open default browser\n            show_tips=True,  # Automatically display the latest features of gradio\n            auth=usr_pwd,  # login interface\n            share=is_share,  # Project sharing, other devices can access\n            favicon_path=\".\/icon\/logo.ico\",  # web icon\n            show_error=True,  # Display error message in browser console\n            quiet=True,  # Suppress most print statements\n        )\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args)\n\n```"}
{"question":"\nI have a machine learning model that can take in an image and classify it into one of several categories, such as cat, dog, bird, etc. With my current code, I can load the model, pass in an image, and get the predicted class label as output.\n\nI would like to create an interactive Gradio demo for my image classification model with the following features:\n- Allow the user to upload an image file \n- Display the uploaded image\n- Have a button to submit the image for classification\n- Display the top predicted class label and confidence score\n- Show a radio button group to select one of the possible image categories \n- Use the selected category to filter the dataset and display a random image from that class\n- Include a button to clear and reset the interface\n","answer":"here is the gradio solution for your use case: ```python\nInvalid username or password.\n```"}
{"question":"\nI have a simple Python function called `greet` that takes in a name as a string and returns a greeting message that says \"Hello [name]!!\". \n\nI would like to create a basic Gradio interface for this greeting function with the following:\n- A text input box for the user to enter their name\n- The interface should run the `greet` function whenever the user enters their name and hits enter\n- Display the output greeting message returned by the function\n- Include a text description at the top of the interface with a link to another Hugging Face Space\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\n\n\ndescription=\"<a href='https:\/\/huggingface.co\/spaces\/EleutherAI\/VQGAN_CLIP' target='_blank'>Space for VQGAN_CLIP is now at https:\/\/huggingface.co\/spaces\/EleutherAI\/VQGAN_CLIP<\/a><\/p>\"\n\ndef greet(name):\n    return \"Hello \" + name + \"!!\"\n\niface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\",description=description)\niface.launch()\n```"}
{"question":"\nI have a simple Python function called `greet` that takes in a name as a string and returns a greeting message that says \"Hello [name]!!\". \n\nI would like to create a basic Gradio interface for this greeting function with the following:\n\n- A text input box for the user to enter their name\n- The interface should run the `greet` function whenever the user enters their name and hits enter\n- Display the output greeting message that is returned by the function \n- Include a text description at the top of the interface with a link to another Hugging Face Space\n\nThe interface doesn't need any other components or features. I mainly want to demonstrate how to create a simple function, set up the Gradio interface, and launch it.\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\n\n\ndescription=\"<a href='https:\/\/huggingface.co\/spaces\/EleutherAI\/clip-guided-diffusion' target='_blank'>Space for CLIP_guided_diffusion is now at https:\/\/huggingface.co\/spaces\/EleutherAI\/clip-guided-diffusion<\/a><\/p>\"\n\ndef greet(name):\n    return \"Hello \" + name + \"!!\"\n\niface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\",description=description)\niface.launch()\n```"}
{"question":"\nI have a BlendGAN model that can take a content image and a style image as input and generate a stylized version of the content image in the style of the style image. The model is based on the paper \"BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation\". With my current code, I can load the pretrained model weights and run inference to generate the stylized image output.\n\nI would like to create an interactive Gradio web demo for my BlendGAN model with the following features:\n- Two image upload components, one for the content image and one for the style image \n- A slider input to control the weight\/strength of the style transfer, with a default value and range of 1-30\n- Display the generated stylized image output\n- Provide some example images that can be loaded into the interface with one click\n- Add a title, description, and link to the paper and Github repo\n- Enable the submit button queueing feature so multiple requests can be handled \n","answer":"here is the gradio solution for your use case: ```python\nimport os \nimport gradio as gr\nfrom PIL import Image\nos.system('pip install dlib')\nos.system(\"wget https:\/\/www.dropbox.com\/s\/fgupbov77x4rrru\/blendgan.pt\")\nos.system(\"wget https:\/\/www.dropbox.com\/s\/v8q0dd3r4u20659\/psp_encoder.pt\")\nos.system(\"wget https:\/\/github.com\/kim-ninh\/align_face_ffhq\/raw\/main\/shape_predictor_68_face_landmarks.dat -P \/home\/user\/app\/ffhq_dataset\/\")\n\n\nfrom ffhq_dataset.gen_aligned_image import FaceAlign\n\n\nfa = FaceAlign()\nimport cv2\n\n\n\ndef inference(content, style, index):\n    content.save('content.png')\n    style.save('style.png')\n    imgc = cv2.imread('content.png')\n    img_cropc = fa.get_crop_image(imgc)\n    cv2.imwrite('contentcrop.png', img_cropc)\n    os.system(\"\"\"python style_transfer_folder.py --size 1024 --add_weight_index \"\"\"+str(int(index))+\"\"\" --ckpt .\/blendgan.pt --psp_encoder_ckpt .\/psp_encoder.pt --style_img_path style.png --input_img_path contentcrop.png\"\"\")\n    return \"out.jpg\"\n  \ntitle = \"BlendGAN\"\ndescription = \"Gradio Demo for BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation. To use it, simply upload your images, or click one of the examples to load them. Read more at the links below. Please use a cropped portrait picture for best results similar to the examples below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/arxiv.org\/abs\/2110.11728' target='_blank'>BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation<\/a> | <a href='https:\/\/github.com\/onion-liu\/BlendGAN' target='_blank'>Github Repo<\/a><\/p><p style='text-align: center'>samples from repo: <img src='https:\/\/user-images.githubusercontent.com\/6346064\/142623312-3e6f09aa-ce88-465c-b956-a8b4db95b4da.gif' alt='animation'\/> <img src='https:\/\/user-images.githubusercontent.com\/6346064\/142621044-086cde48-8604-467b-8c43-8768b6670ec2.gif' alt='animation'\/><\/p>\"\n\nexamples=[['000000.png','100001.png',6]]\ngr.Interface(inference, [gr.inputs.Image(type=\"pil\"),gr.inputs.Image(type=\"pil\"),gr.inputs.Slider(minimum=1, maximum=30, step=1, default=6, label=\"Weight Index\")\n], gr.outputs.Image(type=\"file\"),title=title,description=description,article=article,enable_queue=True,examples=examples,allow_flagging=False).launch()\n```"}
{"question":"\nI have a YOLOv5 object detection model that can detect objects in images. I'm using the SAHI library to perform \"sliced inference\" which helps detect small objects in large images by splitting the image into overlapping slices and running the model on each slice. With my current code, I can load an image, run the YOLOv5 model with and without sliced inference using SAHI, and visualize the detected objects on the image.\n\nI would like to create an interactive Gradio demo to showcase the benefits of using sliced inference for small object detection. The demo should allow the user to:\n- Upload an image \n- Adjust the sliced inference parameters like slice height\/width, overlap ratios, postprocessing settings etc. using input sliders and dropdowns\n- Display the object detection results on the original image and the sliced image side-by-side after clicking submit\n- Provide some example images that the user can easily test out\n- Have a nice title, description and article explaining what the demo is about\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport sahi.utils\nfrom sahi import AutoDetectionModel\nimport sahi.predict\nimport sahi.slicing\nfrom PIL import Image\nimport numpy\n\nIMAGE_SIZE = 640\n\n# Images\nsahi.utils.file.download_from_url(\n    \"https:\/\/user-images.githubusercontent.com\/34196005\/142730935-2ace3999-a47b-49bb-83e0-2bdd509f1c90.jpg\",\n    \"apple_tree.jpg\",\n)\nsahi.utils.file.download_from_url(\n    \"https:\/\/user-images.githubusercontent.com\/34196005\/142730936-1b397756-52e5-43be-a949-42ec0134d5d8.jpg\",\n    \"highway.jpg\",\n)\n\nsahi.utils.file.download_from_url(\n    \"https:\/\/user-images.githubusercontent.com\/34196005\/142742871-bf485f84-0355-43a3-be86-96b44e63c3a2.jpg\",\n    \"highway2.jpg\",\n)\n\nsahi.utils.file.download_from_url(\n    \"https:\/\/user-images.githubusercontent.com\/34196005\/142742872-1fefcc4d-d7e6-4c43-bbb7-6b5982f7e4ba.jpg\",\n    \"highway3.jpg\",\n)\n\n\n# Model\nmodel = AutoDetectionModel.from_pretrained(\n    model_type=\"yolov5\", model_path=\"yolov5s6.pt\", device=\"cpu\", confidence_threshold=0.5, image_size=IMAGE_SIZE\n)\n\n\ndef sahi_yolo_inference(\n    image,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n    postprocess_type=\"NMS\",\n    postprocess_match_metric=\"IOU\",\n    postprocess_match_threshold=0.5,\n    postprocess_class_agnostic=False,\n):\n\n    image_width, image_height = image.size\n    sliced_bboxes = sahi.slicing.get_slice_bboxes(\n        image_height,\n        image_width,\n        slice_height,\n        slice_width,\n        False,\n        overlap_height_ratio,\n        overlap_width_ratio,\n    )\n    if len(sliced_bboxes) > 60:\n        raise ValueError(\n            f\"{len(sliced_bboxes)} slices are too much for huggingface spaces, try smaller slice size.\"\n        )\n\n    # standard inference\n    prediction_result_1 = sahi.predict.get_prediction(\n        image=image, detection_model=model\n    )\n    print(image)\n    visual_result_1 = sahi.utils.cv.visualize_object_predictions(\n        image=numpy.array(image),\n        object_prediction_list=prediction_result_1.object_prediction_list,\n    )\n    output_1 = Image.fromarray(visual_result_1[\"image\"])\n\n    # sliced inference\n    prediction_result_2 = sahi.predict.get_sliced_prediction(\n        image=image,\n        detection_model=model,\n        slice_height=int(slice_height),\n        slice_width=int(slice_width),\n        overlap_height_ratio=overlap_height_ratio,\n        overlap_width_ratio=overlap_width_ratio,\n        postprocess_type=postprocess_type,\n        postprocess_match_metric=postprocess_match_metric,\n        postprocess_match_threshold=postprocess_match_threshold,\n        postprocess_class_agnostic=postprocess_class_agnostic,\n    )\n    visual_result_2 = sahi.utils.cv.visualize_object_predictions(\n        image=numpy.array(image),\n        object_prediction_list=prediction_result_2.object_prediction_list,\n    )\n\n    output_2 = Image.fromarray(visual_result_2[\"image\"])\n\n    return output_1, output_2\n\n\ninputs = [\n    gr.Image(type=\"pil\", label=\"Original Image\"),\n    gr.Number(default=512, label=\"slice_height\"),\n    gr.Number(default=512, label=\"slice_width\"),\n    gr.Number(default=0.2, label=\"overlap_height_ratio\"),\n    gr.Number(default=0.2, label=\"overlap_width_ratio\"),\n    gr.Dropdown(\n        [\"NMS\", \"GREEDYNMM\"],\n        type=\"value\",\n        value=\"NMS\",\n        label=\"postprocess_type\",\n    ),\n    gr.Dropdown(\n        [\"IOU\", \"IOS\"], type=\"value\", default=\"IOU\", label=\"postprocess_type\"\n    ),\n    gr.Number(default=0.5, label=\"postprocess_match_threshold\"),\n    gr.Checkbox(default=True, label=\"postprocess_class_agnostic\"),\n]\n\noutputs = [\n    gr.Image(type=\"pil\", label=\"YOLOv5s\"),\n    gr.Image(type=\"pil\", label=\"YOLOv5s + SAHI\"),\n]\n\ntitle = \"Small Object Detection with SAHI + YOLOv5\"\ndescription = \"SAHI + YOLOv5 demo for small object detection. Upload an image or click an example image to use.\"\narticle = \"<p style='text-align: center'>SAHI is a lightweight vision library for performing large scale object detection\/ instance segmentation.. <a href='https:\/\/github.com\/obss\/sahi'>SAHI Github<\/a> | <a href='https:\/\/medium.com\/codable\/sahi-a-vision-library-for-performing-sliced-inference-on-large-images-small-objects-c8b086af3b80'>SAHI Blog<\/a> | <a href='https:\/\/github.com\/fcakyon\/yolov5-pip'>YOLOv5 Github<\/a> <\/p>\"\nexamples = [\n    [\"apple_tree.jpg\", 256, 256, 0.2, 0.2, \"NMS\", \"IOU\", 0.4, True],\n    [\"highway.jpg\", 256, 256, 0.2, 0.2, \"NMS\", \"IOU\", 0.4, True],\n    [\"highway2.jpg\", 512, 512, 0.2, 0.2, \"NMS\", \"IOU\", 0.4, True],\n    [\"highway3.jpg\", 512, 512, 0.2, 0.2, \"NMS\", \"IOU\", 0.4, True],\n]\n\ngr.Interface(\n    sahi_yolo_inference,\n    inputs,\n    outputs,\n    title=title,\n    description=description,\n    article=article,\n    examples=examples,\n    theme=\"huggingface\",\n    cache_examples=True,\n).launch(debug=True, enable_queue=True)\n\n```"}
{"question":"\nI have a Python script that uses the OpenPifPaf pose estimation models to detect poses of humans, animals, and vehicles in images. With my current code, I can run the appropriate OpenPifPaf model checkpoint for a given image and output version, and generate an output image with the detected poses\/keypoints overlaid on the original image.\n\nI would like to create an interactive Gradio demo for this OpenPifPaf script with the following features:\n- Allow the user to upload an input image \n- Have a radio button input to select the OpenPifPaf model version (pose, whole-body, animal, vehicles)\n- Include a checkbox to optionally overlay the poses with a white color\n- Display the output image with detected poses after running the model\n- Provide some pre-loaded example images that demonstrate each model version\n- Add a title, description and links to relevant pages\n- Enable the queuing option to handle multiple submissions\n","answer":"here is the gradio solution for your use case: ```python\nimport os\nimport gradio as gr\n\ndef inference(img, ver, white_overlay):\n\n    if white_overlay:\n        white_overlay = \"--white-overlay=0.3\"\n    else:\n        white_overlay = \"\"\n\n    if ver == 'pose':\n        os.system(\"python -m openpifpaf.predict \"+img.name+\" --checkpoint=shufflenetv2k30 --line-width=4 \" + white_overlay + \" -o out.jpg\")\n    elif ver == 'whole-body':\n        os.system(\"python -m openpifpaf.predict \"+img.name+\" --checkpoint=shufflenetv2k30-wholebody --instance-threshold 0.05 \" + white_overlay + \" --seed-threshold 0.05 \\\n                                                             --line-width 3 -o out.jpg\")\n    elif ver == 'vehicles':\n        os.system(\"python -m openpifpaf.predict \"+img.name+\" --checkpoint=shufflenetv2k16-apollo-24 --line-width=5  \" + white_overlay + \" -o out.jpg\")\n    elif ver == 'animal':\n        os.system(\"python -m openpifpaf.predict \"+img.name+\" --checkpoint=shufflenetv2k30-animalpose --line-width=5 --font-size=6 \" + white_overlay + \" \\\n                   --long-edge=500  -o out.jpg\")\n    else:\n        raise ValueError('invalid version')\n\n    return \"out.jpg\"\n      \n  \ntitle = \"Openpifpaf - pose estimation for human, vehicles and animals\"\ndescription = \"Gradio demo for openpifpaf. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below and don't hesitate to SMASH THAT LIKE BUTTON (and you do not have a dislike there either so...)\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/openpifpaf\/openpifpaf' target='_blank'>Github Repo Openpifpaf<\/a> | <a href='https:\/\/github.com\/peterbonnesoeur' target='_blank'>Github Repo peterbonnesoeur<\/a><\/p>\"\n\nwith open(\"article.html\", \"r\", encoding='utf-8') as f:\n    article= f.read()\n\nexamples=[ \n    ['basketball.jpg','whole-body'],\n    ['bill.png','whole-body'],\n    ['billie.png','whole-body'],\n    ['meeting.jpeg','pose'],\n    ['crowd.jpg','pose'],\n    ['dalmatian.jpg', 'animal'],\n    ['tappo_loomo.jpg', 'animal'],\n    ['cow.jpg', 'animal'],\n    ['india-vehicles.jpeg', 'vehicles'],\n    ['russia-vehicles.jpg', 'vehicles'],\n    ['paris-vehicles.jpg', 'vehicles'],\n\n    ]\n\ngr.Interface(\n    inference,\n    [\n        gr.inputs.Image(type=\"file\", label=\"Input\"),\n        gr.inputs.Radio(['whole-body', 'pose', 'vehicles', 'animal'], type=\"value\", default='whole-body', label='version'),\n        gr.inputs.Checkbox(default=False, label=\"White overlay\")\n    ],\n    gr.outputs.Image(type=\"file\", label=\"Output\"),\n    title=title,\n    description=description,\n    article=article,\n    enable_queue=True,\n    examples=examples).launch()\n```"}
{"question":"\nI have a convolutional Hough matching network model that can take a source image and a target image, and find correspondences between a 7x7 grid of points on the source image and their matching locations on the target image. With my current code, I can run the model on a pair of images and visualize the predicted point correspondences.\n\nI would like to create an interactive Gradio demo for my CHM model with the following features:\n- Two image upload components to allow the user to provide the source and target images \n- Sliders to let the user adjust the bounding box of the 7x7 grid on the source image\n- A button to run the model and generate the predicted correspondences \n- Display a plot visualizing the point correspondences, with the source image and points on the left, and the target image and transferred points on the right\n- Include pre-populated example image pairs that showcase the model's capabilities\n- Provide a brief description of the model and link to relevant papers\/code\n\nThis will allow users to interactively experiment with the CHM model on their own images and visualize how it is able to find semantic correspondences between different views of the same object or scene.\n","answer":"here is the gradio solution for your use case: ```python\nimport csv\nimport os\nimport random\nimport sys\nfrom itertools import product\n\nimport gdown\nimport gradio as gr\nimport matplotlib\nimport matplotlib.patches as patches\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\n\nfrom common.evaluation import Evaluator\nfrom common.logger import AverageMeter, Logger\nfrom data import download\nfrom model import chmnet\nfrom model.base.geometry import Geometry\n\ncsv.field_size_limit(sys.maxsize)\n\n# Downloading the Model\n\nmd5 = \"6b7b4d7bad7f89600fac340d6aa7708b\"\n\ngdown.cached_download(\n    url=\"https:\/\/drive.google.com\/u\/0\/uc?id=1zsJRlAsoOn5F0GTCprSFYwDDfV85xDy6&export=download\",\n    path=\"pas_psi.pt\",\n    quiet=False,\n    md5=md5,\n)\n\n# Model Initialization\nargs = dict(\n    {\n        \"alpha\": [0.05, 0.1],\n        \"benchmark\": \"pfpascal\",\n        \"bsz\": 90,\n        \"datapath\": \"..\/Datasets_CHM\",\n        \"img_size\": 240,\n        \"ktype\": \"psi\",\n        \"load\": \"pas_psi.pt\",\n        \"thres\": \"img\",\n    }\n)\n\nmodel = chmnet.CHMNet(args[\"ktype\"])\nmodel.load_state_dict(torch.load(args[\"load\"], map_location=torch.device(\"cpu\")))\nEvaluator.initialize(args[\"alpha\"])\nGeometry.initialize(img_size=args[\"img_size\"])\nmodel.eval()\n\n# Transforms\n\nchm_transform = transforms.Compose(\n    [\n        transforms.Resize(args[\"img_size\"]),\n        transforms.CenterCrop((args[\"img_size\"], args[\"img_size\"])),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\nchm_transform_plot = transforms.Compose(\n    [\n        transforms.Resize(args[\"img_size\"]),\n        transforms.CenterCrop((args[\"img_size\"], args[\"img_size\"])),\n    ]\n)\n\n# A Helper Function\nto_np = lambda x: x.data.to(\"cpu\").numpy()\n\n# Colors for Plotting\ncmap = matplotlib.cm.get_cmap(\"Spectral\")\nrgba = cmap(0.5)\ncolors = []\nfor k in range(49):\n    colors.append(cmap(k \/ 49.0))\n\n\n# CHM MODEL\ndef run_chm(\n    source_image,\n    target_image,\n    selected_points,\n    number_src_points,\n    chm_transform,\n    display_transform,\n):\n    # Convert to Tensor\n    src_img_tnsr = chm_transform(source_image).unsqueeze(0)\n    tgt_img_tnsr = chm_transform(target_image).unsqueeze(0)\n\n    # Selected_points = selected_points.T\n    keypoints = torch.tensor(selected_points).unsqueeze(0)\n    n_pts = torch.tensor(np.asarray([number_src_points]))\n\n    # RUN CHM ------------------------------------------------------------------------\n    with torch.no_grad():\n        corr_matrix = model(src_img_tnsr, tgt_img_tnsr)\n        prd_kps = Geometry.transfer_kps(corr_matrix, keypoints, n_pts, normalized=False)\n\n    # VISUALIZATION\n    src_points = keypoints[0].squeeze(0).squeeze(0).numpy()\n    tgt_points = prd_kps[0].squeeze(0).squeeze(0).cpu().numpy()\n\n    src_points_converted = []\n    w, h = display_transform(source_image).size\n\n    for x, y in zip(src_points[0], src_points[1]):\n        src_points_converted.append(\n            [int(x * w \/ args[\"img_size\"]), int((y) * h \/ args[\"img_size\"])]\n        )\n\n    src_points_converted = np.asarray(src_points_converted[:number_src_points])\n    tgt_points_converted = []\n\n    w, h = display_transform(target_image).size\n    for x, y in zip(tgt_points[0], tgt_points[1]):\n        tgt_points_converted.append(\n            [int(((x + 1) \/ 2.0) * w), int(((y + 1) \/ 2.0) * h)]\n        )\n\n    tgt_points_converted = np.asarray(tgt_points_converted[:number_src_points])\n\n    tgt_grid = []\n\n    for x, y in zip(tgt_points[0], tgt_points[1]):\n        tgt_grid.append([int(((x + 1) \/ 2.0) * 7), int(((y + 1) \/ 2.0) * 7)])\n\n    # VISUALIZATION\n    # PLOT\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n\n    # Source image plot\n    ax[0].imshow(display_transform(source_image))\n    ax[0].scatter(\n        src_points_converted[:, 0],\n        src_points_converted[:, 1],\n        c=\"blue\",\n        edgecolors=\"white\",\n        s=50,\n        label=\"Source points\",\n    )\n    ax[0].set_title(\"Source Image with Selected Points\")\n    ax[0].set_xticks([])\n    ax[0].set_yticks([])\n\n    # Target image plot\n    ax[1].imshow(display_transform(target_image))\n    ax[1].scatter(\n        tgt_points_converted[:, 0],\n        tgt_points_converted[:, 1],\n        c=\"red\",\n        edgecolors=\"white\",\n        s=50,\n        label=\"Target points\",\n    )\n    ax[1].set_title(\"Target Image with Corresponding Points\")\n    ax[1].set_xticks([])\n    ax[1].set_yticks([])\n\n    # Adding labels to points\n    for i, (src, tgt) in enumerate(zip(src_points_converted, tgt_points_converted)):\n        ax[0].text(*src, str(i), color=\"white\", bbox=dict(facecolor=\"black\", alpha=0.5))\n        ax[1].text(*tgt, str(i), color=\"black\", bbox=dict(facecolor=\"white\", alpha=0.7))\n\n    # Create a colormap that will generate 49 distinct colors\n    cmap = plt.get_cmap(\n        \"gist_rainbow\", 49\n    )  # 'gist_rainbow' is just an example, you can choose another colormap\n\n    # Drawing lines between corresponding source and target points\n    # for i, (src, tgt) in enumerate(zip(src_points_converted, tgt_points_converted)):\n    #     con = ConnectionPatch(\n    #         xyA=tgt,\n    #         xyB=src,\n    #         coordsA=\"data\",\n    #         coordsB=\"data\",\n    #         axesA=ax[1],\n    #         axesB=ax[0],\n    #         color=cmap(i),\n    #         linewidth=2,\n    #     )\n        # ax[1].add_artist(con)\n\n    # Adding legend\n    ax[0].legend(loc=\"lower right\", bbox_to_anchor=(1, -0.075))\n    ax[1].legend(loc=\"lower right\", bbox_to_anchor=(1, -0.075))\n\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    fig.suptitle(\"CHM Correspondences\\nUsing $\\it{pas\\_psi.pt}$ Weights \", fontsize=16)\n    return fig\n\n\n# Wrapper\ndef generate_correspondences(\n    sousrce_image, target_image, min_x=1, max_x=100, min_y=1, max_y=100\n):\n    A = np.linspace(min_x, max_x, 7)\n    B = np.linspace(min_y, max_y, 7)\n    point_list = list(product(A, B))\n    new_points = np.asarray(point_list, dtype=np.float64).T\n    return run_chm(\n        sousrce_image,\n        target_image,\n        selected_points=new_points,\n        number_src_points=49,\n        chm_transform=chm_transform,\n        display_transform=chm_transform_plot,\n    )\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n# Correspondence Matching with Convolutional Hough Matching Networks\nPerforms keypoint transform from a 7x7 gird on the source image to the target image. Use the sliders to adjust the grid.\n[Original Paper](https:\/\/arxiv.org\/abs\/2103.16831) - [Github Page](https:\/\/github.com\/juhongm999\/chm)\n    \"\"\"\n    )\n\n    with gr.Row():\n        # Add an Image component to display the source image.\n        image1 = gr.Image(\n            shape=(240, 240),\n            type=\"pil\",\n            label=\"Source Image\",\n        )\n\n        # Add an Image component to display the target image.\n        image2 = gr.Image(\n            shape=(240, 240),\n            type=\"pil\",\n            label=\"Target Image\",\n        )\n\n    with gr.Row():\n        # Add a Slider component to adjust the minimum x-coordinate of the grid.\n        min_x = gr.Slider(\n            minimum=1,\n            maximum=240,\n            step=1,\n            default=15,\n            label=\"Min X\",\n        )\n\n        # Add a Slider component to adjust the maximum x-coordinate of the grid.\n        max_x = gr.Slider(\n            minimum=1,\n            maximum=240,\n            step=1,\n            default=215,\n            label=\"Max X\",\n        )\n\n        # Add a Slider component to adjust the minimum y-coordinate of the grid.\n        min_y = gr.Slider(\n            minimum=1,\n            maximum=240,\n            step=1,\n            default=15,\n            label=\"Min Y\",\n        )\n\n        # Add a Slider component to adjust the maximum y-coordinate of the grid.\n        max_y = gr.Slider(\n            minimum=1,\n            maximum=240,\n            step=1,\n            default=215,\n            label=\"Max Y\",\n        )\n\n    with gr.Row():\n        output_plot = gr.Plot(\n            type=\"plot\",\n            label=\"Output Plot\",\n        )\n\n    gr.Examples(\n        [\n            [\".\/examples\/sample1.jpeg\", \".\/examples\/sample2.jpeg\", 17, 223, 17, 223],\n            [\n                \".\/examples\/Red_Winged_Blackbird_0012_6015.jpg\",\n                \".\/examples\/Red_Winged_Blackbird_0025_5342.jpg\",\n                17,\n                223,\n                17,\n                223,\n            ],\n            [\n                \".\/examples\/Yellow_Headed_Blackbird_0026_8545.jpg\",\n                \".\/examples\/Yellow_Headed_Blackbird_0020_8549.jpg\",\n                17,\n                223,\n                17,\n                223,\n            ],\n        ],\n        inputs=[\n            image1,\n            image2,\n            min_x,\n            max_x,\n            min_y,\n            max_y,\n        ],\n    )\n\n    run_btn = gr.Button(\"Run\")\n\n    run_btn.click(\n        generate_correspondences,\n        inputs=[image1, image2, min_x, max_x, min_y, max_y],\n        outputs=output_plot,\n    )\n\n    demo.launch(debug=True, enable_queue=False)\n\n```"}
{"question":"\nI have an OCR (optical character recognition) system that can detect and recognize both handwritten and printed text in images. My current code uses the CRAFT text detection model to identify text regions in an input image, then applies the TrOCR recognition model to extract the actual text content from those regions. The TrOCR model was trained on lines of text, so my code also handles detecting individual lines and extracting them. With my current models and code, I can take an input image and produce two outputs: 1) The input image with bounding boxes drawn around the detected text regions, and 2) The extracted text content itself.\n\nI would like to create an interactive Gradio demo to showcase my OCR system. The Gradio interface should have the following components and features:\n\n- An image upload component for the user to provide an input image \n- Two output components:\n   1) To display the processed image with bounding boxes around detected text\n   2) A textbox to show the recognized text extracted from the image\n- A title and description explaining what the demo does\n- A few sample images that can be loaded into the interface as examples\n- An 'article' section with links to the open-source libraries used (CRAFT and TrOCR)\n- Enable the queuing option to handle multiple requests\n\nThe interface should allow the user to upload an image, then it will run my OCR code and display the outputs. The examples will provide some sample images to quickly test it out.\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nfrom craft_hw_ocr import OCR\n\nocr = OCR.load_models()\n\ndef do_ocr(inp):\n  img, results = OCR.detection(inp, ocr[2])\n  bboxes, text = OCR.recoginition(img, results, ocr[0], ocr[1])\n  return OCR.visualize(img, results), text\n  \ninputs = gr.inputs.Image()\no1 = gr.outputs.Image()\no2 = gr.outputs.Textbox()\n\ntitle = \"CRAFT-OCR\"\ndescription = \"OCR of both handwriting and printed text using CRAFT Text detector and TrOCR recognition, detection of lines and extraction of them are happening here because TrOCR pre-trained models are modelled on IAM lines dataset and the same needs to be implemented here.\"\nexamples=[['example_1.png'],['example_2.jpg']]\n\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/Vishnunkumar\/craft_hw_ocr' target='_blank'>craft_hw_ocr<\/a><\/p><p style='text-align: center'> <p style='text-align: center'><a href='https:\/\/github.com\/fcakyon\/craft-text-detector' target='_blank'>craft-text-detector<\/a><\/p><p style='text-align: center'>\"\ngr.Interface(fn=do_ocr, inputs=inputs, outputs=[o1, o2], title=title, description=description, article=article, examples=examples, enable_queue=True).launch()\n\n```"}
{"question":"\nI have a PyTorch implementation of the AnimeGAN-V2 model that can take an input image and transform it into an anime style. With my current code, I can load the pretrained model and run inference on an input image to generate the anime version of that image.\n\nI would like to create an interactive Gradio web interface that allows the user to apply the AnimeGAN-V2 model to videos, both from an uploaded file or directly from their webcam. The interface should have the following features:\n\n- Two tabs, one for uploading a video file and one for using the webcam as input \n- For the upload tab:\n  - A video file upload component to let the user select an input video\n  - A few example videos the user can quickly test\n- For both tabs:  \n  - Sliders to let the user select the start time and duration of the video clip to process\n  - Display the generated anime-style output video after running the model\n- Use a queueing system to handle multiple requests\n- Show processing status updates while the video is being generated\n- Provide an article section that links to the model's Github repo\n\nSome key aspects of the interface:\n- Break the input video into frames, run AnimeGAN on each frame, then reconstruct into an output video  \n- Enable uploading the output video with the original audio track added back\n- Allow setting the output video frame rate\n- Show two separate interfaces in a tabbed layout - one for upload and one for webcam input\n","answer":"here is the gradio solution for your use case: ```python\nimport gc\nimport math\nimport traceback\n\nimport gradio as gr\nimport numpy as np\nimport torch\nfrom encoded_video import EncodedVideo, write_video\nfrom PIL import Image\nfrom torchvision.transforms.functional import center_crop, to_tensor\n\n\n\n\nprint(\"\ud83e\udde0 Loading Model...\")\nmodel = torch.hub.load(\n    \"AK391\/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    device=\"cuda\",\n    progress=True,\n)\n\n\ndef face2paint(model: torch.nn.Module, img: Image.Image, size: int = 512, device: str = 'cuda'):\n    w, h = img.size\n    s = min(w, h)\n    img = img.crop(((w - s) \/\/ 2, (h - s) \/\/ 2, (w + s) \/\/ 2, (h + s) \/\/ 2))\n    img = img.resize((size, size), Image.LANCZOS)\n\n    with torch.no_grad():\n        input = to_tensor(img).unsqueeze(0) * 2 - 1\n        output = model(input.to(device)).cpu()[0]\n\n        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n\n    return output\n\n\n# This function is taken from pytorchvideo!\ndef uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = -3) -> torch.Tensor:\n    \"\"\"\n    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n    When num_samples is larger than the size of temporal dimension of the video, it\n    will sample frames based on nearest neighbor interpolation.\n    Args:\n        x (torch.Tensor): A video tensor with dimension larger than one with torch\n            tensor type includes int, long, float, complex, etc.\n        num_samples (int): The number of equispaced samples to be selected\n        temporal_dim (int): dimension of temporal to perform temporal subsample.\n    Returns:\n        An x-like Tensor with subsampled temporal dimension.\n    \"\"\"\n    t = x.shape[temporal_dim]\n    assert num_samples > 0 and t > 0\n    # Sample by nearest neighbor interpolation if num_samples > t.\n    indices = torch.linspace(0, t - 1, num_samples)\n    indices = torch.clamp(indices, 0, t - 1).long()\n    return torch.index_select(x, temporal_dim, indices)\n\n\n# This function is taken from pytorchvideo!\ndef short_side_scale(\n    x: torch.Tensor,\n    size: int,\n    interpolation: str = \"bilinear\",\n) -> torch.Tensor:\n    \"\"\"\n    Determines the shorter spatial dim of the video (i.e. width or height) and scales\n    it to the given size. To maintain aspect ratio, the longer side is then scaled\n    accordingly.\n    Args:\n        x (torch.Tensor): A video tensor of shape (C, T, H, W) and type torch.float32.\n        size (int): The size the shorter side is scaled to.\n        interpolation (str): Algorithm used for upsampling,\n            options: nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'\n    Returns:\n        An x-like Tensor with scaled spatial dims.\n    \"\"\"\n    assert len(x.shape) == 4\n    assert x.dtype == torch.float32\n    c, t, h, w = x.shape\n    if w < h:\n        new_h = int(math.floor((float(h) \/ w) * size))\n        new_w = size\n    else:\n        new_h = size\n        new_w = int(math.floor((float(w) \/ h) * size))\n\n    return torch.nn.functional.interpolate(x, size=(new_h, new_w), mode=interpolation, align_corners=False)\n\n\ndef inference_step(vid, start_sec, duration, out_fps):\n\n    clip = vid.get_clip(start_sec, start_sec + duration)\n    video_arr = torch.from_numpy(clip['video']).permute(3, 0, 1, 2)\n    audio_arr = np.expand_dims(clip['audio'], 0)\n    audio_fps = None if not vid._has_audio else vid._container.streams.audio[0].sample_rate\n\n    x = uniform_temporal_subsample(video_arr, duration * out_fps)\n    x = center_crop(short_side_scale(x, 512), 512)\n    x \/= 255.0\n    x = x.permute(1, 0, 2, 3)\n    with torch.no_grad():\n        output = model(x.to('cuda')).detach().cpu()\n        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n        output_video = output.permute(0, 2, 3, 1).numpy()\n\n    return output_video, audio_arr, out_fps, audio_fps\n\n\ndef predict_fn(filepath, start_sec, duration):\n    out_fps = 18\n    vid = EncodedVideo.from_path(filepath)\n    for i in range(duration):\n        print(f\"\ud83d\uddbc\ufe0f Processing step {i + 1}\/{duration}...\")\n        video, audio, fps, audio_fps = inference_step(vid=vid, start_sec=i + start_sec, duration=1, out_fps=out_fps)\n        gc.collect()\n        if i == 0:\n            video_all = video\n            audio_all = audio\n        else:\n            video_all = np.concatenate((video_all, video))\n            audio_all = np.hstack((audio_all, audio))\n\n    print(f\"\ud83d\udcbe Writing output video...\")\n    \n    try:\n        write_video('out.mp4', video_all, fps=fps, audio_array=audio_all, audio_fps=audio_fps, audio_codec='aac')\n    except Exception:\n        print(\"\u274c Error when writing with audio...trying without audio:\")\n        print(traceback.format_exc())\n        print()\n        print(f\"audio...\", audio.shape)\n        write_video('out.mp4', video_all, fps=fps)\n\n    print(f\"\u2705 Done!\")\n    del video_all\n    del audio_all\n\n    return 'out.mp4'\n\n\narticle = \"\"\"\n<p style='text-align: center'>\n    <a href='https:\/\/github.com\/bryandlee\/animegan2-pytorch' target='_blank'>Github Repo Pytorch<\/a>\n<\/p>\n\"\"\"\n\niface_webcam = gr.Interface(\n    predict_fn,\n    inputs=[\n        gr.Video(source=\"webcam\"),\n        gr.Slider(minimum=0, maximum=300, step=1, default=0),\n        gr.Slider(minimum=1, maximum=10, step=1, default=2),\n        # gr.inputs.Slider(minimum=6, maximum=18, step=6, default=12),  # Leaving manual fps out for now\n    ],\n    outputs=gr.Video(),\n    title='AnimeGANV2 On Videos',\n    description=\"Applying AnimeGAN-V2 to frames from video clips\",\n    article=article,\n    enable_queue=True,\n    allow_flagging=False,\n)\n\niface_file = gr.Interface(\n    predict_fn,\n    inputs=[\n        gr.Video(source=\"upload\"),\n        gr.Slider(minimum=0, maximum=300, step=1, default=0),\n        gr.Slider(minimum=1, maximum=10, step=1, default=2),\n    ],\n    outputs=gr.Video(),\n    title='AnimeGANV2 On Videos',\n    description=\"Applying AnimeGAN-V2 to frames from video clips\",\n    article=article,\n    enable_queue=True,\n    examples=[\n        ['driving.mp4', 0, 6],\n        ['bella_poarch.mp4', 4, 8],\n        ['obama.webm', 0, 4],\n    ],\n    allow_flagging=False,\n)\n\nif __name__ == '__main__':\n    gr.TabbedInterface(\n        interface_list=[iface_file, iface_webcam],\n        tab_names=[\"From a File!\", \"From your Webcam!\"]\n    ).launch()\n```"}
{"question":"\nI have a sentence transformer model that can encode text and images into embeddings. I also have a function that returns a fake set of images and labels. With my current code, I can calculate the embeddings for an input text or image, and theoretically use those to find similar images from a database.\n\nI would like to create a Gradio web interface to demonstrate this functionality with the following:\n\n- Two separate interfaces combined into a tabbed interface: one for \"text-to-image search\" and one for \"image-to-image search\" \n- For the text-to-image tab:\n    - A text input component for the user to enter a text query\n    - A gallery output component that displays the top matching images for the text query\n- For the image-to-image tab: \n    - An image upload component for the user to provide an input image\n    - A gallery output component that displays the top matching images for the query image\n- Execute the relevant search function whenever the user submits a text or image input\n- Display a set of fake images and labels in the output gallery since I don't have a real database of images set up yet\n\nThis will allow me to showcase my search system and get a feel for the user interface, even though I'm using a fake image database for now.\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport random\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('clip-ViT-B-32')\n\ndef fake_gan():\n    images = [\n        (random.choice(\n            [\n                \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/NASA-HS201427a-HubbleUltraDeepField2014-20140603.jpg\",\n                \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/73\/Cycliste_%C3%A0_place_d%27Italie-Paris.jpg\",\n                \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/31\/Great_white_shark_south_africa.jpg\",\n            ]\n        ), f\"label {i}\" if i != 0 else \"label\" * 50)\n        for i in range(3)\n    ]\n    return images\n\ndef search_images_from_text(text):\n    emb = model.encode(text)\n    return fake_gan()\n\ndef search_images_from_image(image):\n    image_emb = model.encode(image)\n    return fake_gan()\n\ndef main():\n    text_to_image_iface = gr.Interface(fn=search_images_from_text, inputs=\"text\", outputs=\"gallery\")\n    image_to_image_iface = gr.Interface(fn=search_images_from_image, inputs=\"image\", outputs=\"gallery\")\n    demo = gr.TabbedInterface([text_to_image_iface, image_to_image_iface], [\"Text query\", \"Image query\"])\n    demo.launch()\n    \nif __name__ == \"__main__\":\n    main()\n\n```"}
{"question":"\nI have a collection of diffusion-based image generation models from the Hugging Face diffusers library. With my current code, I can load a specified model, pass it a text prompt, and generate an image based on that prompt.\n\nI would like to create an interactive Gradio demo that allows the user to explore these different image generation models. The demo should include:\n\n- A dropdown to select the diffusion model to use from the available options \n- A text input box to enter an optional text prompt to guide the image generation (for models that support prompting)\n- After clicking submit, the selected model should be run with the given prompt to generate an image\n- The generated image should be displayed as the output\n- Provide some pre-populated example prompts\/model selections that the user can try out\n- Include an article explaining more about diffusion models and how they work\n- Add a description summarizing what the demo does\n\nThe demo should handle checking if the selected model supports prompting or not and show an error if an incompatible prompt is entered. It should also print any errors that occur when running the models.\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.diffusion import (\n    DiffusersGenerationAlgorithm,\n    DDPMGenerator,\n    DDIMGenerator,\n    ScoreSdeGenerator,\n    LDMTextToImageGenerator,\n    LDMGenerator,\n    StableDiffusionGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(model_type: str, prompt: str):\n\n    if prompt == \"\":\n        config = eval(f\"{model_type}()\")\n    else:\n        config = eval(f'{model_type}(prompt=\"{prompt}\")')\n    if config.modality != \"token2image\" and prompt != \"\":\n        raise ValueError(\n            f\"{model_type} is an unconditional generative model, please remove prompt (not={prompt})\"\n        )\n    model = DiffusersGenerationAlgorithm(config)\n    image = list(model.sample(1))[0]\n\n    return image\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"Diff\" in x[\"algorithm_name\"], all_algos))\n    ]\n    algos = [a for a in algos if not \"GeoDiff\" in a]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"Diffusion-based image generators\",\n        inputs=[\n            gr.Dropdown(\n                algos, label=\"Diffusion model\", value=\"StableDiffusionGenerator\"\n            ),\n            gr.Textbox(label=\"Text prompt\", placeholder=\"A blue tree\", lines=1),\n        ],\n        outputs=gr.outputs.Image(type=\"pil\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have an HTML error response page that displays a 404 \"File not found\" error. The page includes the error code, a brief message, and an explanation of the error code meaning.\n\nI would like to create a simple Gradio web interface that displays this error response page. The interface should include:\n- An HTML viewer component to display the error response page HTML code\n- A way to easily view how the HTML page looks when rendered in a web browser\n","answer":"here is the gradio solution for your use case: ```python\n<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 4.01\/\/EN\"\n        \"http:\/\/www.w3.org\/TR\/html4\/strict.dtd\">\n<html>\n    <head>\n        <meta http-equiv=\"Content-Type\" content=\"text\/html;charset=utf-8\">\n        <title>Error response<\/title>\n    <\/head>\n    <body>\n        <h1>Error response<\/h1>\n        <p>Error code: 404<\/p>\n        <p>Message: File not found.<\/p>\n        <p>Error code explanation: HTTPStatus.NOT_FOUND - Nothing matches the given URI.<\/p>\n    <\/body>\n<\/html>\n\n```"}
{"question":"\nI have a regression transformer model that can generate molecules with desired properties or predict properties of input molecules. With my current code, I can specify the model version, task (generate vs predict), input molecule, and various generation parameters. The model then outputs the generated molecules or predicted molecular properties.\n\nI would like to create an interactive Gradio demo for this regression transformer model with the following features:\n- A dropdown to select the model version from the available options \n- Radio buttons to choose between the \"Generate\" and \"Predict\" tasks\n- A text input box for the user to enter the input molecule string\n- A slider to select the number of molecules to generate (for the generate task)\n- Radio buttons to choose the search strategy (sample or greedy)\n- Sliders to adjust the decoding temperature and tolerance \n- A radio button to enable\/disable the sampling wrapper\n- Sliders and text inputs to specify the sampling wrapper parameters like fraction to mask, property goals, tokens to mask, and substructures to mask\/keep\n- Display the generated molecules or predicted properties in a nice grid layout\n- Include a description of the model and some example inputs\/outputs\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.conditional_generation.regression_transformer import (\n    RegressionTransformer,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\nfrom terminator.tokenization import PolymerGraphTokenizer\nfrom utils import (\n    draw_grid_generate,\n    draw_grid_predict,\n    get_application,\n    get_inference_dict,\n    get_rt_name,\n)\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef regression_transformer(\n    algorithm: str,\n    task: str,\n    target: str,\n    number_of_samples: int,\n    search: str,\n    temperature: float,\n    tolerance: int,\n    wrapper: bool,\n    fraction_to_mask: float,\n    property_goal: str,\n    tokens_to_mask: str,\n    substructures_to_mask: str,\n    substructures_to_keep: str,\n):\n\n    if task == \"Predict\" and wrapper:\n        logger.warning(\n            f\"For prediction, no sampling_wrapper will be used, ignoring: fraction_to_mask: {fraction_to_mask}, \"\n            f\"tokens_to_mask: {tokens_to_mask}, substructures_to_mask={substructures_to_mask}, \"\n            f\"substructures_to_keep: {substructures_to_keep}.\"\n        )\n        sampling_wrapper = {}\n    elif not wrapper:\n        sampling_wrapper = {}\n    else:\n        substructures_to_mask = (\n            []\n            if substructures_to_mask == \"\"\n            else substructures_to_mask.replace(\" \", \"\").split(\",\")\n        )\n        substructures_to_keep = (\n            []\n            if substructures_to_keep == \"\"\n            else substructures_to_keep.replace(\" \", \"\").split(\",\")\n        )\n        tokens_to_mask = [] if tokens_to_mask == \"\" else tokens_to_mask.split(\",\")\n\n        property_goals = {}\n        if property_goal == \"\":\n            raise ValueError(\n                \"For conditional generation you have to specify `property_goal`.\"\n            )\n        for line in property_goal.split(\",\"):\n            property_goals[line.split(\":\")[0].strip()] = float(line.split(\":\")[1])\n\n        sampling_wrapper = {\n            \"substructures_to_keep\": substructures_to_keep,\n            \"substructures_to_mask\": substructures_to_mask,\n            \"text_filtering\": False,\n            \"fraction_to_mask\": fraction_to_mask,\n            \"property_goal\": property_goals,\n        }\n    algorithm_application = get_application(algorithm.split(\":\")[0])\n    algorithm_version = algorithm.split(\" \")[-1].lower()\n    config = algorithm_application(\n        algorithm_version=algorithm_version,\n        search=search.lower(),\n        temperature=temperature,\n        tolerance=tolerance,\n        sampling_wrapper=sampling_wrapper,\n    )\n    model = RegressionTransformer(configuration=config, target=target)\n    samples = list(model.sample(number_of_samples))\n    polymer = isinstance(\n        config.generator.tokenizer.text_tokenizer, PolymerGraphTokenizer\n    )\n    if algorithm_version == \"rop_catalyst\" and task == \"Generate\":\n        correct_samples = [(s, p) for s, p in samples if \".\" in s]\n        while len(correct_samples) < number_of_samples:\n            samples = list(model.sample(number_of_samples))\n            correct_samples.extend(\n                [\n                    (s, p)\n                    for s, p in samples\n                    if \".\" in s and (s, p) not in correct_samples\n                ]\n            )\n        samples = correct_samples\n\n    if task == \"Predict\":\n        return draw_grid_predict(samples[0], target, domain=algorithm.split(\":\")[0])\n    else:\n        return draw_grid_generate(samples, domain=algorithm.split(\":\")[0])\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    rt_algos = list(\n        filter(lambda x: \"RegressionTransformer\" in x[\"algorithm_name\"], all_algos)\n    )\n    rt_names = list(map(get_rt_name, rt_algos))\n\n    properties = {}\n    for algo in rt_algos:\n        application = get_application(\n            algo[\"algorithm_application\"].split(\"Transformer\")[-1]\n        )\n        data = get_inference_dict(\n            application=application, algorithm_version=algo[\"algorithm_version\"]\n        )\n        properties[get_rt_name(algo)] = data\n    properties\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(\n        metadata_root.joinpath(\"regression_transformer_examples.csv\"), header=None\n    ).fillna(\"\")\n\n    with open(metadata_root.joinpath(\"regression_transformer_article.md\"), \"r\") as f:\n        article = f.read()\n    with open(\n        metadata_root.joinpath(\"regression_transformer_description.md\"), \"r\"\n    ) as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=regression_transformer,\n        title=\"Regression Transformer\",\n        inputs=[\n            gr.Dropdown(rt_names, label=\"Algorithm version\", value=\"Molecules: Qed\"),\n            gr.Radio(choices=[\"Predict\", \"Generate\"], label=\"Task\", value=\"Generate\"),\n            gr.Textbox(\n                label=\"Input\", placeholder=\"CC(C#C)N(C)C(=O)NC1=CC=C(Cl)C=C1\", lines=1\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n            gr.Radio(choices=[\"Sample\", \"Greedy\"], label=\"Search\", value=\"Sample\"),\n            gr.Slider(minimum=0.5, maximum=2, value=1, label=\"Decoding temperature\"),\n            gr.Slider(minimum=5, maximum=100, value=30, label=\"Tolerance\", step=1),\n            gr.Radio(choices=[True, False], label=\"Sampling Wrapper\", value=True),\n            gr.Slider(minimum=0, maximum=1, value=0.5, label=\"Fraction to mask\"),\n            gr.Textbox(label=\"Property goal\", placeholder=\"<qed>:0.75\", lines=1),\n            gr.Textbox(label=\"Tokens to mask\", placeholder=\"N, C\", lines=1),\n            gr.Textbox(\n                label=\"Substructures to mask\", placeholder=\"C(=O), C#C\", lines=1\n            ),\n            gr.Textbox(\n                label=\"Substructures to keep\", placeholder=\"C1=CC=C(Cl)C=C1\", lines=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a generative AI model called GeoDiff that can generate novel molecular structures based on a given text prompt describing desired molecular properties. The model takes in a prompt dictionary loaded from a pickle file, selects a specific prompt based on a prompt ID, and generates a specified number of molecular structures. With my current code, I can run the model generation and output the generated molecules as SMILES strings.\n\nI would like to create an interactive Gradio web interface for my GeoDiff model with the following features:\n- A file upload component for the user to upload their pickle file containing the prompt dictionary \n- A numeric input for the user to specify the prompt ID to use for generation\n- A slider to let the user select the number of molecular structures to generate (between 1 and 5)\n- Display the generated molecular structures visually in a grid after submitting\n- Include a description of the model and an article explaining more details\n- Provide some example inputs that the user can easily test\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport pickle\nimport gradio as gr\nfrom typing import Dict, Any\nimport pandas as pd\nfrom gt4sd.algorithms.generation.diffusion import (\n    DiffusersGenerationAlgorithm,\n    GeoDiffGenerator,\n)\nfrom utils import draw_grid_generate\nfrom rdkit import Chem\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(prompt_file: str, prompt_id: int, number_of_samples: int):\n\n    # Read file:\n    with open(prompt_file.name, \"rb\") as f:\n        prompts = pickle.load(f)\n\n    if all(isinstance(x, int) for x in prompts.keys()):\n        prompt = prompts[prompt_id]\n    else:\n        prompt = prompts\n\n    config = GeoDiffGenerator(prompt=prompt)\n    model = DiffusersGenerationAlgorithm(config)\n    results = list(model.sample(number_of_samples))\n    smiles = [Chem.MolToSmiles(m) for m in results]\n\n    return draw_grid_generate(samples=smiles, n_cols=5)\n\n\nif __name__ == \"__main__\":\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = [\n        [metadata_root.joinpath(\"mol_dct.pkl\"), 0, 2],\n        [metadata_root.joinpath(\"mol_dct.pkl\"), 1, 2],\n    ]\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"GeoDiff\",\n        inputs=[\n            gr.File(file_types=[\".pkl\"], label=\"GeoDiff prompt\"),\n            gr.Number(value=0, label=\"Prompt ID\", precision=0),\n            gr.Slider(minimum=1, maximum=5, value=2, label=\"Number of samples\", step=1),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples,\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model called MoLeR (MOlecule-LEvel Representation) that can generate novel molecules given input scaffolds and seed molecules. With my current code, I can specify model parameters like the algorithm version, beam size, sigma value, number of samples to generate, and random seed. The model then generates a set of novel molecules.\n\nI would like to create an interactive Gradio web interface for my MoLeR model with the following features:\n- Dropdown to select the algorithm version \n- Text input box for specifying scaffold molecules\n- Text input box for specifying seed molecules \n- Sliders to adjust the beam size, sigma value, and number of samples to generate\n- Number input for the random seed\n- Display the generated molecules in a grid after submitting\n- Show metadata like a description and example inputs\n- Allow launching the interface in debug mode to view any errors\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.moler import MoLeR, MoLeRDefaultGenerator\n\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nTITLE = \"MoLeR\"\n\n\ndef run_inference(\n    algorithm_version: str,\n    scaffolds: str,\n    seed_smiles: str,\n    beam_size: int,\n    sigma: float,\n    number_of_samples: int,\n    seed: int,\n):\n    config = MoLeRDefaultGenerator(\n        algorithm_version=algorithm_version,\n        scaffolds=scaffolds,\n        beam_size=beam_size,\n        num_samples=32,\n        seed=seed,\n        num_workers=1,\n        seed_smiles=seed_smiles,\n        sigma=sigma,\n    )\n    model = MoLeR(configuration=config)\n    samples = list(model.sample(number_of_samples))\n\n    scaffold_list = [] if scaffolds == \"\" else scaffolds.split(\".\")\n    seed_list = [] if seed_smiles == \"\" else seed_smiles.split(\".\")\n    return draw_grid_generate(seed_list, scaffold_list, samples)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(filter(lambda x: TITLE in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"MoLeR (MOlecule-LEvel Representation)\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"v0\"),\n            gr.Textbox(\n                label=\"Scaffolds\",\n                placeholder=\"CC(C#C)N(C)C(=O)NC1=CC=C(Cl)C=C1\",\n                lines=1,\n            ),\n            gr.Textbox(\n                label=\"Seed SMILES\",\n                placeholder=\"O=C1C2=CC=C(C3=CC=CC=C3)C=C=C2OC2=CC=CC=C12\",\n                lines=1,\n            ),\n            gr.Slider(minimum=1, maximum=5, value=1, step=1, label=\"Beams\"),\n            gr.Slider(minimum=0.0, maximum=3.0, value=0.01, label=\"Sigma\"),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n            gr.Number(value=42, label=\"Seed\", precision=0),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model that can generate molecular structures using the TorchDrug library. Specifically, I have implementations of the GCPN and GraphAF algorithms that can take in some configuration parameters and generate a specified number of novel molecular samples.\n\nWith my current code, I can:\n- Select either the GCPN or GraphAF algorithm to use\n- Specify the trained model version to load \n- Generate a specified number of molecular samples\n- Visualize the generated molecules in a grid\n\nI would like to create an interactive Gradio demo for my molecular generation models with the following:\n- A dropdown to select between the GCPN and GraphAF algorithms\n- A dropdown to select the specific trained model version to use\n- A slider to specify the number of molecular samples to generate (between 1-50) \n- Display the generated molecules in a grid after submitting\n- Provide a text description of the model background and capabilities\n- Show example generated molecules from each model\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.torchdrug import (\n    TorchDrugGenerator,\n    TorchDrugGCPN,\n    TorchDrugGraphAF,\n)\n\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nTITLE = \"MoLeR\"\n\n\ndef run_inference(algorithm: str, algorithm_version: str, number_of_samples: int):\n\n    if algorithm == \"GCPN\":\n        config = TorchDrugGCPN(algorithm_version=algorithm_version)\n    elif algorithm == \"GraphAF\":\n        config = TorchDrugGraphAF(algorithm_version=algorithm_version)\n    else:\n        raise ValueError(f\"Unsupported model {algorithm}.\")\n\n    model = TorchDrugGenerator(configuration=config)\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(samples=samples, n_cols=5)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"TorchDrug\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"TorchDrug (GCPN and GraphAF)\",\n        inputs=[\n            gr.Dropdown([\"GCPN\", \"GraphAF\"], label=\"Algorithm\", value=\"GCPN\"),\n            gr.Dropdown(\n                list(set(algos)), label=\"Algorithm version\", value=\"zinc250k_v0\"\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a set of generative models from the GuacaMol and MOSES libraries that can generate novel molecular structures. With my current code, I can specify the model configuration and generate a set of sample molecules.\n\nI would like to create an interactive Gradio demo to showcase these models with the following features:\n- A dropdown to select the specific model\/algorithm to use for generation\n- Sliders to control generation parameters like the sequence length and number of samples to generate\n- Display the generated molecular structures visually in a grid format\n- Provide example configurations that can be loaded into the interface\n- Include a description of the models and an informational article for users to learn more\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.conditional_generation.guacamol import (\n    AaeGenerator,\n    GraphGAGenerator,\n    GraphMCTSGenerator,\n    GuacaMolGenerator,\n    MosesGenerator,\n    OrganGenerator,\n    VaeGenerator,\n    SMILESGAGenerator,\n    SMILESLSTMHCGenerator,\n    SMILESLSTMPPOGenerator,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nTITLE = \"GuacaMol & MOSES\"\n\nCONFIG_FACTORY = {\n    \"Moses - AaeGenerator\": AaeGenerator,\n    \"Moses - VaeGenerator\": VaeGenerator,\n    \"Moses - OrganGenerator\": OrganGenerator,\n    \"GuacaMol - GraphGAGenerator\": GraphGAGenerator,\n    \"GuacaMol - GraphMCTSGenerator\": GraphMCTSGenerator,\n    \"GuacaMol - SMILESLSTMHCGenerator\": SMILESLSTMHCGenerator,\n    \"GuacaMol - SMILESLSTMPPOGenerator\": SMILESLSTMPPOGenerator,\n    \"GuacaMol - SMILESGAGenerator\": SMILESGAGenerator,\n}\n# OVERWRITE\nCONFIG_FACTORY = {\n    \"AaeGenerator\": AaeGenerator,\n    \"VaeGenerator\": VaeGenerator,\n    \"OrganGenerator\": OrganGenerator,\n}\nMODEL_FACTORY = {\"Moses\": MosesGenerator, \"GuacaMol\": GuacaMolGenerator}\n\n\ndef run_inference(\n    algorithm_version: str,\n    length: int,\n    # population_size: int,\n    # random_start: bool,\n    # patience: int,\n    # generations: int,\n    number_of_samples: int,\n):\n    config_class = CONFIG_FACTORY[algorithm_version]\n    # family = algorithm_version.split(\" - \")[0]\n    family = \"Moses\"\n    model_class = MODEL_FACTORY[family]\n\n    if family == \"Moses\":\n        kwargs = {\"n_samples\": number_of_samples, \"max_len\": length}\n    elif family == \"GuacaMol\":\n        kwargs = {\n            \"population_size\": population_size,\n            \"random_start\": random_start,\n            \"patience\": patience,\n            \"generations\": generations,\n        }\n        if \"MCTS\" in algorithm_version:\n            kwargs.pop(\"random_start\")\n        if \"LSTMHC\" in algorithm_version:\n            kwargs[\"max_len\"] = length\n            kwargs.pop(\"population_size\")\n            kwargs.pop(\"patience\")\n            kwargs.pop(\"generations\")\n        if \"LSTMPPO\" in algorithm_version:\n            kwargs = {}\n    else:\n        raise ValueError(f\"Unknown family {family}\")\n\n    config = config_class(**kwargs)\n\n    model = model_class(configuration=config, target={})\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(seeds=[], samples=samples, n_cols=5)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    guacamol_algos = [\n        \"GuacaMol - \" + x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"GuacaMol\" in x[\"algorithm_name\"], all_algos))\n    ]\n    moses_algos = [\n        \"Moses - \" + x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"Moses\" in x[\"algorithm_name\"], all_algos))\n    ]\n    algos = guacamol_algos + moses_algos\n\n    # Overwrite to have only Moses\n    algos = [\n        x[\"algorithm_application\"]\n        for x in list(filter(lambda x: \"Moses\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"MOSES\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"AaeGenerator\"),\n            gr.Slider(\n                minimum=5, maximum=500, value=100, label=\"Sequence length\", step=1\n            ),\n            # gr.Slider(\n            #     minimum=5, maximum=500, value=100, label=\"Population size\", step=1\n            # ),\n            # gr.Radio(choices=[True, False], label=\"Random start\", value=False),\n            # gr.Slider(minimum=1, maximum=10, value=4, label=\"Patience\"),\n            # gr.Slider(minimum=1, maximum=10, value=2, label=\"Generations\"),\n            gr.Slider(\n                minimum=1, maximum=50, value=5, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model called REINVENT that can take a primer molecule represented as a SMILES string and generate novel molecules similar to the primer molecule. With my current code, I can input a SMILES string, specify some generation parameters, and the model will output a grid of images showing the newly generated molecules.\n\nI would like to create an interactive Gradio demo for my REINVENT model with the following features:\n- A dropdown to select from the available pre-trained REINVENT model versions \n- A text input box for the user to paste in a primer SMILES string\n- A slider to adjust the maximum length of the generated SMILES strings\n- A radio button to toggle sampling uniquely on or off\n- A slider to select the number of new molecules to generate\n- Display the generated molecules in a grid of molecule images\n- Provide an article explaining more about the REINVENT model \n- Show a text description of what the demo does\n- Include some example primer SMILES strings that can be auto-loaded into the input box\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.conditional_generation.reinvent import Reinvent, ReinventGenerator\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(\n    algorithm_version: str,\n    smiles: str,\n    length: float,\n    sample_uniquely: bool,\n    number_of_samples: int,\n):\n\n    config = ReinventGenerator(\n        algorithm_version=algorithm_version,\n        max_sequence_length=length,\n        randomize=True,\n        sample_uniquely=sample_uniquely,\n    )\n    model = Reinvent(config, target=smiles)\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(samples=samples, n_cols=5, seeds=[smiles])\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"Reinvent\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"REINVENT\",\n        inputs=[\n            gr.Dropdown(\n                algos,\n                label=\"Algorithm version\",\n                value=\"v0\",\n            ),\n            gr.Textbox(\n                label=\"Primer SMILES\",\n                placeholder=\"FP(F)F.CP(C)c1ccccc1.[Au]\",\n                lines=1,\n            ),\n            gr.Slider(\n                minimum=5,\n                maximum=400,\n                value=100,\n                label=\"Maximal sequence length\",\n                step=1,\n            ),\n            gr.Radio(choices=[True, False], label=\"Sampling uniquely\", value=True),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model called PaccMannRL that can generate novel molecules either conditionally based on a protein target sequence or gene expression profile, or unconditionally. With my current code, I can run the model with specified parameters and generate a grid of molecule images as output.\n\nI would like to create an interactive Gradio web interface for my PaccMannRL model with the following features:\n- A dropdown to select the specific version of the PaccMannRL model to use\n- Radio buttons to choose between conditional or unconditional generation \n- A text input box for the user to enter the protein target sequence (for protein-based conditional generation)\n- A text input box for the user to enter the gene expression profile as space-separated values (for omics-based conditional generation) \n- Sliders to adjust hyperparameters like the decoding temperature, max sequence length, and number of samples to generate\n- Display the generated molecule images in a grid after submitting the inputs\n- Include an article explaining the model and example inputs that populate the interface\n\nThe interface should allow the user to select their desired options, click a submit button to run the model, and then display the generated molecule images. Please make the interface clean and easy to use.\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nfrom typing import List\n\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nfrom gt4sd.algorithms.conditional_generation.paccmann_rl import (\n    PaccMannRL,\n    PaccMannRLOmicBasedGenerator,\n    PaccMannRLProteinBasedGenerator,\n)\nfrom gt4sd.algorithms.generation.paccmann_vae import PaccMannVAE, PaccMannVAEGenerator\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(\n    algorithm_version: str,\n    inference_type: str,\n    protein_target: str,\n    omics_target: str,\n    temperature: float,\n    length: float,\n    number_of_samples: int,\n):\n    if inference_type == \"Unbiased\":\n        algorithm_class = PaccMannVAEGenerator\n        model_class = PaccMannVAE\n        target = None\n    elif inference_type == \"Conditional\":\n        if \"Protein\" in algorithm_version:\n            algorithm_class = PaccMannRLProteinBasedGenerator\n            target = protein_target\n        elif \"Omic\" in algorithm_version:\n            algorithm_class = PaccMannRLOmicBasedGenerator\n            try:\n                test_target = [float(x) for x in omics_target.split(\" \")]\n            except Exception:\n                raise ValueError(\n                    f\"Expected 2128 space-separated omics values, got {omics_target}\"\n                )\n            if len(test_target) != 2128:\n                raise ValueError(\n                    f\"Expected 2128 omics values, got {len(target)}: {target}\"\n                )\n            target = f\"[{omics_target.replace(' ', ',')}]\"\n        else:\n            raise ValueError(f\"Unknown algorithm version {algorithm_version}\")\n        model_class = PaccMannRL\n    else:\n        raise ValueError(f\"Unknown inference type {inference_type}\")\n\n    config = algorithm_class(\n        algorithm_version.split(\"_\")[-1],\n        temperature=temperature,\n        generated_length=length,\n    )\n    model = model_class(config, target=target)\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(samples=samples, n_cols=5)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_application\"].split(\"Based\")[0].split(\"PaccMannRL\")[-1]\n        + \"_\"\n        + x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"PaccMannRL\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"PaccMannRL\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"Protein_v0\"),\n            gr.Radio(\n                choices=[\"Conditional\", \"Unbiased\"],\n                label=\"Inference type\",\n                value=\"Conditional\",\n            ),\n            gr.Textbox(\n                label=\"Protein target\",\n                placeholder=\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTT\",\n                lines=1,\n            ),\n            gr.Textbox(\n                label=\"Gene expression target\",\n                placeholder=f\"{' '.join(map(str, np.round(np.random.rand(2128), 2)))}\",\n                lines=1,\n            ),\n            gr.Slider(minimum=0.5, maximum=2, value=1, label=\"Decoding temperature\"),\n            gr.Slider(\n                minimum=5,\n                maximum=400,\n                value=100,\n                label=\"Maximal sequence length\",\n                step=1,\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model called PaccMannGP that can generate novel molecules with optimized properties. The model takes in various parameters such as the target properties to optimize, protein target sequence, sampling settings, etc. With my current code, I can run the model with the given parameters and generate a set of molecules along with their predicted properties.\n\nI would like to create a Gradio web interface for my PaccMannGP model with the following features:\n- Dropdown to select the model version \n- Checkboxes to select the target properties to optimize\n- Text input for the protein target sequence\n- Sliders to adjust various model parameters like temperature, length, number of samples, etc.\n- Display the generated molecules in a grid along with their predicted properties \n- Allow selecting different pre-configured examples to populate the interface\n- Provide a description of the model and instructions on how to use the interface\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nfrom typing import List\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.controlled_sampling.paccmann_gp import (\n    PaccMannGPGenerator,\n    PaccMannGP,\n)\nfrom gt4sd.algorithms.controlled_sampling.paccmann_gp.implementation import (\n    MINIMIZATION_FUNCTIONS,\n)\n\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\nMINIMIZATION_FUNCTIONS.pop(\"callable\", None)\nMINIMIZATION_FUNCTIONS.pop(\"molwt\", None)\n\n\ndef run_inference(\n    algorithm_version: str,\n    targets: List[str],\n    protein_target: str,\n    temperature: float,\n    length: float,\n    number_of_samples: int,\n    limit: int,\n    number_of_steps: int,\n    number_of_initial_points: int,\n    number_of_optimization_rounds: int,\n    sampling_variance: float,\n    samples_for_evaluation: int,\n    maximum_number_of_sampling_steps: int,\n    seed: int,\n):\n\n    config = PaccMannGPGenerator(\n        algorithm_version=algorithm_version.split(\"_\")[-1],\n        batch_size=32,\n        temperature=temperature,\n        generated_length=length,\n        limit=limit,\n        acquisition_function=\"EI\",\n        number_of_steps=number_of_steps,\n        number_of_initial_points=number_of_initial_points,\n        initial_point_generator=\"random\",\n        number_of_optimization_rounds=number_of_optimization_rounds,\n        sampling_variance=sampling_variance,\n        samples_for_evaluation=samples_for_evaluation,\n        maximum_number_of_sampling_steps=maximum_number_of_sampling_steps,\n        seed=seed,\n    )\n    target = {i: {} for i in targets}\n    if \"affinity\" in targets:\n        if protein_target == \"\" or not isinstance(protein_target, str):\n            raise ValueError(\n                f\"Protein target must be specified for affinity prediction, not ={protein_target}\"\n            )\n        target[\"affinity\"][\"protein\"] = protein_target\n    else:\n        protein_target = \"\"\n\n    model = PaccMannGP(config, target=target)\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(\n        samples=samples,\n        n_cols=5,\n        properties=set(target.keys()),\n        protein_target=protein_target,\n    )\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(filter(lambda x: \"PaccMannGP\" in x[\"algorithm_name\"], all_algos))\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(\n        metadata_root.joinpath(\"examples.csv\"), header=None, sep=\"|\"\n    ).fillna(\"\")\n    examples[1] = examples[1].apply(eval)\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"PaccMannGP\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"v0\"),\n            gr.CheckboxGroup(\n                choices=list(MINIMIZATION_FUNCTIONS.keys()),\n                value=[\"qed\"],\n                multiselect=True,\n                label=\"Property goals\",\n            ),\n            gr.Textbox(\n                label=\"Protein target\",\n                placeholder=\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTT\",\n                lines=1,\n            ),\n            gr.Slider(minimum=0.5, maximum=2, value=1, label=\"Decoding temperature\"),\n            gr.Slider(\n                minimum=5,\n                maximum=400,\n                value=100,\n                label=\"Maximal sequence length\",\n                step=1,\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n            gr.Slider(minimum=1, maximum=8, value=4.0, label=\"Limit\"),\n            gr.Slider(minimum=1, maximum=32, value=8, label=\"Number of steps\", step=1),\n            gr.Slider(\n                minimum=1, maximum=32, value=4, label=\"Number of initial points\", step=1\n            ),\n            gr.Slider(\n                minimum=1,\n                maximum=4,\n                value=1,\n                label=\"Number of optimization rounds\",\n                step=1,\n            ),\n            gr.Slider(minimum=0.01, maximum=1, value=0.1, label=\"Sampling variance\"),\n            gr.Slider(\n                minimum=1,\n                maximum=10,\n                value=1,\n                label=\"Samples used for evaluation\",\n                step=1,\n            ),\n            gr.Slider(\n                minimum=1,\n                maximum=64,\n                value=4,\n                label=\"Maximum number of sampling steps\",\n                step=1,\n            ),\n            gr.Number(value=42, label=\"Seed\", precision=0),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model that can generate new catalyst molecules for advanced manufacturing applications. The model takes in several parameters such as a target binding energy, a primer molecule, a desired length, and some configuration settings. With my current code, I can run the model sampling process and generate a set of new candidate molecules.\n\nI would like to create an interactive Gradio web interface for my catalyst generation model with the following features:\n- A dropdown to select the model version \n- Sliders to adjust the various input parameters like target binding energy, length, number of points, steps, and samples\n- A text input box for specifying an optional primer molecule\n- Display the generated molecules in a grid of images\n- Show a description of the model and instructions on how to use the interface\n- Allow uploading a CSV file of example inputs to populate the interface\n\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.controlled_sampling.advanced_manufacturing import (\n    CatalystGenerator,\n    AdvancedManufacturing,\n)\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(\n    algorithm_version: str,\n    target_binding_energy: float,\n    primer_smiles: str,\n    length: float,\n    number_of_points: int,\n    number_of_steps: int,\n    number_of_samples: int,\n):\n\n    config = CatalystGenerator(\n        algorithm_version=algorithm_version,\n        number_of_points=number_of_points,\n        number_of_steps=number_of_steps,\n        generated_length=length,\n        primer_smiles=primer_smiles,\n    )\n    model = AdvancedManufacturing(config, target=target_binding_energy)\n    samples = list(model.sample(number_of_samples))\n    seeds = [] if primer_smiles == \"\" else [primer_smiles]\n\n    return draw_grid_generate(samples=samples, n_cols=5, seeds=seeds)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(\n            filter(lambda x: \"AdvancedManufact\" in x[\"algorithm_name\"], all_algos)\n        )\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n    print(\"Examples: \", examples.values.tolist())\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"Advanced Manufacturing\",\n        inputs=[\n            gr.Dropdown(\n                algos,\n                label=\"Algorithm version\",\n                value=\"v0\",\n            ),\n            gr.Slider(minimum=1, maximum=100, value=10, label=\"Target binding energy\"),\n            gr.Textbox(\n                label=\"Primer SMILES\",\n                placeholder=\"FP(F)F.CP(C)c1ccccc1.[Au]\",\n                lines=1,\n            ),\n            gr.Slider(\n                minimum=5,\n                maximum=400,\n                value=100,\n                label=\"Maximal sequence length\",\n                step=1,\n            ),\n            gr.Slider(\n                minimum=16, maximum=128, value=32, label=\"Number of points\", step=1\n            ),\n            gr.Slider(\n                minimum=16, maximum=128, value=50, label=\"Number of steps\", step=1\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        # examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a machine learning model called Polymer Blocks that can generate polymer sequences of a specified length. With my current code, I can specify the algorithm version, desired sequence length, and number of samples to generate. The model will then output the requested number of polymer sequences.\n\nI would like to create an interactive Gradio web interface for Polymer Blocks with the following features:\n- A dropdown to select the algorithm version from the available pretrained models\n- A slider to specify the maximum length of the generated polymer sequences (between 5-400) \n- A slider to choose the number of samples to generate (between 1-50)\n- Display the generated polymer sequences as a grid of images after submitting\n- Show a text description of the Polymer Blocks model and some example outputs\n- Allow launching the interface in debug mode to view detailed errors\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.algorithms.generation.polymer_blocks import (\n    PolymerBlocksGenerator,\n    PolymerBlocks,\n)\n\nfrom gt4sd.algorithms.registry import ApplicationsRegistry\n\nfrom utils import draw_grid_generate\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\ndef run_inference(algorithm_version: str, length: float, number_of_samples: int):\n\n    config = PolymerBlocksGenerator(\n        algorithm_version=algorithm_version,\n        batch_size=32,\n        generated_length=length,\n    )\n    model = PolymerBlocks(config)\n    samples = list(model.sample(number_of_samples))\n\n    return draw_grid_generate(samples=samples, n_cols=5, seeds=[])\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    all_algos = ApplicationsRegistry.list_available()\n    algos = [\n        x[\"algorithm_version\"]\n        for x in list(\n            filter(lambda x: \"PolymerBlocks\" in x[\"algorithm_name\"], all_algos)\n        )\n    ]\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = pd.read_csv(metadata_root.joinpath(\"examples.csv\"), header=None).fillna(\n        \"\"\n    )\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=run_inference,\n        title=\"Polymer Blocks\",\n        inputs=[\n            gr.Dropdown(algos, label=\"Algorithm version\", value=\"v0\"),\n            gr.Slider(\n                minimum=5,\n                maximum=400,\n                value=100,\n                label=\"Maximal sequence length\",\n                step=1,\n            ),\n            gr.Slider(\n                minimum=1, maximum=50, value=10, label=\"Number of samples\", step=1\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples.values.tolist(),\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a collection of machine learning models that can predict various molecular properties given a molecule's SMILES string representation. With my current code, I can load the appropriate model based on the selected property, run inference on either a single SMILES string or a file of multiple SMILES, and get the predicted property values as output.\n\nI would like to create an interactive Gradio web interface for these molecular property prediction models with the following features:\n- A dropdown to select the molecular property to predict from the available options \n- A text input box for the user to paste a single SMILES string\n- A file upload component to allow uploading a .smi file containing multiple SMILES strings\n- After entering a SMILES and clicking submit:\n    - Run the selected model and display the predicted property values\n    - Display a visualization of the input molecule\n    - Display the molecule SMILES and property values in a nicely formatted output\n- Provide a few pre-populated example SMILES strings for quick testing\n- Include a description of the tool and overview of the available properties\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nfrom gt4sd.properties.molecules import MOLECULE_PROPERTY_PREDICTOR_FACTORY\n\nfrom utils import draw_grid_predict\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nMOLFORMER_VERSIONS = {\n    \"molformer_classification\": [\"bace\", \"bbbp\", \"hiv\"],\n    \"molformer_regression\": [\n        \"alpha\",\n        \"cv\",\n        \"g298\",\n        \"gap\",\n        \"h298\",\n        \"homo\",\n        \"lipo\",\n        \"lumo\",\n        \"mu\",\n        \"r2\",\n        \"u0\",\n    ],\n    \"molformer_multitask_classification\": [\"clintox\", \"sider\", \"tox21\"],\n}\n\nREMOVE = [\"docking\", \"docking_tdc\", \"molecule_one\", \"askcos\", \"plogp\"]\nREMOVE.extend([\"similarity_seed\", \"activity_against_target\", \"organtox\"])\nREMOVE.extend(MOLFORMER_VERSIONS.keys())\n\nMODEL_PROP_DESCRIPTION = {\n    \"Tox21\": \"NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER, NR-ER-LBD, NR-PPAR-gamma, SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, SR-p53\",\n    \"Sider\": \"Hepatobiliary disorders,Metabolism and nutrition disorders,Product issues,Eye disorders,Investigations,Musculoskeletal disorders,Gastrointestinal disorders,Social circumstances,Immune system disorders,Reproductive system and breast disorders,Bening & malignant,General disorders,Endocrine disorders,Surgical & medical procedures,Vascular disorders,Blood & lymphatic disorders,Skin & subcutaneous disorders,Congenital & genetic disorders,Infections,Respiratory & thoracic disorders,Psychiatric disorders,Renal & urinary disorders,Pregnancy conditions,Ear disorders,Cardiac disorders,Nervous system disorders,Injury & procedural complications\",\n    \"Clintox\": \"FDA approval, Clinical trial failure\",\n}\n\n\ndef main(property: str, smiles: str, smiles_file: str):\n    if \"Molformer\" in property:\n        version = property.split(\" \")[-1].split(\"(\")[-1].split(\")\")[0]\n        property = property.split(\" \")[0]\n    algo, config = MOLECULE_PROPERTY_PREDICTOR_FACTORY[property.lower()]\n    kwargs = (\n        {\"algorithm_version\": \"v0\"} if property in MODEL_PROP_DESCRIPTION.keys() else {}\n    )\n    if property.lower() in MOLFORMER_VERSIONS.keys():\n        kwargs[\"algorithm_version\"] = version\n\n    model = algo(config(**kwargs))\n    if smiles != \"\" and smiles_file is not None:\n        raise ValueError(\"Pass either smiles or smiles_file, not both.\")\n    elif smiles != \"\":\n        smiles = [smiles]\n    elif smiles_file is not None:\n        smiles = pd.read_csv(smiles_file.name, header=None, sep=\"\\t\")[0].tolist()\n    props = np.array(list(map(model, smiles))).round(2)\n\n    # Expand to 2D array if needed\n    if len(props.shape) == 1:\n        props = np.expand_dims(np.array(props), -1)\n\n    if property in MODEL_PROP_DESCRIPTION.keys():\n        property_names = MODEL_PROP_DESCRIPTION[property].split(\",\")\n    else:\n        property_names = [property]\n\n    return draw_grid_predict(\n        smiles, props, property_names=property_names, domain=\"Molecules\"\n    )\n\n\nif __name__ == \"__main__\":\n    # Preparation (retrieve all available algorithms)\n    properties = list(MOLECULE_PROPERTY_PREDICTOR_FACTORY.keys())[::-1]\n    for prop in REMOVE:\n        prop_to_idx = dict(zip(properties, range(len(properties))))\n        properties.pop(prop_to_idx[prop])\n    properties = list(map(lambda x: x.capitalize(), properties))\n\n    # MolFormer options\n    for key in MOLFORMER_VERSIONS.keys():\n        properties.extend(\n            [f\"{key.capitalize()} ({version})\" for version in MOLFORMER_VERSIONS[key]]\n        )\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = [\n        [\"Qed\", \"\", metadata_root.joinpath(\"examples.smi\")],\n        [\n            \"Esol\",\n            \"CN1CCN(CCCOc2ccc(N3C(=O)C(=Cc4ccc(Oc5ccc([N+](=O)[O-])cc5)cc4)SC3=S)cc2)CC1\",\n            None,\n        ],\n    ]\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=main,\n        title=\"Molecular properties\",\n        inputs=[\n            gr.Dropdown(properties, label=\"Property\", value=\"Scscore\"),\n            gr.Textbox(\n                label=\"Single SMILES\",\n                placeholder=\"CC(C#C)N(C)C(=O)NC1=CC=C(Cl)C=C1\",\n                lines=1,\n            ),\n            gr.File(\n                file_types=[\".smi\"],\n                label=\"Multiple SMILES (tab-separated, `.smi` file)\",\n            ),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples,\n    )\n    demo.launch(debug=True, show_error=True, share=True)\n\n```"}
{"question":"\nI have a set of machine learning models that can predict various protein properties like aliphaticity, charge, isoelectric point, etc. from an input protein sequence. With my current code, I can take an input sequence or read sequences from a file, run the selected property prediction model, and get the predicted property values as output.\n\nI would like to create an interactive Gradio web interface for running protein property predictions with the following features:\n- A dropdown to select which protein property to predict \n- A text input box for the user to paste a single protein sequence\n- Alternatively, a file upload option to select a file containing protein sequences (one per line)\n- A radio button to specify the amide parameter (for applicable properties)\n- A slider to set the pH parameter (for applicable properties)\n- Display the predicted property values for the input sequence(s) after submitting\n- Show the predictions in a nicely formatted output, like a data table\n- Include examples of running different property predictions\n- Provide a description of the tool and how to use it\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport pathlib\n\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nfrom gt4sd.properties.proteins import PROTEIN_PROPERTY_PREDICTOR_FACTORY\n\nfrom utils import draw_grid_predict\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\n\nAMIDE_FNS = [\"protein_weight\", \"charge\", \"charge_density\", \"isoelectric_point\"]\nPH_FNS = [\"charge\", \"charge_density\", \"isoelectric_point\"]\n\n\ndef main(property: str, seq: str, seq_file: str, amide: bool, ph: float):\n    prop_name = property.lower()\n    algo, config = PROTEIN_PROPERTY_PREDICTOR_FACTORY[prop_name]\n\n    # Pass hyperparameters if applicable\n    kwargs = {}\n    if prop_name in AMIDE_FNS:\n        kwargs[\"amide\"] = amide\n    if prop_name in PH_FNS:\n        kwargs[\"ph\"] = ph\n    model = algo(config(**kwargs))\n\n    # Read and parse data\n    if seq != \"\" and seq_file is not None:\n        raise ValueError(\"Pass either smiles or seq_file, not both.\")\n    elif seq != \"\":\n        seqs = [seq]\n    elif seq_file is not None:\n        seqs = pd.read_csv(seq_file.name, header=None, sep=\"\\t\")[0].tolist()\n    props = np.array(list(map(model, seqs))).round(2)\n\n    # Expand to 2D array if needed\n    if len(props.shape) == 1:\n        props = np.expand_dims(np.array(props), -1)\n\n    return draw_grid_predict(seqs, props, property_names=[property], domain=\"Proteins\")\n\n\nif __name__ == \"__main__\":\n    # Preparation (retrieve all available algorithms)\n    properties = list(PROTEIN_PROPERTY_PREDICTOR_FACTORY.keys())[::-1]\n    properties = list(map(lambda x: x.capitalize(), properties))\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = [\n        [\"Aliphaticity\", \"\", metadata_root.joinpath(\"examples.smi\"), False, 7],\n        [\"Isoelectric_point\", \"KFLIYQMECSTMIFGL\", None, False, 7],\n        [\"Charge\", \"KFLIYQMECSTMIFGL\", None, True, 12],\n    ]\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=main,\n        title=\"Protein properties\",\n        inputs=[\n            gr.Dropdown(properties, label=\"Property\", value=\"Instability\"),\n            gr.Textbox(\n                label=\"Single Protein sequence\", placeholder=\"KFLIYQMECSTMIFGL\", lines=1\n            ),\n            gr.File(file_types=[\".smi\"], label=\"One AAS per line\"),\n            gr.Radio(choices=[True, False], label=\"Amide\", value=True),\n            gr.Slider(minimum=0, maximum=14, value=7, label=\"pH\", description=\"Blub\"),\n        ],\n        outputs=gr.HTML(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples,\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a set of machine learning models that can predict various properties of crystal structures based on input files in either CIF, CSV or ZIP format (containing multiple CIF files). The specific properties my models can predict include formation energy, bulk moduli, metal\/nonmetal classification, and others. With my current code, I can load the appropriate model based on the selected property, run the prediction on the input file(s), and output the results as a pandas DataFrame.\n\nI would like to create an interactive Gradio web interface for my crystal property prediction models with the following features:\n- A dropdown to select which property to predict, populated with the available properties \n- A file upload component that accepts .cif, .csv, or .zip files based on the property selected\n- After uploading the file(s) and clicking submit, display the prediction results in a nicely formatted DataFrame\n- Include a description of the interface and what it does\n- Provide some sample input files that can be used to quickly test the different property predictions\n","answer":"here is the gradio solution for your use case: ```python\nimport logging\nimport os\nimport pathlib\nimport shutil\nimport tempfile\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport gradio as gr\nimport pandas as pd\nfrom gt4sd.properties.crystals import CRYSTALS_PROPERTY_PREDICTOR_FACTORY\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\nsuffix_dict = {\"metal_nonmetal_classifier\": [\".csv\"]}\n\n\ndef create_temp_file(path: str) -> str:\n    temp_dir = tempfile.gettempdir()\n    temp_folder = os.path.join(temp_dir, \"gt4sd_crystal\")\n    os.makedirs(temp_folder, exist_ok=True)\n    # Clean up directory\n    for i in os.listdir(temp_folder):\n        print(\"Removing\", i)\n        os.remove(os.path.join(temp_folder, i))\n\n    temp_path = os.path.join(temp_folder, path.split(\"\/\")[-1])\n    shutil.copy2(path, temp_path)\n    return temp_path\n\n\ndef main(property: str, data_file: str):\n\n    print(data_file, data_file.orig_name, data_file.name)\n\n    if data_file is None:\n        raise TypeError(\"You have to pass either an input file for the crystal model\")\n\n    prop_name = property.replace(\" \", \"_\").lower()\n\n    # Copy file into a UNIQUE temporary directory\n    # if data_file.name.endswith(\"cfsdfsdsv\"):\n    #     file_path = Path(create_temp_file(data_file.orig_name))\n    # else:\n    #     file_path = Path(create_temp_file(data_file.name))\n    file_path = Path(create_temp_file(data_file.name))\n    folder = file_path.parent\n    if file_path.suffix == \".cif\":\n        if \".cif\" not in suffix_dict.get(prop_name, [\".cif\", \".zip\"]):\n            raise ValueError(\n                f\"For this property, provide {suffix_dict[prop_name]}, not `.cif`.\"\n            )\n        input_path = folder\n    elif file_path.suffix == \".csv\":\n        if \".csv\" not in suffix_dict.get(prop_name, [\".cif\", \".zip\"]):\n            raise ValueError(\n                f\"For this property, provide {suffix_dict.get(prop_name, ['.cif', '.zip'])}, not `.csv`.\"\n            )\n        input_path = file_path\n    elif file_path.suffix == \".zip\":\n        if \".zip\" not in suffix_dict.get(prop_name, [\".cif\", \".zip\"]):\n            raise ValueError(\n                f\"For this property, provide {suffix_dict[prop_name]}, not `.zip`.\"\n            )\n        # Unzip zip\n        shutil.unpack_archive(file_path, file_path.parent)\n        if len(list(filter(lambda x: x.endswith(\".cif\"), os.listdir(folder)))) == 0:\n            raise ValueError(\"No `.cif` files were found inside the `.zip`.\")\n        input_path = folder\n    else:\n        raise TypeError(\n            \"You have to pass a `.csv` (for `metal_nonmetal_classifier`),\"\n            \" a `.cif` (for all other properties) or a `.zip` with multiple\"\n            f\" `.cif` files. Not {type(data_file)}.\"\n        )\n\n    algo, config = CRYSTALS_PROPERTY_PREDICTOR_FACTORY[prop_name]\n    # Pass hyperparameters if applicable\n    kwargs = {\"algorithm_version\": \"v0\"}\n    model = algo(config(**kwargs))\n\n    result = model(input=input_path)\n    return pd.DataFrame(result)\n\n\nif __name__ == \"__main__\":\n\n    # Preparation (retrieve all available algorithms)\n    properties = list(CRYSTALS_PROPERTY_PREDICTOR_FACTORY.keys())[::-1]\n    properties = list(map(lambda x: x.replace(\"_\", \" \").title(), properties))\n\n    # Load metadata\n    metadata_root = pathlib.Path(__file__).parent.joinpath(\"model_cards\")\n\n    examples = [\n        [\"Formation Energy\", metadata_root.joinpath(\"7206075.cif\")],\n        [\"Bulk moduli\", metadata_root.joinpath(\"crystals.zip\")],\n        [\"Metal Nonmetal Classifier\", metadata_root.joinpath(\"metal.csv\")],\n        [\"Bulk moduli\", metadata_root.joinpath(\"9000046.cif\")],\n    ]\n\n    with open(metadata_root.joinpath(\"article.md\"), \"r\") as f:\n        article = f.read()\n    with open(metadata_root.joinpath(\"description.md\"), \"r\") as f:\n        description = f.read()\n\n    demo = gr.Interface(\n        fn=main,\n        title=\"Crystal properties\",\n        inputs=[\n            gr.Dropdown(properties, label=\"Property\", value=\"Instability\"),\n            gr.File(\n                file_types=[\".cif\", \".csv\", \".zip\"],\n                label=\"Input file for crystal model\",\n            ),\n        ],\n        outputs=gr.DataFrame(label=\"Output\"),\n        article=article,\n        description=description,\n        examples=examples,\n    )\n    demo.launch(debug=True, show_error=True)\n\n```"}
{"question":"\nI have a Python script that can load weather data from CSV files for two locations (Seoul and Washington). The script allows searching the data by specifying a date and selecting which weather elements to include. It can generate a Pandas dataframe with the matching weather data. The script can also visualize the selected weather data in a Matplotlib plot.\n\nI would like to create an interactive Gradio demo for my weather data script with the following:\n\n- Dropdown to select the month \n- Dropdown to select the day of the month\n- Checkbox group to select which weather elements to include (temperature, wind, humidity, air pressure)\n- Radio buttons to choose the location (Seoul or Washington) \n- Checkbox to optionally include precipitation data\n- The dataframe with matching weather data should be displayed as output\n- The plot visualizing the weather data over time should be displayed as output\n- Selecting different input options should interactively update the dataframe and plot outputs\n","answer":"here is the gradio solution for your use case: ```python\n\nimport gradio as gr\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# search weather data that is in csv file\ndef dataSearch(month, day, weather_elements, location, precipitation):\n  if location=='Seoul':\n    df = pd.read_csv('Seoul.csv')\n  elif location=='Washington':\n    df = pd.read_csv('Washington.csv')\n  if precipitation:\n    weather_elements.append('precipitation(mm)')\n  if day in ['1','2','3','4','5','6','7','8','9']:\n        today = '2022-'+month +'-0'+day\n  else:\n        today = '2022-'+month+'-'+day\n\n  df1 = df[df.date == today]\n  columns = ['location', 'date', 'time'] + weather_elements\n  df2 = df1.loc[:, columns]\n  \n  return df2\n\n# show weather data in plot using matplotlib\ndef showOutput(month, day, weather_elements, location, precipitation):\n  if month=='January':\n    month = '01'\n  elif month=='February':\n    month = '02'\n  elif month=='March':\n    month = '03'\n  elif month=='April':\n    month = '04'\n  elif month=='May':\n    month = '05'\n  elif month=='June':\n    month = '06'\n  elif month=='July':\n    month = '07'\n  elif month=='August':\n    month = '08'\n  elif month=='September':\n    month = '09'\n  elif month=='October':\n    month = '10'\n  elif month=='November':\n    month = '11'\n  elif month=='December':\n    month = '12'\n\n  weatherTable = dataSearch(month, day, weather_elements, location, precipitation)\n\n  if precipitation:\n    weather_elements.remove('precipitation(mm)')\n\n  if day in ['1','2','3','4','5','6','7','8','9']:\n        xname = '2022-'+month +'-0'+day\n  else:\n        xname = '2022-'+month+'-'+day\n\n  y_value=[0]*len(weather_elements)\n\n  x_value = weatherTable['time']\n\n  if 'humidity(%)' in weather_elements:\n    humidity_index = weather_elements.index('humidity(%)')\n    if weather_elements[humidity_index] != weather_elements[-1]:\n      temp = weather_elements[humidity_index]\n      weather_elements[humidity_index] = weather_elements[-1]\n      weather_elements[-1] = temp\n    for i in range(len(weather_elements)):\n      y_value[i] = weatherTable[weather_elements[i]]\n\n\n    if len(weather_elements) == 1:\n      weatherPlot = plt.figure(figsize=(10,10))\n      plt.title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      plt.xlabel(xname,labelpad=5, fontsize=15)\n      plt.ylabel(weather_elements[0], labelpad=15, fontsize=15)\n      plt.xticks(size=10, rotation=45)\n\n      plt.bar(x_value, y_value[-1], color='skyblue', label=weather_elements[0])\n      plt.legend(loc = \"upper left\")\n\n    elif len(weather_elements) == 2:\n      weatherPlot, ax1 = plt.subplots(figsize=(10,10))\n      plt.title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      plt.xticks(size=10, rotation=45)\n\n      ax1.bar(x_value, y_value[-1], color='skyblue', label=weather_elements[1])\n      ax1.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax1.set_ylabel(weather_elements[1], labelpad=15, fontsize=15)\n      ax1.legend(loc='upper left')\n\n      ax1_sub = ax1.twinx()\n      ax1_sub.plot(x_value, y_value[0], color='red', marker = \"o\", label=weather_elements[0])\n      ax1_sub.set_ylabel(weather_elements[0], labelpad=25, fontsize=15, rotation=270)\n      ax1_sub.legend(loc='upper right')\n      \n    elif len(weather_elements) == 3:\n      weatherPlot, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,15), constrained_layout=True)\n\n      line1 = ax1.plot(x_value, y_value[0], color='red', marker = \"o\", label=weather_elements[0])\n      ax1.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax1.set_ylabel(weather_elements[0], labelpad=15, fontsize=15)\n      ax1.set_title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      ax1.tick_params(axis='x', rotation=45, labelsize=10)\n\n      ax1_sub = ax1.twinx()\n      line2 = ax1_sub.plot(x_value, y_value[1], color='blue', marker = \"o\", label=weather_elements[1])\n      ax1_sub.set_ylabel(weather_elements[1], labelpad=25, fontsize=15, rotation=270)\n\n      lines = line1 + line2\n      labels = [l.get_label() for l in lines]\n      ax1.legend(lines, labels, loc='upper left')\n\n      ax2.bar(x_value, y_value[-1], color='skyblue', label=weather_elements[-1])\n      ax2.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax2.set_ylabel(weather_elements[-1], labelpad=15, fontsize=15)\n      ax2.tick_params(axis='x', rotation=45, labelsize=10)\n      ax2.legend(loc='upper right')\n\n    elif len(weather_elements) == 4:\n      weatherPlot, (ax1, ax2) = plt.subplots(2,1, figsize=(10,15), constrained_layout=True)\n\n      line1 = ax1.plot(x_value, y_value[0], color='red', marker = \"o\", label=weather_elements[0])\n      ax1.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax1.set_ylabel(weather_elements[0], labelpad=15, fontsize=15)\n      ax1.set_title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      ax1.tick_params(axis='x', rotation=45, labelsize=10)\n\n      ax1_sub = ax1.twinx()\n      line2 = ax1_sub.plot(x_value, y_value[1], color='blue', marker = \"o\", label=weather_elements[1])\n      ax1_sub.set_ylabel(weather_elements[1], labelpad=25, fontsize=15, rotation=270)\n\n      lines = line1 + line2\n      labels = [l.get_label() for l in lines]\n      ax1.legend(lines, labels, loc='upper left')\n\n      ax2.bar(x_value, y_value[-1], color='skyblue', label=weather_elements[-1])\n      ax2.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax2.set_ylabel(weather_elements[-1], labelpad=15, fontsize=15)\n      ax2.tick_params(axis='x', rotation=45, labelsize=10)\n      ax2.legend(loc='upper left')\n\n      ax2_sub = ax2.twinx()\n      ax2_sub.plot(x_value, y_value[2], color='gray', marker = \"o\", label=weather_elements[2])\n      ax2_sub.set_ylabel(weather_elements[2], labelpad=25, fontsize=15, rotation=270)\n      ax2_sub.legend(loc='upper right')\n\n      \n  else:\n    for i in range(len(weather_elements)):\n      y_value[i] = weatherTable[weather_elements[i]]\n\n    if len(weather_elements) == 1:\n      weatherPlot = plt.figure(figsize=(10,10))\n      plt.title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      plt.xlabel(xname,labelpad=5, fontsize=15)\n      plt.ylabel(weather_elements[0], labelpad=15, fontsize=15)\n      plt.xticks(size=10, rotation=45)\n      plt.plot(x_value, y_value[0], color='red', marker='o', label=weather_elements[0])\n      plt.legend(loc = \"upper left\")\n\n    elif len(weather_elements) == 2:\n      weatherPlot, ax1 = plt.subplots(figsize=(10,10))\n      plt.title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      plt.xticks(size=10, rotation=45)\n\n      line1 = ax1.plot(x_value, y_value[0], color='red', marker='o', label=weather_elements[0])\n      ax1.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax1.set_ylabel(weather_elements[0], labelpad=15, fontsize=15)\n\n      ax1_sub = ax1.twinx()\n      line2 = ax1_sub.plot(x_value, y_value[1], color='skyblue', marker='o', label=weather_elements[1])\n      ax1_sub.set_ylabel(weather_elements[1], labelpad=25, fontsize=15, rotation=270)\n\n      lines = line1 + line2\n      labels = [l.get_label() for l in lines]\n      ax1.legend(lines, labels, loc='upper left')\n      \n    elif len(weather_elements) == 3:\n      weatherPlot, (ax1, ax2) = plt.subplots(2,1, figsize=(10,15), constrained_layout=True)\n\n      line1 = ax1.plot(x_value, y_value[0], color='red', marker = \"o\", label=weather_elements[0])\n      ax1.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax1.set_ylabel(weather_elements[0], labelpad=15, fontsize=15)\n      ax1.set_title(\"2022 Weather Graph\", fontsize=20, fontweight='bold')\n      ax1.tick_params(axis='x', rotation=45, labelsize=10)\n\n      ax1_sub = ax1.twinx()\n      line2 = ax1_sub.plot(x_value, y_value[1], color='skyblue', marker = \"o\", label=weather_elements[1])\n      ax1_sub.set_ylabel(weather_elements[1], labelpad=25, fontsize=15, rotation=270)\n\n      lines = line1 + line2\n      labels = [l.get_label() for l in lines]\n      ax1.legend(lines, labels, loc='upper left')\n\n      ax2.plot(x_value, y_value[2], color='gray', marker = \"o\", label=weather_elements[2])\n      ax2.set_xlabel(xname, labelpad=5, fontsize=15)\n      ax2.set_ylabel(weather_elements[2], labelpad=15, fontsize=15)\n      ax2.tick_params(axis='x', rotation=45, labelsize=10)\n      ax2.legend(loc='upper left')\n\n  return [weatherTable, weatherPlot]\n\noutput1 = gr.Dataframe()\noutput2 = gr.Plot()\n\n# make gradio interface\ndemo = gr.Interface(\n    fn=showOutput,\n    inputs=[\n        gr.Dropdown([\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n                     \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n                     label=\"Month\", info=\"Select Months\"),\n        gr.Dropdown([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",\n                     \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\",\n                     \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"], label=\"Day\", info=\"Select Day\"),\n        gr.CheckboxGroup([\"temperature(\u02daC)\", \"wind(m\/s)\", \"humidity(%)\", \"air_pressure(hPa)\"],\n                         label=\"Weather element\", info=\"Choose weather element\"),\n        gr.Radio([\"Washington\", \"Seoul\"],\n                 label=\"Location\", info=\"Choose location\"),\n        gr.Checkbox(label=\"precipation?\")],\n        outputs=[output1,output2]\n)\n\nif __name__==\"__main__\":\n  demo.launch()\n```"}
{"question":"\nI have a semantic segmentation model that can identify the left ventricle in echocardiogram images. The model was trained on the EchoNet-Dynamic dataset of 10k echocardiogram videos with expert annotations. With my current code, I can input an echocardiogram image and the model will output a prediction mask highlighting the location of the left ventricle.\n\nI would like to create an interactive Gradio demo for this left ventricle segmentation model with the following features:\n- An image upload component for the user to provide an input echocardiogram image \n- Run the segmentation model when an image is uploaded and display the output image with the left ventricle highlighted\n- Provide a few example images that can be selected to test the model\n- Display a description of the model, dataset, and application\n- Disable user flagging and analytics for this demo\n- Launch the Gradio interface\n","answer":"here is the gradio solution for your use case: ```python\nimport os, os.path\nfrom os.path import splitext\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport wget \n\n\ndestination_folder = \"output\"\ndestination_for_weights = \"weights\"\n\nif os.path.exists(destination_for_weights):\n    print(\"The weights are at\", destination_for_weights)\nelse:\n    print(\"Creating folder at \", destination_for_weights, \" to store weights\")\n    os.mkdir(destination_for_weights)\n    \nsegmentationWeightsURL = 'https:\/\/github.com\/douyang\/EchoNetDynamic\/releases\/download\/v1.0.0\/deeplabv3_resnet50_random.pt'\n\nif not os.path.exists(os.path.join(destination_for_weights, os.path.basename(segmentationWeightsURL))):\n    print(\"Downloading Segmentation Weights, \", segmentationWeightsURL,\" to \",os.path.join(destination_for_weights, os.path.basename(segmentationWeightsURL)))\n    filename = wget.download(segmentationWeightsURL, out = destination_for_weights)\nelse:\n    print(\"Segmentation Weights already present\")\n\ntorch.cuda.empty_cache()\n\ndef collate_fn(x):\n    x, f = zip(*x)\n    i = list(map(lambda t: t.shape[1], x))\n    x = torch.as_tensor(np.swapaxes(np.concatenate(x, 1), 0, 1))\n    return x, f, i\n\nmodel = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, aux_loss=False)\nmodel.classifier[-1] = torch.nn.Conv2d(model.classifier[-1].in_channels, 1, kernel_size=model.classifier[-1].kernel_size)\n\nprint(\"loading weights from \", os.path.join(destination_for_weights, \"deeplabv3_resnet50_random\"))\n\nif torch.cuda.is_available():\n    print(\"cuda is available, original weights\")\n    device = torch.device(\"cuda\")\n    model = torch.nn.DataParallel(model)\n    model.to(device)\n    checkpoint = torch.load(os.path.join(destination_for_weights, os.path.basename(segmentationWeightsURL)))\n    model.load_state_dict(checkpoint['state_dict'])\nelse:\n    print(\"cuda is not available, cpu weights\")\n    device = torch.device(\"cpu\")\n    checkpoint = torch.load(os.path.join(destination_for_weights, os.path.basename(segmentationWeightsURL)), map_location = \"cpu\")\n    state_dict_cpu = {k[7:]: v for (k, v) in checkpoint['state_dict'].items()}\n    model.load_state_dict(state_dict_cpu)\n\nmodel.eval()\n\ndef segment(inp):\n    x = inp.transpose([2, 0, 1])  #  channels-first\n    x = np.expand_dims(x, axis=0)  # adding a batch dimension    \n    \n    mean = x.mean(axis=(0, 2, 3))\n    std = x.std(axis=(0, 2, 3))\n    x = x - mean.reshape(1, 3, 1, 1)\n    x = x \/ std.reshape(1, 3, 1, 1)\n    \n    with torch.no_grad():\n        x = torch.from_numpy(x).type('torch.FloatTensor').to(device)\n        output = model(x)    \n    \n    y = output['out'].numpy()\n    y = y.squeeze()\n    \n    out = y>0    \n    \n    mask = inp.copy()\n    mask[out] = np.array([0, 0, 255])\n    \n    return mask\n\nimport gradio as gr\n\ni = gr.Image(shape=(112, 112))\no = gr.Image()\n\nexamples = [[\"img1.jpg\"], [\"img2.jpg\"]]\ntitle = None #\"Left Ventricle Segmentation\"\ndescription = \"This semantic segmentation model identifies the left ventricle in echocardiogram images.\"\n# videos. Accurate evaluation of the motion and size of the left ventricle is crucial for the assessment of cardiac function and ejection fraction. In this interface, the user inputs apical-4-chamber images from echocardiography videos and the model will output a prediction of the localization of the left ventricle in blue. This model was trained on the publicly released EchoNet-Dynamic dataset of 10k echocardiogram videos with 20k expert annotations of the left ventricle and published as part of \u2018Video-based AI for beat-to-beat assessment of cardiac function\u2019 by Ouyang et al. in Nature, 2020.\"\nthumbnail = \"https:\/\/raw.githubusercontent.com\/gradio-app\/hub-echonet\/master\/thumbnail.png\"\ngr.Interface(segment, i, o, examples=examples, allow_flagging=False, analytics_enabled=False,\n\ttitle=title, description=description, thumbnail=thumbnail).launch()\n\n```"}
{"question":"\nI have a named entity recognition model that can extract mentions of adverse drug reactions (ADRs) and the associated drug from unstructured text. With my current code, I can input a piece of text and the model will output the text with the extracted ADR and drug entities highlighted.\n\nI would like to create an interactive Gradio web interface for my ADR extraction model with the following features:\n- A text input box allowing the user to paste in a paragraph of text to extract ADRs and drug mentions from\n- Display the output text with the extracted ADR and drug entities stylistically highlighted in different colors\n- Include a description of what the interface does and the definition of an adverse drug reaction \n- Provide several example inputs that demonstrate the model's ADR and drug extraction capabilities\n- Use the \"huggingface\" theme and a horizontal layout for the interface\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nfrom spacy import displacy\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"abhibisht89\/spanbert-large-cased-finetuned-ade_corpus_v2\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"abhibisht89\/spanbert-large-cased-finetuned-ade_corpus_v2\").to('cpu')\nadr_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer,grouped_entities=True)      \n\ndef get_adr_from_text(sentence):\n    tokens = adr_ner_model(sentence)\n    entities = []\n    \n    for token in tokens:\n        label = token[\"entity_group\"]\n        if label != \"O\":\n            token[\"label\"] = label\n            entities.append(token)\n    \n    params = [{\"text\": sentence,\n               \"ents\": entities,\n               \"title\": None}]\n    \n    html = displacy.render(params, style=\"ent\", manual=True, options={\n        \"colors\": {\n                   \"DRUG\": \"#f08080\",\n                   \"ADR\": \"#9bddff\",\n               },\n    })\n    return html\n\nexp=[\"Abortion, miscarriage or uterine hemorrhage associated with misoprostol (Cytotec), a labor-inducing drug.\",\n    \"Addiction to many sedatives and analgesics, such as diazepam, morphine, etc.\",\n    \"Birth defects associated with thalidomide\",\n    \"Bleeding of the intestine associated with aspirin therapy\",\n    \"Cardiovascular disease associated with COX-2 inhibitors (i.e. Vioxx)\",\n    \"Deafness and kidney failure associated with gentamicin (an antibiotic)\",\n    \"Having fever after taking paracetamol\"]\n\ndesc=\"An adverse drug reaction (ADR) can be defined as an appreciably harmful or unpleasant reaction resulting from an intervention related to the use of a medicinal product.\\\n The goal of this project is to extracts the adverse drug reaction from unstructured text with the Drug.\"\n\ninp=gr.inputs.Textbox(lines=5, placeholder=None, default=\"\", label=\"text to extract adverse drug reaction and drug mention\")\nout=gr.outputs.HTML(label=None)\n\niface = gr.Interface(fn=get_adr_from_text, inputs=inp, outputs=out,examples=exp,article=desc,title=\"Adverse Drug Reaction Xtractor\",theme=\"huggingface\",layout='horizontal')\niface.launch()\n```"}
{"question":"\nI have a Detectron2 machine learning model that can perform instance segmentation on microscope images of cells to detect and classify different cell types. With my current code, I can load the trained model, run inference on an input image, and generate an output image with the detected cell instances color-coded by type.\n\nI would like to create an interactive Gradio web interface for this cell segmentation model with the following features:\n- Allow the user to upload an image file \n- Run the segmentation model inference when the user submits an image\n- Display the output image with the detected cells highlighted and the predicted cell type as the title\n- Use a plot output component to render the output image with a custom color map and alpha blending\n- Include an article section explaining the model and linking to relevant resources\n- Show an example input image to demonstrate the interface\n- Enable the submit button queueing feature to handle multiple user submissions\n","answer":"here is the gradio solution for your use case: ```python\nimport os \nos.system('pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.9\/index.html')\nos.system('pip install torch==1.9.0 torchvision==0.10.0')\n\nimport gradio as gr\n# check pytorch installation: \nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\nassert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nimport numpy as np\nimport os, json, random\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom PIL import Image\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\n\n\ncfg = get_cfg()\ncfg.MODEL.DEVICE='cpu'\ncfg.INPUT.MASK_FORMAT='bitmask'\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \ncfg.TEST.DETECTIONS_PER_IMAGE = 1000\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\ncfg.MODEL.WEIGHTS = \"model_final.pth\"\n\npredictor = DefaultPredictor(cfg)\n\n\ndef inference(img):\n    class_names = ['0','1','2'] #['astro', 'cort', 'sh-sy5y']\n    im = np.asarray(Image.open(img).convert('RGB'))\n    outputs = predictor(im)\n    pred_classes = outputs['instances'].pred_classes.cpu().numpy().tolist()\n    take = outputs['instances'].scores >= 0.5 #Threshold\n    pred_masks = outputs['instances'].pred_masks[take].cpu().numpy()\n    pred_class = max(set(pred_classes), key=pred_classes.count)\n\n    mask = np.stack(pred_masks)\n    mask = np.any(mask == 1, axis=0)\n\n    p = plt.imshow(im,cmap='gray')\n    p = plt.imshow(mask, alpha=0.4)\n    p = plt.xticks(fontsize=8)\n    p = plt.yticks(fontsize=8)\n    p = plt.title(\"cell type: \" + class_names[pred_class])\n\n    return plt\n\n\n\n\ntitle = \"Sartorius Cell Instance Segmentation\"\ndescription = \"Sartorius Cell Instance Segmentation Demo: Current Kaggle competition - kaggle.com\/c\/sartorius-cell-instance-segmentation\"\narticle = \"<p style='text-align: center'><a href='https:\/\/ai.facebook.com\/blog\/-detectron2-a-pytorch-based-modular-object-detection-library-\/' target='_blank'>Detectron2: A PyTorch-based modular object detection library<\/a> | <a href='https:\/\/github.com\/facebookresearch\/detectron2' target='_blank'>Github Repo<\/a><\/p>\"\nexamples = [['0030fd0e6378.png']]  \ngr.Interface(inference, inputs=gr.inputs.Image(type=\"filepath\"), outputs=gr.outputs.Image('plot')  ,enable_queue=True, title=title,\n    description=description,\n    article=article,\n    examples=examples).launch(debug=False)\n\n```"}
{"question":"\nI have a login system for a medical web application that checks the provided username and password against a database. If the username and password are valid, it logs the user in. If the username or password are invalid, it returns an error message.\n\nI would like to create a simple Gradio demo of this login system with the following:\n- A text input for the user to enter their username\n- A password input for the user to enter their password \n- A submit button to attempt the login\n- Display a success message if the login succeeds\n- Display an \"Invalid username or password\" error message if the login fails\n","answer":"here is the gradio solution for your use case: ```python\nInvalid username or password.\n```"}
{"question":"\nI have a Tacotron2 model trained on Mandarin Chinese that can generate speech audio from input text using the Coqui TTS toolkit. With my current code, I can input Mandarin text and synthesize it into speech audio.\n\nI would like to create a Gradio web interface for this text-to-speech model with the following:\n- A text input box for the user to enter the Mandarin text they want converted to speech\n- Generate and play the output audio automatically after entering the text (no submit button needed)\n- Display the input text and output audio waveform \n- Provide some example Mandarin text inputs that the user can easily test out\n- A short description of what the model does\n- Links to more information such as a blog post explaining the model and code\n","answer":"here is the gradio solution for your use case: ```python\nimport tempfile\nimport gradio as gr\n# import numpy as np\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\n\n\ntitle = \"Mandarin Text-to-Speech (TTS)\"\ndescription = \"Generate mandarin speech from text using a Tachotron2 model with Coqui TTS \ud83d\udc38, \" \\\n              \"a deep learning toolkit for Text-to-Speech.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/news.machinelearning.sg\/posts\/mandarin_text_to_speech_with_coqui_tts\/' target='blank' class='footer'>Blog<\/a> | <a href='https:\/\/github.com\/eugenesiow\/practical-ml' target='_blank' class='footer'>Github Repo<\/a><\/p>\"\nexamples = [\n    [\"\u8bed\u97f3\u5408\u6210\u662f\u901a\u8fc7\u673a\u68b0\u7684\u3001\u7535\u5b50\u7684\u65b9\u6cd5\u4ea7\u751f\u4eba\u9020\u8bed\u97f3\u7684\u6280\u672f\u3002\"],\n    [\"\u674e\u663e\u9f99\u603b\u7406\u8868\u793a\uff0c\u6211\u56fd\u8981\u8fbe\u5230\u50cf\u610f\u5927\u5229\u7684\u5f00\u653e\u7a0b\u5ea6\uff0c\u51e0\u4e4e\u56de\u5230\u51a0\u75c5\u75ab\u60c5\u524d\u7684\u751f\u6d3b\uff0c\u8fd8\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\u3002\"]\n]\n\n\nmanager = ModelManager()\nmodel_path, config_path, model_item = manager.download_model(\"tts_models\/zh-CN\/baker\/tacotron2-DDC-GST\")\nsynthesizer = Synthesizer(\n    model_path, config_path, None, None, None,\n)\n\n\ndef inference(text: str):\n    wavs = synthesizer.tts(text)\n    # output = (synthesizer.output_sample_rate, np.array(wavs).astype(np.float32))\n    # return output\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        synthesizer.save_wav(wavs, fp)\n        return fp.name\n\n\ngr.Interface(\n    fn=inference,\n    inputs=[\n        gr.inputs.Textbox(\n            label=\"Input\",\n            default=\"\u4f60\u597d\u5417\uff1f\u6211\u5f88\u597d\u3002\",\n        ),\n    ],\n    outputs=gr.outputs.Audio(label=\"Output\"),\n    title=title,\n    description=description,\n    article=article,\n    examples=examples,\n    enable_queue=True,\n    allow_flagging=False,\n    ).launch(debug=False)\n\n```"}
{"question":"\nI have a music source separation model called Demucs that can take an audio file as input and separate out the vocals, bass, drums and other instrumental components into separate audio files. With my current code, I can load an audio file, run the Demucs model, and save the separated components as output .wav files.\n\nI would like to create an interactive Gradio web demo for my Demucs model with the following features:\n- Allow the user to upload an audio file \n- Display the uploaded audio with playback controls\n- Add a button to run the Demucs separation on the uploaded audio\n- Display the separated vocals, bass, drums and other components as audio outputs that can be played back\n- Provide some example audio files the user can quickly load to test out the demo\n- Add a title, description and links to the Demucs research paper and code repository\n","answer":"here is the gradio solution for your use case: ```python\nimport os\nimport gradio as gr\nfrom scipy.io.wavfile import write\n\n\ndef inference(audio):\n  os.makedirs(\"out\", exist_ok=True)\n  write('test.wav', audio[0], audio[1])\n  os.system(\"python3 -m demucs.separate -n mdx_extra_q -d cpu test.wav -o out\")\n  return \".\/out\/mdx_extra_q\/test\/vocals.wav\",\".\/out\/mdx_extra_q\/test\/bass.wav\",\\\n\".\/out\/mdx_extra_q\/test\/drums.wav\",\".\/out\/mdx_extra_q\/test\/other.wav\"\n  \ntitle = \"Demucs\"\ndescription = \"Gradio demo for Demucs: Music Source Separation in the Waveform Domain. To use it, simply upload your audio, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/arxiv.org\/abs\/1911.13254' target='_blank'>Music Source Separation in the Waveform Domain<\/a> | <a href='https:\/\/github.com\/facebookresearch\/demucs' target='_blank'>Github Repo<\/a><\/p>\"\n\nexamples=[['test.mp3']]\ngr.Interface(\n    inference, \n    gr.inputs.Audio(type=\"numpy\", label=\"Input\"), \n    [gr.outputs.Audio(type=\"filepath\", label=\"Vocals\"),gr.outputs.Audio(type=\"filepath\", label=\"Bass\"),gr.outputs.Audio(type=\"filepath\", label=\"Drums\"),gr.outputs.Audio(type=\"filepath\", label=\"Other\")],\n    title=title,\n    description=description,\n    article=article,\n    examples=examples\n    ).launch(enable_queue=True)\n```"}
{"question":"\nI have a speech translation model called XLS-R 2B 22-to-16 from Facebook that can translate speech from 22 different languages into text in 16 different target languages. The model takes in an audio file as input and outputs the translated text.\n\nWith my current code, I can load the pretrained model, process an input audio file, and generate the translated text output for a specified target language. \n\nI would like to create an interactive Gradio web interface for this speech translation model with the following features:\n\n- Allow the user to record audio from their microphone as input \n- And\/or allow the user to upload an audio file as input\n- Let the user select the target language to translate to from a dropdown of the supported 16 languages\n- Display a warning message if the user tries to use both mic recording and audio upload, indicating only the recording will be used\n- Display an error if the user doesn't provide any audio input\n- Process the audio input and display the translated text output from the model\n- Provide some information about the model and link to learn more\n- Enable the submit button queue to handle multiple submissions\n- Use the Hugging Face theme to style the interface\n\n","answer":"here is the gradio solution for your use case: ```python\nimport os\nos.system(\"pip install gradio==2.8.0b2\")\nimport gradio as gr\nimport librosa\nfrom transformers import AutoFeatureExtractor, AutoTokenizer, SpeechEncoderDecoderModel\nimport torch\n\nmodel_name = \"facebook\/wav2vec2-xls-r-2b-22-to-16\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = SpeechEncoderDecoderModel.from_pretrained(model_name).to(device)\n\nif torch.cuda.is_available():\n    model.half()\n\ndef process_audio_file(file):\n    data, sr = librosa.load(file)\n    if sr != 16000:\n        data = librosa.resample(data, sr, 16000)\n    print(data.shape)\n    input_values = feature_extractor(data, return_tensors=\"pt\").input_values.to(device)\n    \n    if torch.cuda.is_available():\n        input_values = input_values.to(torch.float16)\n    return input_values\n    \ndef transcribe(file_mic, file_upload, target_language):\n    \n    target_code = target_language.split(\"(\")[-1].split(\")\")[0]\n    forced_bos_token_id = MAPPING[target_code]\n    \n    warn_output = \"\"\n    if (file_mic is not None) and (file_upload is not None):\n       warn_output = \"WARNING: You've uploaded an audio file and used the microphone. The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n       file = file_mic\n    elif (file_mic is None) and (file_upload is None):\n       return \"ERROR: You have to either use the microphone or upload an audio file\"\n    elif file_mic is not None:\n       file = file_mic\n    else:\n       file = file_upload\n       \n    input_values = process_audio_file(file)\n    \n    sequences = model.generate(input_values, forced_bos_token_id=forced_bos_token_id)\n    \n    transcription = tokenizer.batch_decode(sequences, skip_special_tokens=True)\n    return warn_output + transcription[0]\n    \ntarget_language = [\n    \"English (en)\",\n    \"German (de)\",\n    \"Turkish (tr)\",\n    \"Persian (fa)\",\n    \"Swedish (sv)\",\n    \"Mongolian (mn)\",\n    \"Chinese (zh)\",\n    \"Welsh (cy)\",\n    \"Catalan (ca)\",\n    \"Slovenian (sl)\",\n    \"Estonian (et)\",\n    \"Indonesian (id)\",\n    \"Arabic (ar)\",\n    \"Tamil (ta)\",\n    \"Latvian (lv)\",\n    \"Japanese (ja)\",\n]\n\nMAPPING = {\n    \"en\": 250004,\n    \"de\": 250003,\n    \"tr\": 250023,\n    \"fa\": 250029,\n    \"sv\": 250042,\n    \"mn\": 250037,\n    \"zh\": 250025,\n    \"cy\": 250007,\n    \"ca\": 250005,\n    \"sl\": 250052,\n    \"et\": 250006,\n    \"id\": 250032,\n    \"ar\": 250001,\n    \"ta\": 250044,\n    \"lv\": 250017,\n    \"ja\": 250012,\n}\n    \niface = gr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.inputs.Audio(source=\"microphone\", type='filepath', optional=True),\n        gr.inputs.Audio(source=\"upload\", type='filepath', optional=True),\n        gr.inputs.Dropdown(target_language),\n    ],\n    outputs=\"text\",\n    layout=\"horizontal\",\n    theme=\"huggingface\",\n    title=\"XLS-R 2B 22-to-16 Speech Translation\",\n    description=\"A simple interface to translate from 22 input spoken languages to 16 written languages.\",\n    article = \"<p style='text-align: center'><a href='https:\/\/huggingface.co\/facebook\/wav2vec2-xls-r-2b-22-to-16' target='_blank'>Click to learn more about XLS-R-2B-22-16 <\/a> | <a href='https:\/\/arxiv.org\/abs\/2111.09296' target='_blank'> With \ud83c\udf99\ufe0f from Facebook XLS-R <\/a><\/p>\",\n    enable_queue=True,\n    allow_flagging=False,\n)\niface.launch()\n\n```"}
{"question":"\nI have an HTML file that contains the content and structure for a 404 \"page not found\" error page. The HTML includes the page layout, header, footer, and main content area showing a message that the requested page could not be found.\n\nI would like to create a Gatsby site with the following:\n- Use the existing HTML content for the 404 page \n- Break the HTML into React components for the header, footer, and main content\n- Use Gatsby's built-in 404 page functionality to show this custom 404 page\n- Ensure the page is responsive and looks good on mobile devices\n- Include the site's navigation header and footer on the 404 page\n","answer":"here is the gradio solution for your use case: ```python\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"\/><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"\/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\/><style data-href=\"\/styles.30e990850292b7da350c.css\" id=\"gatsby-global-css\">\/*!\n * Copyright Zendesk, Inc.\n *\n * Use of this source code is governed under the Apache License, Version 2.0\n * found at http:\/\/www.apache.org\/licenses\/LICENSE-2.0.\n *\/html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden],template{display:none}html{background-color:#fff;min-height:100%;box-sizing:border-box;overflow-y:scroll;line-height:20px;color:#2f3941;-webkit-font-feature-settings:\"kern\",\"kern\";font-feature-settings:\"kern\",\"kern\";font-kerning:normal;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,Arial,sans-serif;font-size:14px}*{font-weight:inherit}*,:after,:before{box-sizing:inherit}a{color:#1f73b7}a,ins,u{text-decoration:none}a:focus,a:hover{text-decoration:underline;color:#1f73b7}a:focus{outline:none}b{font-weight:600}button{cursor:pointer;padding:0}button,input,optgroup,select,textarea{line-height:inherit;font-family:inherit}code{font-size:.95em}code,kbd,pre,samp{font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace}em{font-style:inherit}fieldset,iframe{border:0}h1,h2,h3,h4,h5,h6{font-size:inherit}blockquote,dd,dl,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,ol,p,pre,ul{margin:0;padding:0}hr{border:none;border-top:1px solid}ol,ul{list-style:none}img{max-width:100%}strong{font-weight:inherit}svg{max-height:100%}[tabindex=\"-1\"]:focus{outline:none!important}<\/style><meta name=\"generator\" content=\"Gatsby 2.27.5\"\/><title data-react-helmet=\"true\">Not found \/ Coqui<\/title><link data-react-helmet=\"true\" rel=\"mask-icon\" href=\"\/mask-icon.svg\" color=\"#03363d\"\/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"\/apple-touch-icon.png\"\/><link data-react-helmet=\"true\" rel=\"shortcut icon\" href=\"\/favicon.ico\"\/><meta data-react-helmet=\"true\" name=\"application-name\" content=\"Coqui\"\/><meta data-react-helmet=\"true\" name=\"description\" content=\"Coqui, Freeing Speech.\"\/><meta data-react-helmet=\"true\" name=\"msapplication-config\" content=\"\/browserconfig.xml\"\/><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Coqui\"\/><meta data-react-helmet=\"true\" property=\"og:description\" content=\"Coqui, Freeing Speech.\"\/><meta data-react-helmet=\"true\" property=\"og:image\" content=\"https:\/\/coqui.ai\/og-image.png\"\/><meta data-react-helmet=\"true\" property=\"og:image:alt\" content=\"Coqui\"\/><meta data-react-helmet=\"true\" property=\"og:image:width\" content=\"1280\"\/><meta data-react-helmet=\"true\" property=\"og:image:height\" content=\"640\"\/><meta data-react-helmet=\"true\" property=\"twitter:card\" content=\"summary_large_image\"\/><link rel=\"stylesheet\" href=\"https:\/\/cdn.jsdelivr.net\/npm\/docsearch.js@2\/dist\/cdn\/docsearch.min.css\"\/><link rel=\"sitemap\" type=\"application\/xml\" href=\"\/sitemap.xml\"\/><style data-styled=\"boRYJl cSfwhy  hxorDo htdAJp ewKbRz enYstj ToqfT cRCrYO hBrSNz eXTIAY dwrhm cfRqVx djfJbM tqxgC gKXscS bsGQy bEFZhx dNvwjP gOJzTH iLBpii fgbdOu jLMfKX ktJjJv fsJlXk gNCkWG itUkto hQgeTU ePDUHH ceYsmn dTVxdM cRAtmP fMPyRj kPtfMH jdroyu gnprJW iFDdvB hqMsZl ldZTnf kHciob cpgqKd ftAzyZ fWbtTg krVJBD bzlDyR cWtRum dJVuoI gQJeLa\" data-styled-version=\"4.4.1\">\n\/* sc-component-id: gatsby-theme___StyledDiv-x2kfdx-0 *\/\n.boRYJl{width:100%;height:100%;}\n\/* sc-component-id: StyledSkipNav-sc-1tsro34-0 *\/\n.htdAJp{-webkit-transition:opacity 0.2s ease-out,clip 0s linear 0.2s;transition:opacity 0.2s ease-out,clip 0s linear 0.2s;opacity:0;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;position:absolute;left:50%;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);z-index:2;border:1px solid;border-radius:4px;-webkit-text-decoration:underline;text-decoration:underline;white-space:nowrap;top:26px;padding:20px;padding-left:16px;line-height:1.4285714285714286;font-size:14px;border-color:#d8dcde;box-shadow:0 20px 28px 0 rgba(23,73,77,0.15);background-color:#fff;color:#1f73b7;} .htdAJp:focus{-webkit-transition:opacity 0.2s ease-in-out;transition:opacity 0.2s ease-in-out;-webkit-animation:0.2s cubic-bezier(0.15,0.85,0.35,1.2) jtXLs;animation:0.2s cubic-bezier(0.15,0.85,0.35,1.2) jtXLs;opacity:1;-webkit-clip:rect(0,150vw,100vh,-50vw);clip:rect(0,150vw,100vh,-50vw);} .htdAJp:focus{outline:none;} .htdAJp:hover,.htdAJp:focus{color:#1f73b7;}\n\/* sc-component-id: StyledSkipNavIcon-sc-1ika5z4-0 *\/\n.ewKbRz{color:#68737d;margin-right:8px;width:16px;height:16px;}\n\/* sc-component-id: MaxWidth__MaxWidthLayout-sc-1rbvyso-0 *\/\n.tqxgC{margin-right:auto;margin-left:auto;max-width:2880px;}\n\/* sc-component-id: StyledAnchor-sc-xshgmo-0 *\/\n.kPtfMH{display:inline;-webkit-transition:border-color 0.25s ease-in-out,box-shadow 0.1s ease-in-out,background-color 0.25s ease-in-out,color 0.25s ease-in-out,z-index 0.25s ease-in-out;transition:border-color 0.25s ease-in-out,box-shadow 0.1s ease-in-out,background-color 0.25s ease-in-out,color 0.25s ease-in-out,z-index 0.25s ease-in-out;margin:0;border:none;border-radius:0;cursor:pointer;overflow:hidden;-webkit-text-decoration:none;text-decoration:none;text-overflow:ellipsis;font-family:inherit;font-weight:inherit;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;-webkit-touch-callout:none;padding:0;font-size:inherit;background-color:transparent;color:#1f73b7;} .kPtfMH::-moz-focus-inner{border:0;padding:0;} .kPtfMH:focus{outline:none;-webkit-text-decoration:none;text-decoration:none;} .kPtfMH:hover{-webkit-text-decoration:underline;text-decoration:underline;} .kPtfMH[data-garden-focus-visible]{-webkit-text-decoration:underline;text-decoration:underline;} .kPtfMH:active,.kPtfMH[aria-pressed='true'],.kPtfMH[aria-pressed='mixed']{-webkit-transition:border-color 0.1s ease-in-out,background-color 0.1s ease-in-out,color 0.1s ease-in-out,z-index 0.25s ease-in-out;transition:border-color 0.1s ease-in-out,background-color 0.1s ease-in-out,color 0.1s ease-in-out,z-index 0.25s ease-in-out;-webkit-text-decoration:underline;text-decoration:underline;} .kPtfMH:focus{color:#1f73b7;} .kPtfMH:hover,.kPtfMH[data-garden-focus-visible]{color:#144a75;} .kPtfMH:active,.kPtfMH[aria-pressed='true'],.kPtfMH[aria-pressed='mixed']{color:#0f3554;} .kPtfMH:disabled{color:#5293c7;} .kPtfMH:disabled{cursor:default;-webkit-text-decoration:none;text-decoration:none;} .kPtfMH .StyledIcon-sc-19meqgg-0{width:16px;min-width:16px;height:16px;vertical-align:middle;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH{position:relative;margin-left:-1px;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:hover,.StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH[data-garden-focus-visible],.StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:active{z-index:1;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:disabled{z-index:-1;border-top-width:0;border-bottom-width:0;border-right-color:#fff;border-left-color:#fff;background-color:#e9ebed;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:first-of-type:not(:last-of-type){margin-left:0;border-top-right-radius:0;border-bottom-right-radius:0;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:last-of-type:not(:first-of-type){border-top-left-radius:0;border-bottom-left-radius:0;} .StyledButtonGroup-sc-1fbpzef-0 .kPtfMH.kPtfMH:not(:first-of-type):not(:last-of-type){border-radius:0;}\n\/* sc-component-id: Footer__StyledFooterItem-owx86q-0 *\/\n.dJVuoI{margin-right:20px;color:#fff;} @media (max-width:767.98px){.dJVuoI{margin-left:20px;}} .dJVuoI:hover,.dJVuoI:focus{color:inherit;}\n\/* sc-component-id: Footer__StyledLinksItem-owx86q-1 *\/\n.cpgqKd{margin-top:12px;color:#fff;font-weight:400;} .cpgqKd:hover,.cpgqKd:focus{color:inherit;}\n\/* sc-component-id: Footer___StyledFooter-owx86q-2 *\/\n.hQgeTU{background-color:#03363d;padding:20px;line-height:20px;color:#fff;font-size:14px;}\n\/* sc-component-id: Footer___StyledDiv-owx86q-3 *\/\n.ePDUHH{margin:32px auto 32px auto;width:1080px;} @media (max-width:1199.98px){.ePDUHH{width:982px;}} @media (max-width:991.98px){.ePDUHH{width:732px;}} @media (max-width:767.98px){.ePDUHH{width:100%;}}\n\/* sc-component-id: Footer___StyledDiv2-owx86q-4 *\/\n.ceYsmn{display:grid;grid-template-rows:1fr 1fr;grid-template-columns:1fr 1fr;grid-template-areas:'col-1 col-2' 'col-3 col-2';row-gap:20px;} @media (max-width:767.98px){.ceYsmn{grid-template-columns:1fr;grid-template-rows:1fr 2fr 1fr;grid-template-areas:'col-1' 'col-2' 'col-3';row-gap:20px;}}\n\/* sc-component-id: Footer___StyledDiv3-owx86q-5 *\/\n.dTVxdM{grid-area:col-3;}\n\/* sc-component-id: Footer___StyledP-owx86q-6 *\/\n.cRAtmP{margin-bottom:16px;font-weight:700;}\n\/* sc-component-id: Footer___StyledDiv4-owx86q-7 *\/\n.fMPyRj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-bottom:20px;} @media (max-width:767.98px){.fMPyRj{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;}}\n\/* sc-component-id: Footer___StyledDiv5-owx86q-8 *\/\n.jdroyu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}\n\/* sc-component-id: Footer___StyledGitHubIcon-owx86q-9 *\/\n.gnprJW{margin-right:32px;width:26px;height:26px;color:#468D6A;}\n\/* sc-component-id: Footer___StyledDiv6-owx86q-10 *\/\n.iFDdvB{grid-area:col-2;display:grid;grid-template:100px 130px \/ 160px 160px;row-gap:20px;-webkit-column-gap:100px;column-gap:100px;-webkit-box-pack:end;-webkit-justify-content:end;-ms-flex-pack:end;justify-content:end;} @media (max-width:991.98px){.iFDdvB{grid-template-columns:80px 140px;}} @media (max-width:767.98px){.iFDdvB{-webkit-box-pack:start;-webkit-justify-content:start;-ms-flex-pack:start;justify-content:start;}}\n\/* sc-component-id: Footer___StyledDiv7-owx86q-11 *\/\n.hqMsZl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}\n\/* sc-component-id: Footer___StyledDiv8-owx86q-12 *\/\n.ldZTnf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}\n\/* sc-component-id: Footer___StyledP2-owx86q-13 *\/\n.kHciob{font-weight:700;}\n\/* sc-component-id: Footer___StyledDiv9-owx86q-14 *\/\n.ftAzyZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}\n\/* sc-component-id: Footer___StyledDiv10-owx86q-15 *\/\n.fWbtTg{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}\n\/* sc-component-id: Footer___StyledP3-owx86q-16 *\/\n.krVJBD{font-weight:700;}\n\/* sc-component-id: Footer___StyledDiv11-owx86q-17 *\/\n.bzlDyR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-top:20px;border-top:1px solid #467b7c;padding-top:12px;} @media (max-width:767.98px){.bzlDyR{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;}}\n\/* sc-component-id: Footer___StyledDiv12-owx86q-18 *\/\n.cWtRum{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-basis:36px;-ms-flex-preferred-size:36px;flex-basis:36px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}\n\/* sc-component-id: Footer___StyledDiv13-owx86q-19 *\/\n@media (max-width:767.98px){.gQJeLa{margin-top:12px;width:100%;text-align:center;}}\n\/* sc-component-id: StyledFont-sc-1iildbo-0 *\/\n.ktJjJv{line-height:24px;font-size:18px;font-weight:400;direction:ltr;}.fsJlXk{line-height:28px;font-size:22px;font-weight:400;direction:ltr;}\n\/* sc-component-id: Header__StyledHeader-sc-1bc1xqp-3 *\/\n.enYstj{z-index:301;box-shadow:0 16px 24px 0 rgba(47,57,65,0.05);padding:0 42px;height:77px;} .enYstj[data-show-navigation='true']{border-bottom-color:#fff;} @media (max-width:767.98px){.enYstj{padding:0;height:60px;}}\n\/* sc-component-id: Header___StyledDiv-sc-1bc1xqp-4 *\/\n.hBrSNz{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:left;-webkit-justify-content:left;-ms-flex-pack:left;justify-content:left;} @media (max-width:767.98px){.hBrSNz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-left:20px;}}\n\/* sc-component-id: Header___StyledDiv2-sc-1bc1xqp-5 *\/\n.eXTIAY{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}\n\/* sc-component-id: Header___StyledImg-sc-1bc1xqp-6 *\/\n@media (max-width:575.98px){.dwrhm{height:26pxpx;}}\n\/* sc-component-id: Header___StyledLogo-sc-1bc1xqp-7 *\/\n.cRCrYO{-webkit-box-pack:start;-webkit-justify-content:start;-ms-flex-pack:start;justify-content:start;}\n\/* sc-component-id: Header___StyledDiv3-sc-1bc1xqp-12 *\/\n.ToqfT{height:100%;display:grid;grid-template-columns:1fr 1fr 1fr;grid-template-areas:'a b c';-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}\n\/* sc-component-id: Header___StyledNav2-sc-1bc1xqp-13 *\/\n.cfRqVx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:40px;} @media (max-width:767.98px){.cfRqVx{display:none;}}\n\/* sc-component-id: sc-global-3576101690 *\/\n*{-ms-overflow-style:-ms-autohiding-scrollbar;} html{-webkit-scroll-behavior:smooth;-moz-scroll-behavior:smooth;-ms-scroll-behavior:smooth;scroll-behavior:smooth;} body a{color:#313940;} body a:focus,body a:hover{color:#313940;}\n\/* sc-component-id: Root__Main-k2xmz5-0 *\/\n.djfJbM{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;}\n\/* sc-component-id: Root___StyledDiv-k2xmz5-2 *\/\n.cSfwhy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;min-height:100vh;}\n\/* sc-component-id: Root___StyledSkipNav-k2xmz5-3 *\/\n.hxorDo{top:38.5px;box-shadow:0 16px 24px 0 rgba(47,57,65,0.05);}\n\/* sc-component-id: StyledCol-sc-inuw62-0 *\/\n.dNvwjP{box-sizing:border-box;width:100%;padding-right:10px;padding-left:10px;} @media (min-width:0px){} @media (min-width:576px){.dNvwjP{-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;width:auto;max-width:100%;}} @media (min-width:768px){} @media (min-width:992px){} @media (min-width:1200px){}.gOJzTH{box-sizing:border-box;width:100%;padding-right:10px;padding-left:10px;} @media (min-width:0px){} @media (min-width:576px){.gOJzTH{-webkit-flex-basis:41.66666666666667%;-ms-flex-preferred-size:41.66666666666667%;flex-basis:41.66666666666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;max-width:41.66666666666667%;}} @media (min-width:768px){} @media (min-width:992px){} @media (min-width:1200px){}\n\/* sc-component-id: StyledGrid-sc-oxgg5i-0 *\/\n.bsGQy{margin-right:auto;margin-left:auto;width:100%;box-sizing:border-box;padding-right:10px;padding-left:10px;}\n\/* sc-component-id: StyledRow-sc-xjsdg1-0 *\/\n.bEFZhx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;margin-right:-10px;margin-left:-10px;} @media (min-width:0px){} @media (min-width:576px){} @media (min-width:768px){} @media (min-width:992px){} @media (min-width:1200px){}\n\/* sc-component-id: sc-404___StyledGrid-fss6nr-0 *\/\n.gKXscS{margin-top:180px;margin-bottom:288px;} @media (max-width:575.98px){.gKXscS{margin-top:60px;margin-bottom:200px;}}\n\/* sc-component-id: sc-404___StyledImg-fss6nr-1 *\/\n@media (max-width:575.98px){.iLBpii{width:112px !important;height:112px !important;}}\n\/* sc-component-id: sc-404___StyledDiv-fss6nr-2 *\/\n.fgbdOu{margin-left:48px;} @media (max-width:575.98px){.fgbdOu{margin-left:0;}}\n\/* sc-component-id: sc-404___StyledLG-fss6nr-3 *\/\n.jLMfKX{margin-top:20px;margin-bottom:8px;text-transform:uppercase;color:#68737d;font-size:16px;} @media (max-width:575.98px){.jLMfKX{font-size:12px;}}\n\/* sc-component-id: sc-404___StyledStyledH-fss6nr-4 *\/\n.gNCkWG{margin-bottom:12px;color:#03363d;font-weight:600;line-height:1.0833333333333333;font-size:48px;font-weight:700;} .gNCkWG:hover .Anchor__StyledAnchor-sc-1q3ov98-0{color:inherit;} @media (max-width:575.98px){.gNCkWG{margin-bottom:8px;line-height:32px;font-size:26px;font-weight:600;}}\n\/* sc-component-id: sc-404___StyledXL-fss6nr-5 *\/\n.itUkto{color:#68737d;} @media (max-width:575.98px){.itUkto{line-height:20px;font-size:14px;}}<\/style><style data-styled=\"jtXLs\" data-styled-version=\"4.4.1\">\n\/* sc-component-id: sc-keyframes-jtXLs *\/\n@-webkit-keyframes jtXLs{0%{-webkit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);}} @keyframes jtXLs{0%{-webkit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);}}<\/style><link as=\"script\" rel=\"preload\" href=\"\/webpack-runtime-44e5ead90382d20e2583.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/framework-4aeac2b3fca5c098d555.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/app-f2d9ef5b12458aa6692a.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/styles-e9d24b1846c7d6eb9685.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/34b0555e59de69ff419a17e27e803831174e2195-d8768e8481160bb1ccbb.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/8a21a1251dc0d0d2325d5f1285ab0afc14869d23-6723a6429fa98359a82a.js\"\/><link as=\"script\" rel=\"preload\" href=\"\/component---src-pages-404-tsx-e58a34eb0bf91e8e88ce.js\"\/><link as=\"fetch\" rel=\"preload\" href=\"\/page-data\/404.html\/page-data.json\" crossorigin=\"anonymous\"\/><link as=\"fetch\" rel=\"preload\" href=\"\/page-data\/sq\/d\/1942088059.json\" crossorigin=\"anonymous\"\/><link as=\"fetch\" rel=\"preload\" href=\"\/page-data\/sq\/d\/3709355695.json\" crossorigin=\"anonymous\"\/><link as=\"fetch\" rel=\"preload\" href=\"\/page-data\/sq\/d\/767178358.json\" crossorigin=\"anonymous\"\/><link as=\"fetch\" rel=\"preload\" href=\"\/page-data\/app-data.json\" crossorigin=\"anonymous\"\/><\/head><body><div id=\"___gatsby\"><div><div class=\"gatsby-theme___StyledDiv-x2kfdx-0 boRYJl\"><div style=\"outline:none\" tabindex=\"-1\" id=\"gatsby-focus-wrapper\"><div class=\"Root___StyledDiv-k2xmz5-2 cSfwhy\"><a href=\"#main-content\" class=\"StyledSkipNav-sc-1tsro34-0 htdAJp Root___StyledSkipNav-k2xmz5-3 hxorDo\" data-garden-id=\"chrome.skipnav\" data-garden-version=\"8.51.0\"><svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"16\" height=\"16\" focusable=\"false\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" theme=\"[object Object]\" data-garden-id=\"chrome.skipnav_icon\" data-garden-version=\"8.51.0\" class=\"StyledSkipNavIcon-sc-1ika5z4-0 ewKbRz\"><path fill=\"currentColor\" d=\"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"><\/path><\/svg>Skip to main content<\/a><header role=\"banner\" class=\"Header__StyledHeader-sc-1bc1xqp-3 enYstj\"><div class=\"Header___StyledDiv3-sc-1bc1xqp-12 ToqfT\"><div class=\"Header___StyledDiv-sc-1bc1xqp-4 hBrSNz\"><a aria-label=\"Coqui\" href=\"\/\"><div class=\"Header___StyledDiv2-sc-1bc1xqp-5 eXTIAY\"><div class=\"Header___StyledImg-sc-1bc1xqp-6 dwrhm gatsby-image-wrapper\" style=\"position:relative;overflow:hidden;display:inline-block;width:95px;height:26px\"><noscript><picture><source srcset=\"\/static\/38a06ec53309f617be3eb3b8b9367abf\/598c3\/logo-wordmark.png 1x,\n\/static\/38a06ec53309f617be3eb3b8b9367abf\/e3b64\/logo-wordmark.png 1.5x,\n\/static\/38a06ec53309f617be3eb3b8b9367abf\/5aa2f\/logo-wordmark.png 2x\" \/><img loading=\"lazy\" width=\"95\" height=\"26\" srcset=\"\/static\/38a06ec53309f617be3eb3b8b9367abf\/598c3\/logo-wordmark.png 1x,\n\/static\/38a06ec53309f617be3eb3b8b9367abf\/e3b64\/logo-wordmark.png 1.5x,\n\/static\/38a06ec53309f617be3eb3b8b9367abf\/5aa2f\/logo-wordmark.png 2x\" src=\"\/static\/38a06ec53309f617be3eb3b8b9367abf\/598c3\/logo-wordmark.png\" alt=\"\" style=\"position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center\"\/><\/picture><\/noscript><\/div><\/div><\/a><\/div><nav aria-label=\"Global\" class=\"Header___StyledNav2-sc-1bc1xqp-13 cfRqVx\"><\/nav><\/div><\/header><main class=\"Root__Main-k2xmz5-0 djfJbM\"><div class=\"MaxWidth__MaxWidthLayout-sc-1rbvyso-0 tqxgC\"><div class=\"StyledGrid-sc-oxgg5i-0 bsGQy sc-404___StyledGrid-fss6nr-0 gKXscS\" data-garden-id=\"grid.grid\" data-garden-version=\"8.51.0\"><div data-garden-id=\"grid.row\" data-garden-version=\"8.51.0\" class=\"StyledRow-sc-xjsdg1-0 bEFZhx\"><div data-garden-id=\"grid.col\" data-garden-version=\"8.51.0\" class=\"StyledCol-sc-inuw62-0 dNvwjP\"><div class=\"sc-404___StyledImg-fss6nr-1 iLBpii gatsby-image-wrapper\" style=\"position:relative;overflow:hidden;display:inline-block;width:350px;height:350px\"><noscript><picture><source srcset=\"\/static\/8b770b400c92a3ffa53824538ce62c74\/b5a48\/general-error-404.png 1x,\n\/static\/8b770b400c92a3ffa53824538ce62c74\/ce31e\/general-error-404.png 1.5x\" \/><img loading=\"lazy\" width=\"350\" height=\"350\" srcset=\"\/static\/8b770b400c92a3ffa53824538ce62c74\/b5a48\/general-error-404.png 1x,\n\/static\/8b770b400c92a3ffa53824538ce62c74\/ce31e\/general-error-404.png 1.5x\" src=\"\/static\/8b770b400c92a3ffa53824538ce62c74\/b5a48\/general-error-404.png\" alt=\"\" style=\"position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center\"\/><\/picture><\/noscript><\/div><\/div><div data-garden-id=\"grid.col\" data-garden-version=\"8.51.0\" class=\"StyledCol-sc-inuw62-0 gOJzTH\"><div class=\"sc-404___StyledDiv-fss6nr-2 fgbdOu\"><div class=\"StyledFont-sc-1iildbo-0 ktJjJv sc-404___StyledLG-fss6nr-3 jLMfKX\" data-garden-id=\"typography.font\" data-garden-version=\"8.51.0\">404 page not found<\/div><h1 class=\"Typography__StyledH1-xo896r-0 sc-404___StyledStyledH-fss6nr-4 gNCkWG\">Move along<\/h1><div class=\"StyledFont-sc-1iildbo-0 fsJlXk sc-404___StyledXL-fss6nr-5 itUkto\" data-garden-id=\"typography.font\" data-garden-version=\"8.51.0\">This is no place for a frog. Head back to the homepage for more Coqui.<\/div><\/div><\/div><\/div><\/div><\/div><\/main><footer class=\"Footer___StyledFooter-owx86q-2 hQgeTU\"><div class=\"MaxWidth__MaxWidthLayout-sc-1rbvyso-0 tqxgC\"><div class=\"Footer___StyledDiv-owx86q-3 ePDUHH\"><div class=\"Footer___StyledDiv2-owx86q-4 ceYsmn\"><div class=\"Footer___StyledDiv3-owx86q-5 dTVxdM\"><p class=\"Footer___StyledP-owx86q-6 cRAtmP\">Follow us<\/p><div class=\"Footer___StyledDiv4-owx86q-7 fMPyRj\"><a href=\"https:\/\/github.com\/coqui-ai\" aria-label=\"Coqui on GitHub\" data-garden-id=\"buttons.anchor\" data-garden-version=\"8.51.0\" class=\"StyledButton-sc-qe3ace-0 StyledAnchor-sc-xshgmo-0 kPtfMH\"><div class=\"Footer___StyledDiv5-owx86q-8 jdroyu\"><svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"12\" height=\"12\" viewBox=\"0 0 12 12\" focusable=\"false\" role=\"presentation\" class=\"Footer___StyledGitHubIcon-owx86q-9 gnprJW\"><path fill=\"currentColor\" d=\"M6 0C2.687 0 0 2.754 0 6.152c0 2.718 1.719 5.024 4.103 5.837.3.057.41-.133.41-.296 0-.146-.005-.533-.008-1.046-1.669.371-2.021-.825-2.021-.825-.273-.711-.666-.9-.666-.9-.545-.382.04-.374.04-.374.603.044.92.634.92.634.535.94 1.404.668 1.746.511.055-.397.21-.669.381-.822-1.332-.155-2.733-.683-2.733-3.04 0-.672.234-1.221.618-1.651-.062-.156-.268-.781.058-1.629 0 0 .504-.165 1.65.631A5.614 5.614 0 016 2.975a5.58 5.58 0 011.502.207c1.146-.796 1.649-.63 1.649-.63.327.847.121 1.472.06 1.628.384.43.616.979.616 1.65 0 2.364-1.403 2.884-2.74 3.036.216.19.408.565.408 1.14 0 .821-.007 1.485-.007 1.687 0 .164.108.356.412.296 2.382-.816 4.1-3.12 4.1-5.837C12 2.754 9.313 0 6 0\"><\/path><\/svg><\/div><\/a><\/div><\/div><div class=\"Footer___StyledDiv6-owx86q-10 iFDdvB\"><div class=\"Footer___StyledDiv7-owx86q-11 hqMsZl\"><\/div><div class=\"Footer___StyledDiv8-owx86q-12 ldZTnf\"><p class=\"Footer___StyledP2-owx86q-13 kHciob\">Resources<\/p><a class=\"Footer__StyledLinksItem-owx86q-1 cpgqKd\" href=\"\/tos\">Terms &amp; Conditions<\/a><a class=\"Footer__StyledLinksItem-owx86q-1 cpgqKd\" href=\"\/privacy\">Privacy Policy<\/a><\/div><div class=\"Footer___StyledDiv9-owx86q-14 ftAzyZ\"><\/div><div class=\"Footer___StyledDiv10-owx86q-15 fWbtTg\"><p class=\"Footer___StyledP3-owx86q-16 krVJBD\">Support<\/p><a href=\"https:\/\/discord.gg\/CzxHHp8mtZ\" class=\"StyledButton-sc-qe3ace-0 StyledAnchor-sc-xshgmo-0 kPtfMH Footer__StyledLinksItem-owx86q-1 cpgqKd\" data-garden-id=\"buttons.anchor\" data-garden-version=\"8.51.0\">Community<\/a><a href=\"\/cdn-cgi\/l\/email-protection#0e676068614e6d617f7b67206f67\" class=\"StyledButton-sc-qe3ace-0 StyledAnchor-sc-xshgmo-0 kPtfMH Footer__StyledLinksItem-owx86q-1 cpgqKd\" data-garden-id=\"buttons.anchor\" data-garden-version=\"8.51.0\">Contact us<\/a><\/div><\/div><\/div><div class=\"Footer___StyledDiv11-owx86q-17 bzlDyR\"><div class=\"Footer___StyledDiv12-owx86q-18 cWtRum\"><a href=\"https:\/\/berlinlovesyou.com\/\" class=\"StyledButton-sc-qe3ace-0 StyledAnchor-sc-xshgmo-0 kPtfMH Footer__StyledFooterItem-owx86q-0 dJVuoI\" data-garden-id=\"buttons.anchor\" data-garden-version=\"8.51.0\">Made with<!-- --> <span role=\"img\" aria-label=\"heart\">\u2764\ufe0f<\/span> <!-- -->in Berlin!<\/a><\/div><div class=\"Footer___StyledDiv13-owx86q-19 gQJeLa\">\u00a9 Coqui <!-- -->2023<\/div><\/div><\/div><\/div><\/footer><\/div><\/div><div id=\"gatsby-announcer\" style=\"position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0\" aria-live=\"assertive\" aria-atomic=\"true\"><\/div><\/div><\/div><\/div><script data-cfasync=\"false\" src=\"\/cdn-cgi\/scripts\/5c5dd728\/cloudflare-static\/email-decode.min.js\"><\/script><script type=\"text\/javascript\" src=\"https:\/\/cdn.jsdelivr.net\/npm\/docsearch.js@2\/dist\/cdn\/docsearch.min.js\"><\/script><script id=\"gatsby-script-loader\">\/*<![CDATA[*\/window.pagePath=\"\/404.html\";\/*]]>*\/<\/script><script id=\"gatsby-chunk-mapping\">\/*<![CDATA[*\/window.___chunkMapping={\"polyfill\":[\"\/polyfill-ccdf4b824d3f769bf7b7.js\"],\"app\":[\"\/app-f2d9ef5b12458aa6692a.js\"],\"component---src-pages-404-tsx\":[\"\/component---src-pages-404-tsx-e58a34eb0bf91e8e88ce.js\"],\"component---src-pages-about-tsx\":[\"\/component---src-pages-about-tsx-33e90646fcb5d452b984.js\"],\"component---src-pages-account-tsx\":[\"\/component---src-pages-account-tsx-0b1661fb8065b5ccb649.js\"],\"component---src-pages-auth-signin-tsx\":[\"\/component---src-pages-auth-signin-tsx-956d3f82792728cac023.js\"],\"component---src-pages-auth-signout-tsx\":[\"\/component---src-pages-auth-signout-tsx-5685b8c6212c493da7b5.js\"],\"component---src-pages-auth-signup-tsx\":[\"\/component---src-pages-auth-signup-tsx-c026329f07435c19d85c.js\"],\"component---src-pages-blog-general-investment-march-mdx\":[\"\/component---src-pages-blog-general-investment-march-mdx-3fe393a172e0a0ff78a2.js\"],\"component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx\":[\"\/component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx-e6805ef047be1a0a4ad2.js\"],\"component---src-pages-blog-stt-deepspeech-0-6-speech-to-text-engine-mdx\":[\"\/component---src-pages-blog-stt-deepspeech-0-6-speech-to-text-engine-mdx-d1397a9978ad9ebf2bae.js\"],\"component---src-pages-blog-stt-speech-recognition-deepspeech-mdx\":[\"\/component---src-pages-blog-stt-speech-recognition-deepspeech-mdx-b8b2be30a899b30fbcbc.js\"],\"component---src-pages-blog-tsx\":[\"\/component---src-pages-blog-tsx-f6941fb4e05587eaedb9.js\"],\"component---src-pages-blog-tts-african-languages-dataset-collaboration-mdx\":[\"\/component---src-pages-blog-tts-african-languages-dataset-collaboration-mdx-a0a685f766ba1942361a.js\"],\"component---src-pages-blog-tts-coqui-studio-feb-2023-release-mdx\":[\"\/component---src-pages-blog-tts-coqui-studio-feb-2023-release-mdx-b8994175402731b832c7.js\"],\"component---src-pages-blog-tts-coqui-the-polyglot-mdx\":[\"\/component---src-pages-blog-tts-coqui-the-polyglot-mdx-dd0c66661471847803d1.js\"],\"component---src-pages-blog-tts-cpml-mdx\":[\"\/component---src-pages-blog-tts-cpml-mdx-c63f8164e1f09144bc18.js\"],\"component---src-pages-blog-tts-dive-deeper-into-the-world-of-xtts-mdx\":[\"\/component---src-pages-blog-tts-dive-deeper-into-the-world-of-xtts-mdx-2fcec53f41eb898f6184.js\"],\"component---src-pages-blog-tts-gradual-training-with-tacotron-for-faster-convergence-mdx\":[\"\/component---src-pages-blog-tts-gradual-training-with-tacotron-for-faster-convergence-mdx-f86131794b3d2e2cb831.js\"],\"component---src-pages-blog-tts-introducing-xtts-for-game-studios-mdx\":[\"\/component---src-pages-blog-tts-introducing-xtts-for-game-studios-mdx-87ad04337a11be88579d.js\"],\"component---src-pages-blog-tts-open-xtts-mdx\":[\"\/component---src-pages-blog-tts-open-xtts-mdx-d7e9b3f29c00cf0bde28.js\"],\"component---src-pages-blog-tts-prompt-to-voice-mdx\":[\"\/component---src-pages-blog-tts-prompt-to-voice-mdx-1c8ed243349232028312.js\"],\"component---src-pages-blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-mdx\":[\"\/component---src-pages-blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-mdx-3b82d8a672f8f5dd5655.js\"],\"component---src-pages-blog-tts-two-methods-for-better-attention-in-tacotron-mdx\":[\"\/component---src-pages-blog-tts-two-methods-for-better-attention-in-tacotron-mdx-6ca36d65aba009c44cc3.js\"],\"component---src-pages-blog-tts-unleashing-creativity-mdx\":[\"\/component---src-pages-blog-tts-unleashing-creativity-mdx-661ae326667662ab7b56.js\"],\"component---src-pages-blog-tts-unleashing-emotion-with-xtts-mdx\":[\"\/component---src-pages-blog-tts-unleashing-emotion-with-xtts-mdx-26fd3427f2c0c4be51a1.js\"],\"component---src-pages-blog-tts-unleashing-soundscapes-mdx\":[\"\/component---src-pages-blog-tts-unleashing-soundscapes-mdx-06044597aad0b500e4bc.js\"],\"component---src-pages-blog-tts-unleashing-the-power-of-xtts-mdx\":[\"\/component---src-pages-blog-tts-unleashing-the-power-of-xtts-mdx-30e80bb639533a739c1c.js\"],\"component---src-pages-blog-tts-unleashing-your-vision-mdx\":[\"\/component---src-pages-blog-tts-unleashing-your-vision-mdx-000d042acfb15a104c8e.js\"],\"component---src-pages-blog-tts-voice-fusion-mdx\":[\"\/component---src-pages-blog-tts-voice-fusion-mdx-5c78d2a5dbcc136a85ec.js\"],\"component---src-pages-blog-tts-xtts-taking-tts-to-the-next-level-mdx\":[\"\/component---src-pages-blog-tts-xtts-taking-tts-to-the-next-level-mdx-e9e5f011f3f4bcf5a154.js\"],\"component---src-pages-blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-mdx\":[\"\/component---src-pages-blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-mdx-2db84efbce5dec3b3525.js\"],\"component---src-pages-brand-voices-tsx\":[\"\/component---src-pages-brand-voices-tsx-b0c75e535378655a7ab8.js\"],\"component---src-pages-cpml-tsx\":[\"\/component---src-pages-cpml-tsx-9873a62428dc2e3101b0.js\"],\"component---src-pages-dubbing-tsx\":[\"\/component---src-pages-dubbing-tsx-5bf70691f04a62cd4598.js\"],\"component---src-pages-editor-tsx\":[\"\/component---src-pages-editor-tsx-338169f0dbe4b0be4dfb.js\"],\"component---src-pages-faq-tsx\":[\"\/component---src-pages-faq-tsx-b0d388d4a62b64531bdc.js\"],\"component---src-pages-how-it-works-tsx\":[\"\/component---src-pages-how-it-works-tsx-ff2a439e7a260253d44b.js\"],\"component---src-pages-index-tsx\":[\"\/component---src-pages-index-tsx-1bf96d12356607387863.js\"],\"component---src-pages-job-principal-full-stack-engineer-mdx\":[\"\/component---src-pages-job-principal-full-stack-engineer-mdx-e3ac281ec04a0922d666.js\"],\"component---src-pages-job-renaissance-sales-representative-mdx\":[\"\/component---src-pages-job-renaissance-sales-representative-mdx-8f3511b91d0e04cd0b6d.js\"],\"component---src-pages-job-senior-full-stack-engineer-mdx\":[\"\/component---src-pages-job-senior-full-stack-engineer-mdx-90fa29be7bb6d6e08d93.js\"],\"component---src-pages-job-senior-ui-ux-engineer-mdx\":[\"\/component---src-pages-job-senior-ui-ux-engineer-mdx-38ec233cf8d751ba5e7e.js\"],\"component---src-pages-jobs-tsx\":[\"\/component---src-pages-jobs-tsx-00b4775f15da482852d6.js\"],\"component---src-pages-newsletter-01-11-2021-mdx\":[\"\/component---src-pages-newsletter-01-11-2021-mdx-e1d55849ba0e2d3cabe4.js\"],\"component---src-pages-newsletter-02-05-2022-mdx\":[\"\/component---src-pages-newsletter-02-05-2022-mdx-864f82167b608c61bb7f.js\"],\"component---src-pages-newsletter-02-08-2021-mdx\":[\"\/component---src-pages-newsletter-02-08-2021-mdx-e21fddd6331d0c354e9d.js\"],\"component---src-pages-newsletter-03-01-2022-mdx\":[\"\/component---src-pages-newsletter-03-01-2022-mdx-127335b0f5be56dd28d7.js\"],\"component---src-pages-newsletter-03-05-2021-mdx\":[\"\/component---src-pages-newsletter-03-05-2021-mdx-a961cdc7dfc909698358.js\"],\"component---src-pages-newsletter-04-04-2022-mdx\":[\"\/component---src-pages-newsletter-04-04-2022-mdx-6c4b5ce28f0d8a8816b8.js\"],\"component---src-pages-newsletter-04-10-2021-mdx\":[\"\/component---src-pages-newsletter-04-10-2021-mdx-30e321de29f67cf3f0a7.js\"],\"component---src-pages-newsletter-05-04-2021-mdx\":[\"\/component---src-pages-newsletter-05-04-2021-mdx-18da6f0d177958869418.js\"],\"component---src-pages-newsletter-05-07-2021-mdx\":[\"\/component---src-pages-newsletter-05-07-2021-mdx-9bc9708fc0c2f42a447a.js\"],\"component---src-pages-newsletter-06-06-2021-mdx\":[\"\/component---src-pages-newsletter-06-06-2021-mdx-99c79c6baf0294c32ed5.js\"],\"component---src-pages-newsletter-06-09-2021-mdx\":[\"\/component---src-pages-newsletter-06-09-2021-mdx-544f2aedb9c9a84b295c.js\"],\"component---src-pages-newsletter-06-12-2021-mdx\":[\"\/component---src-pages-newsletter-06-12-2021-mdx-1a92e44f0a3459eb1677.js\"],\"component---src-pages-newsletter-07-02-2022-mdx\":[\"\/component---src-pages-newsletter-07-02-2022-mdx-21aa0deb24599e10d632.js\"],\"component---src-pages-newsletter-07-03-2022-mdx\":[\"\/component---src-pages-newsletter-07-03-2022-mdx-d3cbf3210d45d83b06f3.js\"],\"component---src-pages-post-production-tsx\":[\"\/component---src-pages-post-production-tsx-69cd784dd1c05ae76825.js\"],\"component---src-pages-privacy-tsx\":[\"\/component---src-pages-privacy-tsx-392694387ae0cd1779cb.js\"],\"component---src-pages-signup-tsx\":[\"\/component---src-pages-signup-tsx-a8e1217f0de055b563f2.js\"],\"component---src-pages-tos-tsx\":[\"\/component---src-pages-tos-tsx-da10f9535ff929c1ea86.js\"],\"component---src-pages-video-games-tsx\":[\"\/component---src-pages-video-games-tsx-841d463ce5b14bb81104.js\"],\"component---src-pages-voices-[id]-samples-tsx\":[\"\/component---src-pages-voices-[id]-samples-tsx-9cf3fe7b57aa095ce793.js\"],\"component---src-pages-voices-[id]-synthesize-tsx\":[\"\/component---src-pages-voices-[id]-synthesize-tsx-db7660c74a466373b270.js\"],\"component---src-pages-voices-create-tsx\":[\"\/component---src-pages-voices-create-tsx-ceb392dc1b57bc133ab7.js\"],\"component---src-pages-voices-tsx\":[\"\/component---src-pages-voices-tsx-8e9d660bc8d539349429.js\"],\"component---src-pages-xtts-tos-tsx\":[\"\/component---src-pages-xtts-tos-tsx-8af05e9dbf2e535b4381.js\"]};\/*]]>*\/<\/script><script src=\"\/polyfill-ccdf4b824d3f769bf7b7.js\" nomodule=\"\"><\/script><script src=\"\/component---src-pages-404-tsx-e58a34eb0bf91e8e88ce.js\" async=\"\"><\/script><script src=\"\/8a21a1251dc0d0d2325d5f1285ab0afc14869d23-6723a6429fa98359a82a.js\" async=\"\"><\/script><script src=\"\/34b0555e59de69ff419a17e27e803831174e2195-d8768e8481160bb1ccbb.js\" async=\"\"><\/script><script src=\"\/styles-e9d24b1846c7d6eb9685.js\" async=\"\"><\/script><script src=\"\/app-f2d9ef5b12458aa6692a.js\" async=\"\"><\/script><script src=\"\/framework-4aeac2b3fca5c098d555.js\" async=\"\"><\/script><script src=\"\/webpack-runtime-44e5ead90382d20e2583.js\" async=\"\"><\/script><\/body><\/html>\n```"}
{"question":"\nI have an audio processing library called AudioFusion that can apply various effects to audio files, such as:\n- 8D audio effect with parameters to control the pan boundary, jump percentage, time from left to right, and volume multiplier \n- Slowed effect with a speed multiplier parameter\n- Reverb effect with parameters for room size, damping, width, wet level, and dry level\nWith my current code, I can load an audio file, apply selected effects based on passed parameters, and save the processed audio to an output file.\n\nI would like to create an interactive Gradio web interface for AudioFusion with the following features:\n- An upload component to allow the user to provide an input audio file\n- Separate tabs for each effect (8D, Slowed, Reverb) \n- Within each effect tab:\n    - A checkbox to select\/deselect applying that effect\n    - Sliders to control the parameters for that effect\n    - Show\/hide the parameter sliders based on the checkbox state\n- A \"Run\" button to process the audio with the selected effects and parameters\n- Display the processed audio file for the user to play and download \n- A \"Clear\" button to reset the checkboxes and audio components\n","answer":"here is the gradio solution for your use case: ```python\n\nimport os\nimport bot\nimport secrets\nimport gradio as gr\nfrom AudioFusion import Fusion\n\ndef process_audio(input_file,\n    effect_8d, pan_boundary, jump_percentage, time_l_to_r, volume_multiplier,\n    effect_slowed, speed_multiplier,\n    effect_reverb, room_size, damping, width, wet_level, dry_level\n):\n    # Load the sound file\n    sound = Fusion.loadSound(input_file)\n    os.remove(os.path.abspath(input_file))\n    effects_str = []\n    \n    # Apply effects based on user choices\n    if effect_8d:\n        sound = Fusion.effect8D(sound, pan_boundary, jump_percentage, time_l_to_r*1000, volume_multiplier)\n        effects_str.append(\"8d\")\n    if effect_slowed:\n        sound = Fusion.effectSlowed(sound, speed_multiplier)\n        effects_str.append(\"Slowed\")\n    if effect_reverb:\n        sound = Fusion.effectReverb(sound, room_size, damping, width, wet_level, dry_level, str(secrets.token_hex(5)))\n        effects_str.append(\"Reverb\")\n\n    output_file = f\"{input_file} {' + '.join(effects_str)} - {'By AudioFusion'}\"\n    \n    # Save the processed sound and return the output file\n    output = Fusion.saveSound(sound, output_file)\n    return output\n\n\nwith gr.Blocks(title=\"Audio Fusion\") as iface:\n    gr.Markdown(\"<p align='center'><h1>Audio Fusion<\/h1><\/p>\")\n    input_audio = gr.Audio(label=\"Upload your music file\", type=\"filepath\")\n    \n    # SLowed Effect and its arguments\n    with gr.Tab(\"Slowed Effect\"):\n        speed_check = gr.Checkbox(label=\"Apply slowed effect\")\n        with gr.Column(visible=False) as se_col:\n            speed = gr.Slider(label=\"Speed Multiplier\", minimum=0.1, maximum=4, step=0.05, value=0.90)\n    \n    # Reverb Effect and its argument\n    with gr.Tab(\"Reverb Effect\"):\n        reverb_check = gr.Checkbox(label=\"Apply reverb effect\")\n        with gr.Column(visible=False) as re_col:\n            with gr.Row():\n                room = gr.Slider(label=\"Room Size\", minimum=0, maximum=1, step=0.01, value=0.8)\n                damp = gr.Slider(label=\"Damping\", minimum=0, maximum=1, step=0.05, value=1)\n            width = gr.Slider(label=\"Width\", minimum=0, maximum=1, step=0.05, value=0.5)\n            with gr.Row():\n                wet = gr.Slider(label=\"Wet Level\", minimum=0, maximum=1, step=0.05, value=0.3)\n                dry = gr.Slider(label=\"Dry Level\", minimum=0, maximum=1, step=0.05, value=0.8)\n    \n    # 8d Effect and its arguments\n    with gr.Tab(\"8d Effect\"):\n        dimension_check = gr.Checkbox(label=\"Apply 8D effect\")\n        with gr.Column(visible=False) as di_col:\n            with gr.Row():\n                pan = gr.Slider(label=\"Pan Boundary\", minimum=0, maximum=100, value=90)\n                jump = gr.Slider(label=\"Jump Percentage\", minimum=1, maximum=100, value=5)\n            with gr.Row():\n                time = gr.Slider(label=\"Time L to R (s)\", minimum=1, maximum=30, value=10)\n                volx = gr.Slider(label=\"Volume Multiplier\", minimum=1, maximum=20, value=6)\n\n    # =====================================================\n    def di_v(check):\n        if check:\n            return {di_col: gr.Column(visible=True)}\n        else:\n            return {di_col: gr.Column(visible=False)}\n            \n    def se_v(check):\n        if check:\n            return {se_col: gr.Column(visible=True)}\n        else:\n            return {se_col: gr.Column(visible=False)}\n            \n    def re_v(check):\n        if check:\n            return {re_col: gr.Column(visible=True)}\n        else:\n            return {re_col: gr.Column(visible=False)}\n        \n\n    dimension_check.change(di_v, inputs=[dimension_check], outputs=[di_col])\n    speed_check.change(se_v, inputs=[speed_check], outputs=[se_col])\n    reverb_check.change(re_v, inputs=[reverb_check], outputs=[re_col])\n    # =====================================================\n\n    with gr.Row():\n        btnClear = gr.ClearButton(components=[dimension_check, speed_check, reverb_check])\n        btnRun = gr.Button(\"Run\", size=\"sm\", variant=\"primary\")\n    \n    inputs = [input_audio, dimension_check, pan, jump, time, volx, speed_check, speed, reverb_check, room, damp, width, wet, dry]\n    output = [gr.Audio(label=\"Download processed music\", type=\"filepath\")]\n\n    btnClear.add(components=output)\n    btnRun.click(fn=process_audio, inputs=inputs, outputs=output, api_name=\"AudioFusion\")\n\niface.launch(share=False)\n\n```"}
{"question":"\nI have code that implements Gaussian Process (GP) regression and approximates the GP posterior using a transformer model called a Prior Network Fitted (PNF). Given a small set of (x,y) data points, my code can plot the posterior mean and confidence intervals of both the exact GP and the PNF approximation.\n\nI would like to create an interactive Gradio demo to showcase this capability. The demo should allow the user to:\n\n- Input a small set of (x,y) data points via an editable dataframe \n- Select between PNF models trained with different amounts of data (to show how approximation improves with more training)\n- Display the resulting plot with the data points, GP posterior, and PNF posterior\n- Show an error message if the user enters invalid data (not numbers, outside allowed x and y ranges, or too many points)\n\nI'd also like the demo to have:\n- A title and description explaining what it is demonstrating\n- A link to the research paper with more details on PNFs\n- Nice formatting of the plot and interface\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gpytorch\nimport torch\nimport sys\n\nimport gpytorch\n\n# We will use the simplest form of GP model, exact inference\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef get_model(x, y, hyperparameters):\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1.e-9))\n    model = ExactGPModel(x, y, likelihood)\n    model.likelihood.noise = torch.ones_like(model.likelihood.noise) * hyperparameters[\"noise\"]\n    model.covar_module.outputscale = torch.ones_like(model.covar_module.outputscale) * hyperparameters[\"outputscale\"]\n    model.covar_module.base_kernel.lengthscale = torch.ones_like(model.covar_module.base_kernel.lengthscale) * \\\n                                                 hyperparameters[\"lengthscale\"]\n    return model, likelihood\n\n\n\nexcuse = \"Please only specify numbers, x values should be in [0,1] and y values in [-1,1].\"\nexcuse_max_examples = \"This model is trained to work with up to 4 input points.\"\nhyperparameters = {'noise': 1e-4, 'outputscale': 1., 'lengthscale': .1, 'fast_computations': (False,False,False)}\n\n\nconf = .5\n\ndef mean_and_bounds_for_gp(x,y,test_xs):\n    gp_model, likelihood = get_model(x,y,hyperparameters)\n    gp_model.eval()\n    l = likelihood(gp_model(test_xs))\n    means = l.mean.squeeze()\n    varis = torch.diagonal(l.covariance_matrix.squeeze())\n    stds = varis.sqrt()\n    return means, means-stds, means+stds\n\n\ndef mean_and_bounds_for_pnf(x,y,test_xs, choice):\n    sys.path.append('prior-fitting\/')\n    model = torch.load(f'onefeature_gp_ls.1_pnf_{choice}.pt')\n\n    logits = model((torch.cat([x,test_xs],0).unsqueeze(1),y.unsqueeze(1)),single_eval_pos=len(x))\n    bounds = model.criterion.quantile(logits,center_prob=.682).squeeze(1)\n    return model.criterion.mean(logits).squeeze(1), bounds[:,0], bounds[:,1]\n\ndef plot_w_conf_interval(ax_or_plt, x, m, lb, ub, color, label_prefix):\n    ax_or_plt.plot(x.squeeze(-1),m, color=color, label=label_prefix+' mean')\n    ax_or_plt.fill_between(x.squeeze(-1), lb, ub, alpha=.1, color=color, label=label_prefix+' conf. interval')\n\n\n\n\n@torch.no_grad()\ndef infer(table, choice):\n    vfunc = np.vectorize(lambda s: len(s))\n    non_empty_row_mask = (vfunc(table).sum(1) != 0)\n    table = table[non_empty_row_mask]\n\n    try:\n        table = table.astype(np.float32)\n    except ValueError:\n        return excuse, None\n    x = torch.tensor(table[:,0]).unsqueeze(1)\n    y = torch.tensor(table[:,1])\n    fig = plt.figure(figsize=(8,4),dpi=1000)\n\n    if len(x) > 4:\n        return excuse_max_examples, None\n    if (x<0.).any() or (x>1.).any() or (y<-1).any() or (y>1).any():\n        return excuse, None\n\n    plt.scatter(x,y, color='black', label='Examples in given dataset')\n\n\n    \n    test_xs = torch.linspace(0,1,100).unsqueeze(1)\n    \n    plot_w_conf_interval(plt, test_xs, *mean_and_bounds_for_gp(x,y,test_xs), 'green', 'GP')\n    plot_w_conf_interval(plt, test_xs, *mean_and_bounds_for_pnf(x,y,test_xs, choice), 'blue', 'PFN')\n    \n    plt.legend(ncol=2,bbox_to_anchor=[0.5,-.14],loc=\"upper center\")\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.tight_layout()\n\n    \n    return 'There you go, your plot. \ud83d\udcc8', plt.gcf()\n\niface = gr.Interface(fn=infer,\n                     title='GP Posterior Approximation with Transformers',\n                     description='''This is a demo of PFNs as we describe them in our recent paper (https:\/\/openreview.net\/forum?id=KSugKcbNf9).\nLines represent means and shaded areas are the confidence interval (68.2% quantile). In green, we have the ground truth GP posterior and in blue we have our approximation.\nWe provide three models that are architecturally the same, but with different training budgets.\nThe GP (approximated) uses an RBF Kernel with a little noise (1e-4), 0 mean and a length scale of 0.1.\n                     ''',\n                     article=\"<p style='text-align: center'><a href='https:\/\/arxiv.org\/abs\/2112.10510'>Paper: Transformers Can Do Bayesian Inference<\/a><\/p>\",\n                     inputs=[\n                         gr.inputs.Dataframe(headers=[\"x\", \"y\"], datatype=[\"number\", \"number\"], type='numpy', default=[['.25','.1'],['.75','.4']], col_count=2, label='The data: you can change this and increase the number of data points using the `enter` key.'),\n                         gr.inputs.Radio(['160K','800K','4M'], type=\"value\", default='4M', label='Number of Sampled Datasets in Training (Training Costs), higher values yield better results')\n                     ], outputs=[\"text\",gr.outputs.Plot(type=\"matplotlib\")])\niface.launch()\n\n\n\n\n```"}
{"question":"\nI have two pre-trained AnimeGANv2 models that can take an input portrait image and transform it into an anime style portrait. The two models trade off between better stylization vs better robustness. With my current code, I can load the models and run inference on an input image to generate the anime portrait.\n\nI would like to create an interactive Gradio web demo showcasing this AnimeGANv2 portrait generation with the following features:\n- Allow the user to upload their own portrait image \n- Have a radio button input to select between the two model versions (stylization vs robustness)\n- Display the generated anime portrait image output\n- Provide some pre-loaded example images that the user can easily test out\n- Include article text with links to the model's Github repo\n- Customize the title and description text for the interface\n","answer":"here is the gradio solution for your use case: ```python\nfrom PIL import Image\nimport torch\nimport gradio as gr\n\n\n\nmodel2 = torch.hub.load(\n    \"AK391\/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    device=\"cuda\",\n    progress=False\n)\n\n\nmodel1 = torch.hub.load(\"AK391\/animegan2-pytorch:main\", \"generator\", pretrained=\"face_paint_512_v1\",  device=\"cuda\")\nface2paint = torch.hub.load(\n    'AK391\/animegan2-pytorch:main', 'face2paint', \n    size=512, device=\"cuda\",side_by_side=False\n)\ndef inference(img, ver):\n    if ver == 'version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)':\n        out = face2paint(model2, img)\n    else:\n        out = face2paint(model1, img)\n    return out\n  \ntitle = \"AnimeGANv2\"\ndescription = \"Gradio Demo for AnimeGanv2 Face Portrait. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below. Please use a cropped portrait picture for best results similar to the examples below.\"\narticle = \"<p style='text-align: center'><a href='https:\/\/github.com\/bryandlee\/animegan2-pytorch' target='_blank'>Github Repo Pytorch<\/a><\/p> <center><img src='https:\/\/visitor-badge.glitch.me\/badge?page_id=akhaliq_animegan' alt='visitor badge'><\/center><\/p>\"\nexamples=[['groot.jpeg','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['bill.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['tony.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['elon.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['IU.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['billie.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['will.png','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['beyonce.png','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)'],['gongyoo.jpeg','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)']]\ngr.Interface(inference, [gr.inputs.Image(type=\"pil\"),gr.inputs.Radio(['version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'], type=\"value\", default='version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)', label='version')\n], gr.outputs.Image(type=\"pil\"),title=title,description=description,article=article,examples=examples,allow_flagging=False,allow_screenshot=False).launch()\n```"}
{"question":"\nI have a PyTorch implementation of the AnimeGAN-V2 model that can take an input image and transform it into an anime-style version of that image. With my current code, I can load the pre-trained model and run inference on an input image to generate the anime version.\n\nI would like to create an interactive Gradio web interface that allows users to apply the AnimeGAN-V2 model to videos, both from pre-recorded video files and live from their webcam. The interface should have the following features:\n\n- Two tabs, one for uploading a video file and one for using the webcam as input \n- For the upload tab:\n  - A video file upload component to let the user select an input video\n  - A few example videos the user can quickly test\n- For both tabs:  \n  - Sliders to let the user select the start time and duration of the video clip to process\n  - Display the generated anime-style output video after running the model\n- Use a queueing system to handle multiple requests\n- Show a description of the model and link to the Github repo\n- Disable the user flagging feature\n\nSome key technical details:\n- I'll need to write functions to extract frames from the input video, run the model on each frame, and stitch the output frames back into a video\n- To handle different length videos, add logic to concatenate the output frames\n- Use the new Gradio 3.0 features like gr.TabbedInterface to organize the upload vs webcam interfaces\n- Wrap the video processing logic in a predict function to work with the Gradio queueing system\n\nLet me know if you have any other suggestions to improve the interface!\n","answer":"here is the gradio solution for your use case: ```python\nimport gc\nimport math\nimport traceback\n\nimport gradio as gr\nimport numpy as np\nimport torch\nfrom encoded_video import EncodedVideo, write_video\nfrom PIL import Image\nfrom torchvision.transforms.functional import center_crop, to_tensor\n\n\n\n\nprint(\"\ud83e\udde0 Loading Model...\")\nmodel = torch.hub.load(\n    \"AK391\/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    device=\"cuda\",\n    progress=True,\n)\n\n\ndef face2paint(model: torch.nn.Module, img: Image.Image, size: int = 512, device: str = 'cuda'):\n    w, h = img.size\n    s = min(w, h)\n    img = img.crop(((w - s) \/\/ 2, (h - s) \/\/ 2, (w + s) \/\/ 2, (h + s) \/\/ 2))\n    img = img.resize((size, size), Image.LANCZOS)\n\n    with torch.no_grad():\n        input = to_tensor(img).unsqueeze(0) * 2 - 1\n        output = model(input.to(device)).cpu()[0]\n\n        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n\n    return output\n\n\n# This function is taken from pytorchvideo!\ndef uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = -3) -> torch.Tensor:\n    \"\"\"\n    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n    When num_samples is larger than the size of temporal dimension of the video, it\n    will sample frames based on nearest neighbor interpolation.\n    Args:\n        x (torch.Tensor): A video tensor with dimension larger than one with torch\n            tensor type includes int, long, float, complex, etc.\n        num_samples (int): The number of equispaced samples to be selected\n        temporal_dim (int): dimension of temporal to perform temporal subsample.\n    Returns:\n        An x-like Tensor with subsampled temporal dimension.\n    \"\"\"\n    t = x.shape[temporal_dim]\n    assert num_samples > 0 and t > 0\n    # Sample by nearest neighbor interpolation if num_samples > t.\n    indices = torch.linspace(0, t - 1, num_samples)\n    indices = torch.clamp(indices, 0, t - 1).long()\n    return torch.index_select(x, temporal_dim, indices)\n\n\n# This function is taken from pytorchvideo!\ndef short_side_scale(\n    x: torch.Tensor,\n    size: int,\n    interpolation: str = \"bilinear\",\n) -> torch.Tensor:\n    \"\"\"\n    Determines the shorter spatial dim of the video (i.e. width or height) and scales\n    it to the given size. To maintain aspect ratio, the longer side is then scaled\n    accordingly.\n    Args:\n        x (torch.Tensor): A video tensor of shape (C, T, H, W) and type torch.float32.\n        size (int): The size the shorter side is scaled to.\n        interpolation (str): Algorithm used for upsampling,\n            options: nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'\n    Returns:\n        An x-like Tensor with scaled spatial dims.\n    \"\"\"\n    assert len(x.shape) == 4\n    assert x.dtype == torch.float32\n    c, t, h, w = x.shape\n    if w < h:\n        new_h = int(math.floor((float(h) \/ w) * size))\n        new_w = size\n    else:\n        new_h = size\n        new_w = int(math.floor((float(w) \/ h) * size))\n\n    return torch.nn.functional.interpolate(x, size=(new_h, new_w), mode=interpolation, align_corners=False)\n\n\ndef inference_step(vid, start_sec, duration, out_fps):\n\n    clip = vid.get_clip(start_sec, start_sec + duration)\n    video_arr = torch.from_numpy(clip['video']).permute(3, 0, 1, 2)\n    audio_arr = np.expand_dims(clip['audio'], 0)\n    audio_fps = None if not vid._has_audio else vid._container.streams.audio[0].sample_rate\n\n    x = uniform_temporal_subsample(video_arr, duration * out_fps)\n    x = center_crop(short_side_scale(x, 512), 512)\n    x \/= 255.0\n    x = x.permute(1, 0, 2, 3)\n    with torch.no_grad():\n        output = model(x.to('cuda')).detach().cpu()\n        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n        output_video = output.permute(0, 2, 3, 1).numpy()\n\n    return output_video, audio_arr, out_fps, audio_fps\n\n\ndef predict_fn(filepath, start_sec, duration):\n    out_fps = 18\n    vid = EncodedVideo.from_path(filepath)\n    for i in range(duration):\n        print(f\"\ud83d\uddbc\ufe0f Processing step {i + 1}\/{duration}...\")\n        video, audio, fps, audio_fps = inference_step(vid=vid, start_sec=i + start_sec, duration=1, out_fps=out_fps)\n        gc.collect()\n        if i == 0:\n            video_all = video\n            audio_all = audio\n        else:\n            video_all = np.concatenate((video_all, video))\n            audio_all = np.hstack((audio_all, audio))\n\n    print(f\"\ud83d\udcbe Writing output video...\")\n    \n    try:\n        write_video('out.mp4', video_all, fps=fps, audio_array=audio_all, audio_fps=audio_fps, audio_codec='aac')\n    except Exception:\n        print(\"\u274c Error when writing with audio...trying without audio:\")\n        print(traceback.format_exc())\n        print()\n        print(f\"audio...\", audio.shape)\n        write_video('out.mp4', video_all, fps=fps)\n\n    print(f\"\u2705 Done!\")\n    del video_all\n    del audio_all\n\n    return 'out.mp4'\n\n\narticle = \"\"\"\n<p style='text-align: center'>\n    <a href='https:\/\/github.com\/bryandlee\/animegan2-pytorch' target='_blank'>Github Repo Pytorch<\/a>\n<\/p>\n\"\"\"\n\niface_webcam = gr.Interface(\n    predict_fn,\n    inputs=[\n        gr.Video(source=\"webcam\"),\n        gr.Slider(minimum=0, maximum=300, step=1, default=0),\n        gr.Slider(minimum=1, maximum=10, step=1, default=2),\n        # gr.inputs.Slider(minimum=6, maximum=18, step=6, default=12),  # Leaving manual fps out for now\n    ],\n    outputs=gr.Video(),\n    title='AnimeGANV2 On Videos',\n    description=\"Applying AnimeGAN-V2 to frames from video clips\",\n    article=article,\n    enable_queue=True,\n    allow_flagging=False,\n)\n\niface_file = gr.Interface(\n    predict_fn,\n    inputs=[\n        gr.Video(source=\"upload\"),\n        gr.Slider(minimum=0, maximum=300, step=1, default=0),\n        gr.Slider(minimum=1, maximum=10, step=1, default=2),\n    ],\n    outputs=gr.Video(),\n    title='AnimeGANV2 On Videos',\n    description=\"Applying AnimeGAN-V2 to frames from video clips\",\n    article=article,\n    enable_queue=True,\n    examples=[\n        ['driving.mp4', 0, 6],\n        ['bella_poarch.mp4', 4, 8],\n        ['obama.webm', 0, 4],\n    ],\n    allow_flagging=False,\n)\n\nif __name__ == '__main__':\n    gr.TabbedInterface(\n        interface_list=[iface_file, iface_webcam],\n        tab_names=[\"From a File!\", \"From your Webcam!\"]\n    ).launch()\n```"}
{"question":"\nI have an anime face detection model that can detect faces and facial landmarks in anime-style images. With my current code, I can load the model and run inference on an input image to get the bounding boxes and landmark points for any detected faces.\n\nI would like to create an interactive Gradio demo to showcase the model's capabilities, with the following features:\n- Allow the user to upload an anime image \n- Display the uploaded image\n- Let the user select between two different detector models (faster-rcnn and yolov3)\n- Have sliders to adjust the confidence score thresholds for face detection and landmark detection\n- When the user clicks submit, run the selected model on the image with the specified settings\n- Display the output image with the detected faces and landmarks drawn on it\n- Provide some sample images that the user can easily test out\n- Show metadata like the project title, description, links, etc.\n- Configure sharing and other interface options\n","answer":"here is the gradio solution for your use case: ```python\n#!\/usr\/bin\/env python\n\nfrom __future__ import annotations\n\nimport argparse\nimport functools\nimport os\nimport pathlib\nimport subprocess\nimport tarfile\n\nif os.environ.get('SYSTEM') == 'spaces':\n    import mim\n\n    mim.uninstall('mmcv-full', confirm_yes=True)\n    mim.install('mmcv-full==1.3.16', is_yes=True)\n\n    subprocess.call('pip uninstall -y opencv-python'.split())\n    subprocess.call('pip uninstall -y opencv-python-headless'.split())\n    subprocess.call('pip install opencv-python-headless'.split())\n\nimport anime_face_detector\nimport cv2\nimport gradio as gr\nimport huggingface_hub\nimport numpy as np\nimport torch\n\nTITLE = 'hysts\/anime-face-detector'\nDESCRIPTION = 'This is a demo for https:\/\/github.com\/hysts\/anime-face-detector.'\nARTICLE = '<center><img src=\"https:\/\/visitor-badge.glitch.me\/badge?page_id=hysts.anime-face-detector\" alt=\"visitor badge\"\/><\/center>'\n\nTOKEN = os.environ['TOKEN']\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', type=str, default='cpu')\n    parser.add_argument('--theme', type=str)\n    parser.add_argument('--live', action='store_true')\n    parser.add_argument('--share', action='store_true')\n    parser.add_argument('--port', type=int)\n    parser.add_argument('--disable-queue',\n                        dest='enable_queue',\n                        action='store_false')\n    parser.add_argument('--allow-flagging', type=str, default='never')\n    parser.add_argument('--allow-screenshot', action='store_true')\n    return parser.parse_args()\n\n\ndef load_sample_image_paths() -> list[pathlib.Path]:\n    image_dir = pathlib.Path('images')\n    if not image_dir.exists():\n        dataset_repo = 'hysts\/sample-images-TADNE'\n        path = huggingface_hub.hf_hub_download(dataset_repo,\n                                               'images.tar.gz',\n                                               repo_type='dataset',\n                                               use_auth_token=TOKEN)\n        with tarfile.open(path) as f:\n            f.extractall()\n    return sorted(image_dir.glob('*'))\n\n\ndef detect(\n        image: np.ndarray, detector_name: str, face_score_threshold: float,\n        landmark_score_threshold: float,\n        detectors: dict[str,\n                        anime_face_detector.LandmarkDetector]) -> np.ndarray:\n    detector = detectors[detector_name]\n    # RGB -> BGR\n    image = image[:, :, ::-1]\n    preds = detector(image)\n\n    res = image.copy()\n    for pred in preds:\n        box = pred['bbox']\n        box, score = box[:4], box[4]\n        if score < face_score_threshold:\n            continue\n        box = np.round(box).astype(int)\n\n        line_width = max(2, int(3 * (box[2:] - box[:2]).max() \/ 256))\n\n        cv2.rectangle(res, tuple(box[:2]), tuple(box[2:]), (0, 255, 0),\n                      line_width)\n\n        pred_pts = pred['keypoints']\n        for *pt, score in pred_pts:\n            if score < landmark_score_threshold:\n                color = (0, 255, 255)\n            else:\n                color = (0, 0, 255)\n            pt = np.round(pt).astype(int)\n            cv2.circle(res, tuple(pt), line_width, color, cv2.FILLED)\n    return res[:, :, ::-1]\n\n\ndef main():\n    args = parse_args()\n    device = torch.device(args.device)\n\n    detector_names = ['faster-rcnn', 'yolov3']\n    detectors = {\n        detector_name: anime_face_detector.create_detector(detector_name,\n                                                           device=device)\n        for detector_name in detector_names\n    }\n\n    func = functools.partial(detect, detectors=detectors)\n    func = functools.update_wrapper(func, detect)\n\n    image_paths = load_sample_image_paths()\n    examples = [[path.as_posix(), 'yolov3', 0.5, 0.3] for path in image_paths]\n\n    gr.Interface(\n        func,\n        [\n            gr.inputs.Image(type='numpy', label='Input'),\n            gr.inputs.Radio(detector_names,\n                            type='value',\n                            default='yolov3',\n                            label='Detector'),\n            gr.inputs.Slider(\n                0, 1, step=0.05, default=0.5, label='Face Score Threshold'),\n            gr.inputs.Slider(\n                0, 1, step=0.05, default=0.3,\n                label='Landmark Score Threshold'),\n        ],\n        gr.outputs.Image(type='numpy', label='Output'),\n        examples=examples,\n        title=TITLE,\n        description=DESCRIPTION,\n        article=ARTICLE,\n        theme=args.theme,\n        allow_screenshot=args.allow_screenshot,\n        allow_flagging=args.allow_flagging,\n        live=args.live,\n    ).launch(\n        enable_queue=args.enable_queue,\n        server_port=args.port,\n        share=args.share,\n    )\n\n\nif __name__ == '__main__':\n    main()\n\n```"}
{"question":"\nI have a machine learning model from Spotify that can take song lyrics as input and predict the top genres that the song belongs to. With my current code, I can input lyrics, run the model, and get back the top predicted genre labels and their confidence scores as output.\n\nI would like to create an interactive Gradio web interface for this song genre classification model with the following features:\n- A large text input box for the user to paste in song lyrics \n- A title and description explaining what the interface does\n- Display the top 5 predicted genre labels as output, after running the model\n- Include some examples of song lyrics that can be loaded into the input box, from a few different well-known songs\n- Launch the interface so it can be used in a web browser\n","answer":"here is the gradio solution for your use case: ```python\nimport gradio as gr\nimport numpy as np\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom examples import yellow, stairway, numb, puppets, firework \n\ndef lyrics_categories(input_text):\n  spotify_model = \"juliensimon\/autonlp-song-lyrics-18753417\"\n  model = AutoModelForSequenceClassification.from_pretrained(spotify_model)\n  tokenizer = AutoTokenizer.from_pretrained(spotify_model)\n  labels = model.config.id2label\n  inputs = tokenizer(input_text, return_tensors=\"pt\")\n  outputs = model(**inputs)\n  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n  predictions = predictions.detach().numpy()[0]\n  index_sorted = np.argsort(predictions)[::-1]\n  clean_outputs = {labels[idx]:str(predictions[idx]) for idx in index_sorted}\n  print(clean_outputs)\n  return clean_outputs\n  \ndescription = \"With lyrics, find the top 5 genres this song belongs to! (Powered by Spotify)\"\n \niface = gr.Interface(fn=lyrics_categories, \n                     inputs=gr.inputs.Textbox(lines=20, placeholder=\"Enter song lyrics here...\", label=\"Song Lyrics\"),\n                     outputs=gr.outputs.Label(num_top_classes=5, label=\"Genres\/Categories\"),\n                     examples=[stairway, numb, puppets, firework, yellow],\n                     article=description,\n                     title=\"Song Genre Predictor\",\n                    )\niface.launch()\n\n```"}
