{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ed4c80-6ad7-40dc-aeed-bd843fb2c9ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91d884a-7789-4286-b8f8-f5be33ec1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2748a681-2f0e-447d-a1fb-933382dfda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b086321-fb2b-481a-a782-4de9e0e4eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9071fde9-aec2-4b97-8ae1-b69c574319f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['ANTHROPIC_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f1cf7b-4327-49f1-ab84-bcb5e2e00e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Use ** to match all files in all subdirectories\n",
    "files = glob('../data/latest-docs/**/*.md', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "677c4e54-a1ab-438c-9a76-47cef15dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.Series(files).rename('fp').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2a27908-d474-4c4d-bffb-d1a867dc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['fn'] = files.fp.str.rsplit(\"/\",n=1).str[-1]#,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b81577ee-58b6-4771-a44d-b1414f2dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['category'] = files.fp.str.split(\"/\",expand=True)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de828-3132-4ed9-bb97-27ab2335f0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8430e3cb-3b75-46d2-9701-1ec8892fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d02cabe-c1a9-4e3f-83b9-295e2d123b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = \"\"\"\n",
    "<prompt_template>\n",
    "You will be acting as a coding assistant to help answer questions about the Gradio Python framework. I will provide you with a chunk of documentation about Gradio. Your task is to identify the core concepts covered in this documentation and generate questions that the documentation would be key in answering. Then, you will provide concise, reformatted answers to these questions based on the documentation.\n",
    "You will only retun the jsonl question answer pair content no preamble.\n",
    "\n",
    "Here is the chunk of Gradio documentation:\n",
    "<documentation_chunk>\n",
    "{{DOCUMENTATION_CHUNK}}\n",
    "</documentation_chunk>\n",
    "\n",
    "Please carefully read through the documentation chunk and identify the core concepts it covers. For each core concept, simulate a question that a user might ask where this part of the documentation would be essential to providing a good answer. Try to generate at least one question per core concept.\n",
    "\n",
    "Once you have your list of questions, answer each one by reformatting the relevant parts of the documentation into an optimal, concise answer. Focus on capturing the key information needed to address the question, but don't simply copy and paste from the documentation - aim to rephrase things in a more accessible way.\n",
    "\n",
    "Please return your output in JSONL format, with each line containing a JSON object representing a simulated question and answer pair. The JSON object should have a \"question\" field with the simulated question, and an \"answer\" field with the concise, reformatted answer you generated from the documentation.\n",
    "</prompt_template>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d6402f-da50-475b-a164-d0a5cd303d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_markdown(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def parse_markdown(content):\n",
    "    # Regex pattern to match markdown headings\n",
    "    heading_pattern = re.compile(r'^(#{1,6})\\s+(.*)', re.MULTILINE)\n",
    "    headings = []\n",
    "    for match in heading_pattern.finditer(content):\n",
    "        level = len(match.group(1))\n",
    "        title = match.group(2).strip()\n",
    "        start = match.start()\n",
    "        headings.append((level, title, start))\n",
    "    return headings\n",
    "\n",
    "def get_text_chunks(content, headings):\n",
    "    chunks = []\n",
    "    for i in range(len(headings)):\n",
    "        level, title, start = headings[i]\n",
    "        if i + 1 < len(headings):\n",
    "            end = headings[i + 1][2]\n",
    "        else:\n",
    "            end = len(content)\n",
    "        chunk = content[start:end].strip()\n",
    "        chunks.append((level, title, chunk))\n",
    "    return chunks\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    hierarchy = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}\n",
    "    processed_chunks = []\n",
    "\n",
    "    for level, title, chunk in chunks:\n",
    "        hierarchy[level] = title\n",
    "        upper_headings = [hierarchy[i] for i in range(1, level) if hierarchy[i] is not None]\n",
    "        full_title = \" > \".join(upper_headings + [title])\n",
    "        processed_chunks.append((full_title, chunk))\n",
    "\n",
    "    return processed_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22089be5-3511-4f01-b3b3-567d39fd3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for index, row in files.iterrows():\n",
    "    content = read_markdown(row.fp)\n",
    "    headings = parse_markdown(content)\n",
    "    chunks = get_text_chunks(content, headings)\n",
    "    processed_chunks = process_chunks(chunks)\n",
    "    \n",
    "    for title, chunk in processed_chunks:\n",
    "        all_chunks.append(f\"Heading: {title}\\n Content:\\n{chunk}\\n\")\n",
    "        #all_chunks.append(f\"Category: {row.category}\\n Heading: {title}\\n Content:\\n{chunk}\\n\") # maybe the folder name might be valueable context?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f74b888-fc98-4aa9-9605-edf254d462af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk = all_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27b2ce5d-1369-47f9-b5ba-4f6f67bcb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8eba0-daca-48b8-a3b7-7f53e65ca271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the responses\n",
    "responses = []\n",
    "# Iterate through the documentation chunks\n",
    "counter_incase_crash = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd9332-4950-4584-956f-586274412ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in tqdm(all_chunks):\n",
    "    # Insert the chunk into the prompt template\n",
    "    prompt = prompt_template.replace(\"{{DOCUMENTATION_CHUNK}}\", chunk)\n",
    "    \n",
    "    # Send the prompt to Claude Opus\n",
    "    response = client.messages.create(\n",
    "    max_tokens=2048,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    )\n",
    "    \n",
    "    # Get the assistant's response\n",
    "    assistant_response = response.content\n",
    "    \n",
    "    # Append the response to the list\n",
    "    responses.append(assistant_response)\n",
    "    counter_incase_crash+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d84c75-c1ba-4365-a5ba-aa2710497067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the responses\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cde6c33-70eb-45de-aa5f-d2544346e3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'306of553'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress=f'{counter_incase_crash}of{len(all_chunks)}'\n",
    "progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db9bf902-d761-45ac-9f15-fd563c07e9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/MitchellBaskerville/code/gradio-fine-tuning/notebooks'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c544dcc1-86ed-46f2-ad55-cdab0710cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = pd.Series([all_chunks[i].split('\\n')[0].replace('Heading: ','') for i in range(len(all_chunks))]).rename('header_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ef65931-1835-4367-aaf1-2b7f8bb7fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.Series(responses).rename('all_data')\n",
    "pd.concat([header_names,responses_df],axis=1).astype(str).to_parquet(os.getcwd() + f'/../datasets/raw/opus_doc_qas{progress}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6484c645-e056-4a15-b638-e92c70eb4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.getcwd() + f'/../datasets/qa_pairs/opus_doc_qas_{progress}.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "861bb765-c155-46b4-b610-2bac121300e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_filepath, 'w') as jsonl_file:\n",
    "    for index, opus_response in responses_df.items():\n",
    "        for jsl in opus_response[0].text.split('\\n'):\n",
    "            jsl=jsl.replace('\\\\','').replace('\"','`').replace(\"`question`: `\",'\"question\": \"').replace(\"`, `answer`: `\",'\", \"answer\": \"').replace(\"`}\",'\"}')\n",
    "            jsonl_file.write(jsl + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77b639-8616-4162-9426-4f2814e2c819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
