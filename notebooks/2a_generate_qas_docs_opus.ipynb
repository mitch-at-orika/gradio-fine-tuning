{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ed4c80-6ad7-40dc-aeed-bd843fb2c9ec",
   "metadata": {},
   "source": [
    "# TASK OVERVIEW\n",
    "Here is the detailed context content for the next phase of your task to fine-tune an assistant that can help with developing and coding Gradio apps:\n",
    "To generate synthetic data for fine-tuning a Gradio app development assistant, we can follow a two-step process of distillation from a stronger model:\n",
    "\n",
    "Generate diverse instructions and input context related to Gradio app development\n",
    "Generate high-quality responses to those instructions\n",
    "\n",
    "For step 1, start by creating a small seed set of human-written instructions covering key aspects of Gradio app development, such as:\n",
    "\n",
    "Designing the user interface and layout\n",
    "Defining input and output components\n",
    "Integrating machine learning models\n",
    "Handling events and interactivity\n",
    "Customizing the look and feel\n",
    "Deploying the app\n",
    "\n",
    "Example seed instructions:\n",
    "\"Design a simple Gradio interface that allows a user to input text and displays the sentiment (positive, negative, neutral).\"\n",
    "\"Explain how to load a pretrained machine learning model from Hugging Face and use it to make predictions in a Gradio app.\"\n",
    "\"What are the different options for deploying a Gradio app and how do you configure them?\"\n",
    "Then, use a strong language model like GPT-3.5 or GPT-4 to generate additional diverse instructions by few-shot prompting with the seed examples. Encourage the model to be creative and cover a wide range of Gradio development topics. Also have it generate relevant input context for each instruction.\n",
    "Filter the generated instructions to remove any that are invalid, irrelevant, or duplicates. Aim to build a dataset of hundreds to thousands of unique instructions spanning beginner to advanced Gradio development.\n",
    "For step 2, use the same strong language model to generate high-quality responses for each instruction-input pair. Provide the model with the instruction and input context, and sample multiple candidate responses using techniques like nucleus sampling to encourage diversity while maintaining coherence.\n",
    "Have the model self-evaluate each candidate response and select the best one, or rank them. Criteria for evaluation can include:\n",
    "\n",
    "Directly addressing the instruction\n",
    "Correctness of the information\n",
    "Clarity of explanation\n",
    "Conciseness\n",
    "Helpfulness for a developer\n",
    "\n",
    "The final result will be a synthetic dataset of instruction-input-response pairs covering a wide range of topics and skills needed for Gradio app development. This can then be used to fine-tune the target LLaMA model using the LoRA adapter.\n",
    "Some key considerations during the data generation process:\n",
    "\n",
    "Ensure a good mix of high-level and low-level instructions, covering both conceptual topics as well as specific coding details\n",
    "Aim for diversity of instructions to teach a broad skillset, but also generate multiple examples for key concepts to reinforce them\n",
    "Prompt the language model to include code snippets and examples in the responses to make them more actionable\n",
    "Experiment with different prompts, sampling parameters, and filtering criteria to optimize the quality of the synthetic data\n",
    "Spot check samples of the generated data and iterate to improve quality\n",
    "\n",
    "By distilling high-quality instruction-tuning data from a strong language model, we can imbue the target LLaMA model with comprehensive knowledge of Gradio app development best practices. The resulting fine-tuned model should be able to engage in substantive conversations about Gradio and provide detailed, helpful guidance to developers working on Gradio projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91d884a-7789-4286-b8f8-f5be33ec1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2748a681-2f0e-447d-a1fb-933382dfda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b086321-fb2b-481a-a782-4de9e0e4eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9071fde9-aec2-4b97-8ae1-b69c574319f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['ANTHROPIC_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f1cf7b-4327-49f1-ab84-bcb5e2e00e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Use ** to match all files in all subdirectories\n",
    "files = glob('../data/latest-docs/**/*.md', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "677c4e54-a1ab-438c-9a76-47cef15dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.Series(files).rename('fp').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2a27908-d474-4c4d-bffb-d1a867dc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['fn'] = files.fp.str.rsplit(\"/\",n=1).str[-1]#,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b81577ee-58b6-4771-a44d-b1414f2dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['category'] = files.fp.str.split(\"/\",expand=True)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de828-3132-4ed9-bb97-27ab2335f0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8430e3cb-3b75-46d2-9701-1ec8892fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d02cabe-c1a9-4e3f-83b9-295e2d123b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = \"\"\"\n",
    "<prompt_template>\n",
    "You will be acting as a coding assistant to help answer questions about the Gradio Python framework. I will provide you with a chunk of documentation about Gradio. Your task is to identify the core concepts covered in this documentation and generate questions that the documentation would be key in answering. Then, you will provide concise, reformatted answers to these questions based on the documentation.\n",
    "You will only retun the jsonl question answer pair content no preamble.\n",
    "\n",
    "Here is the chunk of Gradio documentation:\n",
    "<documentation_chunk>\n",
    "{{DOCUMENTATION_CHUNK}}\n",
    "</documentation_chunk>\n",
    "\n",
    "Please carefully read through the documentation chunk and identify the core concepts it covers. For each core concept, simulate a question that a user might ask where this part of the documentation would be essential to providing a good answer. Try to generate at least one question per core concept.\n",
    "\n",
    "Once you have your list of questions, answer each one by reformatting the relevant parts of the documentation into an optimal, concise answer. Focus on capturing the key information needed to address the question, but don't simply copy and paste from the documentation - aim to rephrase things in a more accessible way.\n",
    "\n",
    "Please return your output in JSONL format, with each line containing a JSON object representing a simulated question and answer pair. The JSON object should have a \"question\" field with the simulated question, and an \"answer\" field with the concise, reformatted answer you generated from the documentation.\n",
    "</prompt_template>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d6402f-da50-475b-a164-d0a5cd303d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_markdown(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def parse_markdown(content):\n",
    "    # Regex pattern to match markdown headings\n",
    "    heading_pattern = re.compile(r'^(#{1,6})\\s+(.*)', re.MULTILINE)\n",
    "    headings = []\n",
    "    for match in heading_pattern.finditer(content):\n",
    "        level = len(match.group(1))\n",
    "        title = match.group(2).strip()\n",
    "        start = match.start()\n",
    "        headings.append((level, title, start))\n",
    "    return headings\n",
    "\n",
    "def get_text_chunks(content, headings):\n",
    "    chunks = []\n",
    "    for i in range(len(headings)):\n",
    "        level, title, start = headings[i]\n",
    "        if i + 1 < len(headings):\n",
    "            end = headings[i + 1][2]\n",
    "        else:\n",
    "            end = len(content)\n",
    "        chunk = content[start:end].strip()\n",
    "        chunks.append((level, title, chunk))\n",
    "    return chunks\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    hierarchy = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}\n",
    "    processed_chunks = []\n",
    "\n",
    "    for level, title, chunk in chunks:\n",
    "        hierarchy[level] = title\n",
    "        upper_headings = [hierarchy[i] for i in range(1, level) if hierarchy[i] is not None]\n",
    "        full_title = \" > \".join(upper_headings + [title])\n",
    "        processed_chunks.append((full_title, chunk))\n",
    "\n",
    "    return processed_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22089be5-3511-4f01-b3b3-567d39fd3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for index, row in files.iterrows():\n",
    "    content = read_markdown(row.fp)\n",
    "    headings = parse_markdown(content)\n",
    "    chunks = get_text_chunks(content, headings)\n",
    "    processed_chunks = process_chunks(chunks)\n",
    "    \n",
    "    for title, chunk in processed_chunks:\n",
    "        all_chunks.append(f\"Heading: {title}\\n Content:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f74b888-fc98-4aa9-9605-edf254d462af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk = all_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27b2ce5d-1369-47f9-b5ba-4f6f67bcb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1618b4-d262-4bfe-9672-8052fc1ad3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████▋                                         | 271/553 [48:19<49:00, 10.43s/it]"
     ]
    }
   ],
   "source": [
    "# List to store the responses\n",
    "responses = []\n",
    "# Iterate through the documentation chunks\n",
    "counter_incase_crash = 0\n",
    "for chunk in tqdm(all_chunks):\n",
    "    # Insert the chunk into the prompt template\n",
    "    prompt = prompt_template.replace(\"{{DOCUMENTATION_CHUNK}}\", chunk)\n",
    "    \n",
    "    # Send the prompt to Claude Opus\n",
    "    response = client.messages.create(\n",
    "    max_tokens=2048,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    )\n",
    "    \n",
    "    # Get the assistant's response\n",
    "    assistant_response = response.content\n",
    "    \n",
    "    # Append the response to the list\n",
    "    responses.append(assistant_response)\n",
    "    counter_incase_crash+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d84c75-c1ba-4365-a5ba-aa2710497067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the responses\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ef65931-1835-4367-aaf1-2b7f8bb7fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[TextBlock(text='{\"question\": \"What are the two main types of guides that can be contributed to Gradio?\", \"answer\": \"The two main types of guides that can be contributed to Gradio are:\\\\n1. Use cases: step-by-step guides on how to build a particular type of machine learning demo or app using Gradio.\\\\n2. Feature explanation: detailed guides that describe a particular feature of Gradio.\"}\\n{\"question\": \"Can you give an example of a \\'use case\\' style guide in the Gradio documentation?\", \"answer\": \"Yes, an example of a \\'use case\\' style guide in the Gradio documentation is the guide titled \\\\\"Creating a Chatbot\\\\\". This guide covers step-by-step how to build a chatbot demo or app using Gradio.\"}\\n{\"question\": \"What\\'s an example of a \\'feature explanation\\' style guide from the Gradio docs?\", \"answer\": \"An example of a \\'feature explanation\\' style guide in the Gradio documentation is the guide titled \\\\\"Using Flagging\\\\\". This guide describes the flagging feature of Gradio in detail.\"}\\n{\"question\": \"Where can I find ideas for guides to contribute to Gradio?\", \"answer\": \"If you\\'re looking for ideas on guides to contribute to Gradio, you can check the open issues on the Gradio GitHub repository. There may be issues where users have requested guides on particular topics.\"}', type='text')],\n",
       " [TextBlock(text='{\"question\": \"What is the typical structure and content of a Gradio Guide?\", \"answer\": \"Gradio Guides are markdown documents that usually start with an introduction describing the topic, include subheadings for easy navigation, feature real code snippets to follow along with, and have embedded interactive Gradio demos hosted on Hugging Face Spaces to demonstrate the concepts being discussed.\"}\\n{\"question\": \"How can I include interactive demos in my Gradio Guide?\", \"answer\": \"To make your Gradio Guide more interactive, you can embed Gradio demos that are hosted on Hugging Face Spaces. Use the standard HTML <iframe> tag to embed the demo directly in your markdown document.\"}\\n{\"question\": \"Where should I host the Gradio demos that I want to include in my Guide?\", \"answer\": \"Host the Gradio demos you want to embed in your Guide on Hugging Face Spaces. This will allow you to easily embed them in your markdown document using an <iframe> tag.\"}', type='text')],\n",
       " [TextBlock(text='{\"question\": \"How do I contribute a new guide to the Gradio documentation?\", \"answer\": \"To contribute a guide, first clone or fork the Gradio repo. Then add a new markdown file with a descriptive title to the /guides folder. Write your guide content in the markdown file, embedding Gradio demos where helpful. At the top of the file, include a list of related_spaces and 3 relevant tags. Finally, open a PR to have your new guide reviewed and added.\"}\\n{\"question\": \"What format should I write my Gradio guide in?\", \"answer\": \"Guides should be written in standard markdown format. You can embed Gradio demos directly in the markdown wherever it helps illustrate concepts in your guide.\"}\\n{\"question\": \"How do I help users find my Gradio guide?\", \"answer\": \"To help users discover your guide, add 3 relevant tags at the top of your markdown file. This will allow your guide to surface when users search for those topics. You should also include a list of related Hugging Face Spaces at the top of the file to link your guide to relevant Gradio demos.\"}', type='text')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df = pd.Series(responses).rename('all_data')\n",
    "responses_df.to_frame().to_parquet('../datasets/raw/opus_doc_qas.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e1a96-2365-4e87-b71b-f112a4d92218",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "opus_response = responses_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19df128-7827-46bf-8cf8-05e9f7e31863",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, opus_response in responses_df.items():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
