{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e976a1b-9954-4153-8ece-bf243eb8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7835d94-1494-42cb-97f1-8cf31fbcbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c6720c-5ab8-44c5-b5cd-5f7cc055f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from fastchat.conversation import Conversation, SeparatorStyle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57f8665-7a68-4284-8aca-3c1201c6b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_dir = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f595db-9b99-41a0-9712-f2c708bca27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600a1e7a-3662-4dd8-b353-105ecb3873e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"Read a JSONL file and return a list of valid JSON objects.\"\"\"\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                lines.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54c87839-326e-40fb-8fac-2e22324e15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lines = []\n",
    "for file in glob(this_nb_dir+\"/../datasets/qa_pairs/*.jsonl\"):\n",
    "    combined_lines.extend(read_jsonl(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df1bdd-ae11-426f-88e9-946a610982c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c822da09-e9b7-4bb5-a94d-0086a9644f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.getcwd() + f'/../datasets/finetune_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d63023-fc05-4e05-b9f0-dc641993a5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7363c-3f49-416d-a043-f8a589c49db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fastchat.conversation import Conversation, SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94d3d0eb-85e0-4550-b82e-8a2f3f8e1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_filepath, 'w') as jsonl_file:\n",
    "    for line in combined_lines:\n",
    "        # Accessing the instruction and response based on known keys\n",
    "        #instruction = line[\"instruction\"]\n",
    "        #response = line[\"response\"]\n",
    "        instruction, response = line[list(line.keys())[0]],line[list(line.keys())[1]]\n",
    "        conv = Conversation(\n",
    "            name=\"alpaca\",\n",
    "            system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "            roles=(\"### Instruction\", \"### Response\"),\n",
    "            sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "            sep=\"\\n\\n\",\n",
    "            sep2=\"</s>\",\n",
    "            messages=[]\n",
    "        )\n",
    "        \n",
    "        # Append messages correctly\n",
    "        conv.append_message(\"### Instruction\", instruction)\n",
    "        conv.append_message(\"### Response\", response)\n",
    "        \n",
    "        # Get the prompt from the conversation\n",
    "        prompt = conv.get_prompt()\n",
    "        \n",
    "        # Write the prompt to the JSONL file\n",
    "        json_line = json.dumps({\"prompt\": prompt})\n",
    "        jsonl_file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01d1c6-7156-426c-8f22-62e422db7d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8693af9-a07a-4f8c-b86b-75dbd7dc4d7c",
   "metadata": {},
   "source": [
    "# autotrain wants a CSV that has it all concatenated as a \"text\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffde8c9-2821-42c4-98ce-5fdaf6c8c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_filepath, 'w') as f:\n",
    "    for row in combined_lines:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3b1d96b-c9cc-4dd9-b0cb-9db55a469ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_filepath = os.getcwd() + f'/../datasets/finetune_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89ed98ca-ad5e-4a56-bf7b-8960f46429e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_csv_filepath, 'w') as f:\n",
    "    f.write('blank,text\\n')\n",
    "    for row in combined_lines:\n",
    "        try:\n",
    "            q = row['question'].replace('\"',\"'\")\n",
    "            a = row['answer'].replace('\"',\"'\")\n",
    "            conversation = f',\"you are a Gradio ui ai assistant do use your knowledge to respond to this request accurately\\n\\n### Instruction:\\n{q} \\n\\n### Response:\\n{a}\"' \n",
    "                \n",
    "            f.write(conversation + '\\n')\n",
    "        except:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38df978-a185-49b4-981d-94cfa0094c3f",
   "metadata": {},
   "source": [
    "check that it can be read locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99610f05-54f4-4299-a67d-f5e03026fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blank</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you are a Gradio ui ai assistant do use your k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     blank                                               text\n",
       "0      NaN  you are a Gradio ui ai assistant do use your k...\n",
       "1      NaN  you are a Gradio ui ai assistant do use your k...\n",
       "2      NaN  you are a Gradio ui ai assistant do use your k...\n",
       "3      NaN  you are a Gradio ui ai assistant do use your k...\n",
       "4      NaN  you are a Gradio ui ai assistant do use your k...\n",
       "..     ...                                                ...\n",
       "932    NaN  you are a Gradio ui ai assistant do use your k...\n",
       "933    NaN  you are a Gradio ui ai assistant do use your k...\n",
       "934    NaN  you are a Gradio ui ai assistant do use your k...\n",
       "935    NaN  you are a Gradio ui ai assistant do use your k...\n",
       "936    NaN  you are a Gradio ui ai assistant do use your k...\n",
       "\n",
       "[937 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.read_csv(output_csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ad574-e612-4a49-81cb-5ad1d09e1969",
   "metadata": {},
   "source": [
    "# Trying Predibase Mistral 7b \n",
    "wants this:\n",
    "{\"prompt\": ..., \"completion\": ...}\n",
    "{\"prompt\": \"Please summarize the following article ...\", \"completion\": \"Madonna kicks off Celebration World Tour in London\"}\n",
    "{\"prompt\": \"Please summarize the following article ...\", \"completion\": \"Facebook Releases First Transparency Report on what Americans see on the platform\"}\n",
    "{\"prompt\": ..., \"completion\": ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3be1b9b-e677-4312-9444-7cdb483632a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = combined_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24866e43-3396-4d73-8020-da6a9fe87ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"prompt\": \"\\\\nI have an English-to-Russian translation model and the ruDALL-E text-to-image generation model. With my current code, I can input English text, translate it to Russian, feed the Russian text to ruDALL-E to generate an image, and output the generated image.\\\\n\\\\nI would like to create a Gradio web interface to demonstrate this pipeline end-to-end:\\\\n- Have a text input box where the user can enter an English text prompt \\\\n- When the user submits, first translate the English text to Russian\\\\n- Display the translated Russian text\\\\n- Feed the Russian text to ruDALL-E to generate an image \\\\n- Display the generated image in the interface\\\\n- Allow the user to resubmit the form to generate new images from the same prompt\\\\n- Provide some pre-filled example prompts the user can try\\\\n- Put the translation and image generation components side-by-side in the interface\\\\n- Launch the interface with a Hugging Face theme\\\\n- Include links to the model\\'s GitHub repo and a related article\\\\n\", \"response\": \\'here is the gradio solution for your use case: ```python\\\\nimport random\\\\nimport torch\\\\nimport numpy as np\\\\nfrom tqdm import tqdm\\\\nfrom functools import partialmethod\\\\nimport gradio as gr\\\\nfrom gradio.mix import Series\\\\nfrom transformers import pipeline, FSMTForConditionalGeneration, FSMTTokenizer\\\\nfrom rudalle.pipelines import generate_images\\\\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae\\\\n\\\\n# disable tqdm logging from the rudalle pipeline\\\\ntqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\\\\n\\\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\\\ntranslation_model = FSMTForConditionalGeneration.from_pretrained(\"facebook/wmt19-en-ru\", torch_dtype=torch.float16).half().to(device)\\\\ntranslation_tokenizer = FSMTTokenizer.from_pretrained(\"facebook/wmt19-en-ru\")\\\\ndalle = get_rudalle_model(\"Malevich\", pretrained=True, fp16=True, device=device)\\\\ntokenizer = get_tokenizer()\\\\nvae = get_vae().to(device)\\\\n\\\\ndef translation_wrapper(text: str):\\\\n    input_ids = translation_tokenizer.encode(text, return_tensors=\"pt\")\\\\n    outputs = translation_model.generate(input_ids.to(device))\\\\n    decoded = translation_tokenizer.decode(outputs[0].float(), skip_special_tokens=True)\\\\n    return decoded\\\\n\\\\ndef dalle_wrapper(prompt: str):\\\\n    top_k, top_p = random.choice([\\\\n        (1024, 0.98),\\\\n        (512, 0.97),\\\\n        (384, 0.96),\\\\n    ])\\\\n    \\\\n    images , _ = generate_images(\\\\n        prompt, \\\\n        tokenizer, \\\\n        dalle, \\\\n        vae, \\\\n        top_k=top_k, \\\\n        images_num=1, \\\\n        top_p=top_p\\\\n    )\\\\n    title = f\"<b>{prompt}</b>\"\\\\n    return title, images[0]\\\\n\\\\n\\\\ntranslator = gr.Interface(fn=translation_wrapper, \\\\n                          inputs=[gr.inputs.Textbox(label=\\\\\\'What would you like to see?\\\\\\')],\\\\n                          outputs=\"text\")\\\\noutputs = [\\\\n    gr.outputs.HTML(label=\"\"),   \\\\n    gr.outputs.Image(label=\"\"),\\\\n]\\\\ngenerator = gr.Interface(fn=dalle_wrapper, inputs=\"text\", outputs=outputs)\\\\n\\\\n\\\\ndescription = (\\\\n    \"ruDALL-E is a 1.3B params text-to-image model by SberAI (links at the bottom). \"\\\\n    \"This demo uses an English-Russian translation model to adapt the prompts. \"\\\\n    \"Try pressing [Submit] multiple times to generate new images!\"\\\\n)\\\\narticle = (\\\\n    \"<p style=\\\\\\'text-align: center\\\\\\'>\"\\\\n    \"<a href=\\\\\\'https://github.com/sberbank-ai/ru-dalle\\\\\\'>GitHub</a> | \"\\\\n    \"<a href=\\\\\\'https://habr.com/ru/company/sberbank/blog/586926/\\\\\\'>Article (in Russian)</a>\"\\\\n    \"</p>\"\\\\n)\\\\nexamples = [[\"A still life of grapes and a bottle of wine\"], \\\\n            [\"Город в стиле киберпанк\"], \\\\n            [\"A colorful photo of a coral reef\"], \\\\n            [\"A white cat sitting in a cardboard box\"]]\\\\n            \\\\nseries = Series(translator, generator, \\\\n                title=\\\\\\'Kinda-English ruDALL-E\\\\\\',\\\\n                description=description,\\\\n                article=article,\\\\n                layout=\\\\\\'horizontal\\\\\\',\\\\n                theme=\\\\\\'huggingface\\\\\\',\\\\n                examples=examples,\\\\n                allow_flagging=False,\\\\n                live=False, \\\\n                enable_queue=True,\\\\n               )\\\\nseries.launch()\\\\n\\\\n\\\\n```\\'}'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(line).replace(\"'question'\",'\"prompt\"').replace(\"'answer'\",'\"response\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aaf85886-050a-42a8-9042-ba0aa7fe9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"\\nI have an English-to-Russian translation model and the ruDALL-E text-to-image generation model. With my current code, I can input English text, translate it to Russian, feed the Russian text to ruDALL-E to generate an image, and output the generated image.\\n\\nI would like to create a Gradio web interface to demonstrate this pipeline end-to-end:\\n- Have a text input box where the user can enter an English text prompt \\n- When the user submits, first translate the English text to Russian\\n- Display the translated Russian text\\n- Feed the Russian text to ruDALL-E to generate an image \\n- Display the generated image in the interface\\n- Allow the user to resubmit the form to generate new images from the same prompt\\n- Provide some pre-filled example prompts the user can try\\n- Put the translation and image generation components side-by-side in the interface\\n- Launch the interface with a Hugging Face theme\\n- Include links to the model's GitHub repo and a related article\\n\",\n",
       " 'response': 'here is the gradio solution for your use case: ```python\\nimport random\\nimport torch\\nimport numpy as np\\nfrom tqdm import tqdm\\nfrom functools import partialmethod\\nimport gradio as gr\\nfrom gradio.mix import Series\\nfrom transformers import pipeline, FSMTForConditionalGeneration, FSMTTokenizer\\nfrom rudalle.pipelines import generate_images\\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae\\n\\n# disable tqdm logging from the rudalle pipeline\\ntqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ntranslation_model = FSMTForConditionalGeneration.from_pretrained(\"facebook/wmt19-en-ru\", torch_dtype=torch.float16).half().to(device)\\ntranslation_tokenizer = FSMTTokenizer.from_pretrained(\"facebook/wmt19-en-ru\")\\ndalle = get_rudalle_model(\"Malevich\", pretrained=True, fp16=True, device=device)\\ntokenizer = get_tokenizer()\\nvae = get_vae().to(device)\\n\\ndef translation_wrapper(text: str):\\n    input_ids = translation_tokenizer.encode(text, return_tensors=\"pt\")\\n    outputs = translation_model.generate(input_ids.to(device))\\n    decoded = translation_tokenizer.decode(outputs[0].float(), skip_special_tokens=True)\\n    return decoded\\n\\ndef dalle_wrapper(prompt: str):\\n    top_k, top_p = random.choice([\\n        (1024, 0.98),\\n        (512, 0.97),\\n        (384, 0.96),\\n    ])\\n    \\n    images , _ = generate_images(\\n        prompt, \\n        tokenizer, \\n        dalle, \\n        vae, \\n        top_k=top_k, \\n        images_num=1, \\n        top_p=top_p\\n    )\\n    title = f\"<b>{prompt}</b>\"\\n    return title, images[0]\\n\\n\\ntranslator = gr.Interface(fn=translation_wrapper, \\n                          inputs=[gr.inputs.Textbox(label=\\'What would you like to see?\\')],\\n                          outputs=\"text\")\\noutputs = [\\n    gr.outputs.HTML(label=\"\"),   \\n    gr.outputs.Image(label=\"\"),\\n]\\ngenerator = gr.Interface(fn=dalle_wrapper, inputs=\"text\", outputs=outputs)\\n\\n\\ndescription = (\\n    \"ruDALL-E is a 1.3B params text-to-image model by SberAI (links at the bottom). \"\\n    \"This demo uses an English-Russian translation model to adapt the prompts. \"\\n    \"Try pressing [Submit] multiple times to generate new images!\"\\n)\\narticle = (\\n    \"<p style=\\'text-align: center\\'>\"\\n    \"<a href=\\'https://github.com/sberbank-ai/ru-dalle\\'>GitHub</a> | \"\\n    \"<a href=\\'https://habr.com/ru/company/sberbank/blog/586926/\\'>Article (in Russian)</a>\"\\n    \"</p>\"\\n)\\nexamples = [[\"A still life of grapes and a bottle of wine\"], \\n            [\"Город в стиле киберпанк\"], \\n            [\"A colorful photo of a coral reef\"], \\n            [\"A white cat sitting in a cardboard box\"]]\\n            \\nseries = Series(translator, generator, \\n                title=\\'Kinda-English ruDALL-E\\',\\n                description=description,\\n                article=article,\\n                layout=\\'horizontal\\',\\n                theme=\\'huggingface\\',\\n                examples=examples,\\n                allow_flagging=False,\\n                live=False, \\n                enable_queue=True,\\n               )\\nseries.launch()\\n\\n\\n```'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bdd38ed7-e922-412a-832b-d97d4637276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath_predibase = os.getcwd() + f'/../datasets/finetune_data_mistral_predibase.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "623981b1-7212-4f4b-a7c9-bea5a73f771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_filepath_predibase, 'w') as jsonl_file:\n",
    "    for line in combined_lines:\n",
    "        new_line = {}\n",
    "        new_line[\"prompt\"] = line['question']\n",
    "        new_line[\"completion\"] = line['answer']\n",
    "        jsonl_file.write(json.dumps(new_line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59b29dab-eaed-45fa-aeee-227e7162a01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'answer'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f7396-e503-4577-844a-fb2a86ba6e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
